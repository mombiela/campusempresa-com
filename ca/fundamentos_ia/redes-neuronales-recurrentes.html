<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Xarxes Neuronals Recurrents (RNN)</title>

    <link rel="alternate" href="https://campusempresa.com/es/fundamentos_ia/redes-neuronales-recurrentes" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/fundamentos_ia/redes-neuronales-recurrentes" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/fundamentos_ia/redes-neuronales-recurrentes" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/ca/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construint la societat d'avui i del demà</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
							<a href="/es/fundamentos_ia/redes-neuronales-recurrentes" id="lnk_lang_es" data-lang="es" class="px-2">ES</a></b>
				|
				<b id="lit_lang_ca" class="px-2">CA</b>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/ca/objective">El Projecte</a>
				<a href="/ca/about">Sobre nosaltres</a>
				<a href="/ca/contribute">Contribuir</a>
				<a href="/ca/donate">Donacions</a>
				<a href="/ca/licence">Llicència</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'><div class='col-6'><a href='redes-neuronales-convolucionales'>&#x25C4; Xarxes Neuronals Convolucionals (CNN)</a></div><div class='col-6 text-end'><a href='logica-difusa'>Lògica Difusa &#x25BA;</a></div></div><div class='content'></div><h1>Introducció a les Xarxes Neuronals Recurrents (RNN)</h1>
<div class='content'><p>Les Xarxes Neuronals Recurrents (RNN) són un tipus de xarxa neuronal dissenyada per reconèixer patrons en seqüències de dades, com text, sèries temporals i dades d'àudio. A diferència de les xarxes neuronals tradicionals, les RNN tenen connexions recurrents que permeten que la informació persisteixi.</p>
<ul>
<li><strong>Seqüències de dades</strong>: Les RNN són especialment útils per a dades seqüencials on l'ordre de les dades és important.</li>
<li><strong>Memòria</strong>: Les RNN tenen una &quot;memòria&quot; que els permet recordar informació sobre les dades anteriors en la seqüència.</li>
<li><strong>Aplicacions</strong>: Processament de llenguatge natural (NLP), traducció automàtica, reconeixement de veu, anàlisi de sèries temporals.</li>
</ul>
</div><h1>Arquitectura d'una RNN</h1>
<div class='content'><p>Una RNN típica consisteix en una capa d'entrada, una o més capes recurrents i una capa de sortida.</p>
<ul>
<li><strong>Capa d'entrada</strong>: Rep la seqüència de dades.</li>
<li><strong>Capa recurrent</strong>: Processa la seqüència de dades, mantenint una memòria dels estats anteriors.</li>
<li><strong>Capa de sortida</strong>: Produeix la sortida final després de processar tota la seqüència.</li>
</ul>
</div><h2>Estructura d'una RNN</h2>
<div class='content'><p>L'estructura bàsica d'una RNN pot representar-se com:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("eF90IC0+IGhfdCAtPiB5X3Q="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>x_t -&gt; h_t -&gt; y_t</pre></div><div class='content'><p>On:</p>
<ul>
<li><code>x_t</code> és l'entrada en el temps <code>t</code>.</li>
<li><code>h_t</code> és l'estat ocult en el temps <code>t</code>.</li>
<li><code>y_t</code> és la sortida en el temps <code>t</code>.</li>
</ul>
<p>La relació entre aquests elements es pot expressar mitjançant les següents equacions:</p>
<ul>
<li><code>h_t = f(W_hh * h_{t-1} + W_xh * x_t + b_h)</code></li>
<li><code>y_t = g(W_hy * h_t + b_y)</code></li>
</ul>
<p>Aquí, <code>W_hh</code>, <code>W_xh</code>, i <code>W_hy</code> són matrius de pesos, <code>b_h</code> i <code>b_y</code> són biaixos, i <code>f</code> i <code>g</code> són funcions d'activació.</p>
</div><h1>Exemple de Codi: Implementació Bàsica d'una RNN en Python</h1>
<div class='content'><p>A continuació es mostra un exemple bàsic d'una RNN utilitzant la biblioteca <code>TensorFlow</code> en Python.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLm1vZGVscyBpbXBvcnQgU2VxdWVudGlhbApmcm9tIHRlbnNvcmZsb3cua2VyYXMubGF5ZXJzIGltcG9ydCBTaW1wbGVSTk4sIERlbnNlCgojIERlZmluaXIgbGEgc2Vxw7zDqG5jaWEgZGUgZGFkZXMgZCdlbnRyYWRhCmlucHV0X2RhdGEgPSB0Zi5yYW5kb20ubm9ybWFsKFszMiwgMTAsIDhdKSAgIyAoYmF0Y2hfc2l6ZSwgdGltZXN0ZXBzLCBpbnB1dF9kaW0pCgojIENyZWFyIGVsIG1vZGVsIFJOTgptb2RlbCA9IFNlcXVlbnRpYWwoKQptb2RlbC5hZGQoU2ltcGxlUk5OKDE2LCBpbnB1dF9zaGFwZT0oMTAsIDgpKSkgICMgMTYgdW5pdGF0cyBlbiBsYSBjYXBhIFJOTgptb2RlbC5hZGQoRGVuc2UoMSkpICAjIENhcGEgZGUgc29ydGlkYQoKIyBDb21waWxhciBlbCBtb2RlbAptb2RlbC5jb21waWxlKG9wdGltaXplcj0nYWRhbScsIGxvc3M9J21zZScpCgojIFJlc3VtIGRlbCBtb2RlbAptb2RlbC5zdW1tYXJ5KCk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

# Definir la seq&uuml;&egrave;ncia de dades d'entrada
input_data = tf.random.normal([32, 10, 8])  # (batch_size, timesteps, input_dim)

# Crear el model RNN
model = Sequential()
model.add(SimpleRNN(16, input_shape=(10, 8)))  # 16 unitats en la capa RNN
model.add(Dense(1))  # Capa de sortida

# Compilar el model
model.compile(optimizer='adam', loss='mse')

# Resum del model
model.summary()</pre></div><div class='content'><p>En aquest exemple:</p>
<ul>
<li><code>input_data</code> és un tensor de dades d'entrada amb una mida de lot de 32, 10 passos de temps i 8 característiques per pas de temps.</li>
<li><code>SimpleRNN</code> és una capa recurrent amb 16 unitats.</li>
<li><code>Dense</code> és una capa densa que produeix una sortida d'un sol valor.</li>
</ul>
</div><h1>Avantatges i Desavantatges de les RNN</h1>
<h2>Avantatges</h2>
<div class='content'><ul>
<li><strong>Capacitat de manejar seqüències de longitud variable</strong>: Les RNN poden processar seqüències de diferents longituds.</li>
<li><strong>Memòria de curt termini</strong>: Les RNN poden recordar informació dels passos de temps anteriors.</li>
</ul>
</div><h2>Desavantatges</h2>
<div class='content'><ul>
<li><strong>Problemes de gradient</strong>: Les RNN poden patir problemes de desvaniment i explosió del gradient, cosa que dificulta l'entrenament.</li>
<li><strong>Memòria de llarg termini limitada</strong>: Les RNN estàndard tenen dificultats per recordar informació de passos de temps molt anteriors.</li>
</ul>
</div><h1>Conclusió</h1>
<div class='content'><p>Les Xarxes Neuronals Recurrents (RNN) són una eina poderosa per al processament de dades seqüencials. Tot i que tenen algunes limitacions, com problemes de gradient i memòria de llarg termini limitada, són fonamentals en aplicacions com el processament de llenguatge natural i l'anàlisi de sèries temporals. Amb l'avanç de tècniques com LSTM i GRU, moltes d'aquestes limitacions s'han mitigat, permetent a les RNN manejar seqüències més llargues i complexes de manera més efectiva.</p>
</div><div class='row navigation'><div class='col-6'><a href='redes-neuronales-convolucionales'>&#x25C4; Xarxes Neuronals Convolucionals (CNN)</a></div><div class='col-6 text-end'><a href='logica-difusa'>Lògica Difusa &#x25BA;</a></div></div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicitat</h1>
			<p>Aquest espai està destinat a publicitat.</p>
			<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Gràcies per col·laborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/ca/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
