<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title></title>

	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construint la societat d'avui<br> i del demà</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
							<a href="https://campusempresa.com/deep_learning/funciones-activacion" id="lnk_lang_es" data-lang="es">Castellano</a></b>
				|
				<b id="lit_lang_ca">Catala</b>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="objective">El Projecte</a>
				<a href="about">Sobre nosaltres</a>
				<a href="contribute">Contribuir</a>
				<a href="donate">Donacions</a>
				<a href="licence">Llicència</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'></div><h1>Introducció</h1>
<div class='content'><p>En el context del Deep Learning, les funcions d'activació juguen un paper crucial en la capacitat de les xarxes neuronals per aprendre i modelar dades complexes. Aquestes funcions determinen si una neurona ha d'activar-se o no, introduint no linealitats que permeten a la xarxa neuronal aprendre relacions complexes en les dades.</p>
</div><h1>Tipus de Funcions d'Activació</h1>
<div class='content'><p>Hi ha diverses funcions d'activació que s'utilitzen comunament en les xarxes neuronals. A continuació, es descriuen algunes de les més importants:</p>
</div><h2>Funció Sigmoide</h2>
<div class='content'><p>La funció sigmoide és una de les funcions d'activació més antigues i es defineix com:</p>
<p>\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<ul>
<li><strong>Rang de sortida:</strong> (0, 1)</li>
<li><strong>Avantatges:</strong>
<ul>
<li>Suau i diferenciable.</li>
<li>Sortida en un rang limitat (0 a 1), útil per a probabilitats.</li>
</ul>
</li>
<li><strong>Desavantatges:</strong>
<ul>
<li>Problema de desvaniment del gradient.</li>
<li>Sortides no centrades en zero.</li>
</ul>
</li>
</ul>
<h4>Exemple de Codi</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgc2lnbW9pZCh4KToKICAgIHJldHVybiAxIC8gKDEgKyBucC5leHAoLXgpKQoKIyBFeGVtcGxlIGQnw7pzCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludChzaWdtb2lkKHgpKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Exemple d'&uacute;s
x = np.array([-1.0, 0.0, 1.0])
print(sigmoid(x))</pre></div><div class='content'></div><h2>Funció Tanh</h2>
<div class='content'><p>La funció tangent hiperbòlica (tanh) és similar a la sigmoide però les seves sortides estan centrades en zero:</p>
<p>\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<ul>
<li><strong>Rang de sortida:</strong> (-1, 1)</li>
<li><strong>Avantatges:</strong>
<ul>
<li>Sortides centrades en zero.</li>
<li>Suau i diferenciable.</li>
</ul>
</li>
<li><strong>Desavantatges:</strong>
<ul>
<li>Encara pot patir del problema de desvaniment del gradient.</li>
</ul>
</li>
</ul>
<h4>Exemple de Codi</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHRhbmgoeCk6CiAgICByZXR1cm4gbnAudGFuaCh4KQoKIyBFeGVtcGxlIGQnw7pzCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludCh0YW5oKHgpKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def tanh(x):
    return np.tanh(x)

# Exemple d'&uacute;s
x = np.array([-1.0, 0.0, 1.0])
print(tanh(x))</pre></div><div class='content'></div><h2>Funció ReLU (Rectified Linear Unit)</h2>
<div class='content'><p>La funció ReLU és actualment una de les funcions d'activació més populars:</p>
<p>\[ \text{ReLU}(x) = \max(0, x) \]</p>
<ul>
<li><strong>Rang de sortida:</strong> [0, ∞)</li>
<li><strong>Avantatges:</strong>
<ul>
<li>Simple i eficient.</li>
<li>Mitiga el problema de desvaniment del gradient.</li>
</ul>
</li>
<li><strong>Desavantatges:</strong>
<ul>
<li>Problema de neurones mortes (quan els valors negatius es converteixen en zero).</li>
</ul>
</li>
</ul>
<h4>Exemple de Codi</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHJlbHUoeCk6CiAgICByZXR1cm4gbnAubWF4aW11bSgwLCB4KQoKIyBFeGVtcGxlIGQnw7pzCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludChyZWx1KHgpKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def relu(x):
    return np.maximum(0, x)

# Exemple d'&uacute;s
x = np.array([-1.0, 0.0, 1.0])
print(relu(x))</pre></div><div class='content'></div><h2>Funció Leaky ReLU</h2>
<div class='content'><p>La funció Leaky ReLU és una variant de ReLU que intenta solucionar el problema de les neurones mortes:</p>
<p>\[ \text{Leaky ReLU}(x) = \begin{cases}
x &amp; \text{si } x \geq 0 <br>\alpha x &amp; \text{si } x &lt; 0
\end{cases} \]</p>
<p>on \( \alpha \) és un petit valor positiu.</p>
<ul>
<li><strong>Rang de sortida:</strong> (-∞, ∞)</li>
<li><strong>Avantatges:</strong>
<ul>
<li>Mitiga el problema de neurones mortes.</li>
</ul>
</li>
<li><strong>Desavantatges:</strong>
<ul>
<li>Introdueix un petit biaix en les sortides negatives.</li>
</ul>
</li>
</ul>
<h4>Exemple de Codi</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIGxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSk6CiAgICByZXR1cm4gbnAud2hlcmUoeCA+IDAsIHgsIGFscGhhICogeCkKCiMgRXhlbXBsZSBkJ8O6cwp4ID0gbnAuYXJyYXkoWy0xLjAsIDAuMCwgMS4wXSkKcHJpbnQobGVha3lfcmVsdSh4KSk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

# Exemple d'&uacute;s
x = np.array([-1.0, 0.0, 1.0])
print(leaky_relu(x))</pre></div><div class='content'></div><h1>Comparació de Funcions d'Activació</h1>
<div class='content'><p>| Funció       | Rang de Sortida | Avantatges                              | Desavantatges                          |
|--------------|-----------------|-----------------------------------------|----------------------------------------|
| Sigmoide     | (0, 1)          | Suau, diferenciable, útil per a probabilitats | Desvaniment del gradient, no centrada en zero |
| Tanh         | (-1, 1)         | Sortides centrades en zero, suau        | Desvaniment del gradient               |
| ReLU         | [0, ∞)          | Simple, eficient, mitiga desvaniment del gradient | Neurones mortes                        |
| Leaky ReLU   | (-∞, ∞)         | Mitiga neurones mortes                  | Introdueix biaix en sortides negatives |</p>
</div><h1>Conclusió</h1>
<div class='content'><p>Les funcions d'activació són components essencials en les xarxes neuronals, ja que permeten la introducció de no linealitats i, per tant, la capacitat de modelar relacions complexes en les dades. L'elecció de la funció d'activació adequada pot tenir un impacte significatiu en el rendiment i l'eficiència del model. És important entendre les característiques, avantatges i desavantatges de cada funció per prendre decisions informades en el disseny de xarxes neuronals.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicitat</h1>
			<p>Aquest espai està destinat a publicitat.</p>
			<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Gràcies per col·laborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
       <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
       <a href="cookies">Més informació</a>
   </div>	
	</div>    
</body>
</html>
