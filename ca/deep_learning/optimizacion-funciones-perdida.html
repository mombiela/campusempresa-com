<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Optimització i Funcions de Pèrdua</title>

    <link rel="alternate" href="https://campusempresa.com/es/deep_learning/optimizacion-funciones-perdida" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/deep_learning/optimizacion-funciones-perdida" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/deep_learning/optimizacion-funciones-perdida" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/ca/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construint la societat d'avui i del demà</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
							<a href="/es/deep_learning/optimizacion-funciones-perdida" id="lnk_lang_es" data-lang="es" class="px-2">ES</a></b>
				|
				<b id="lit_lang_ca" class="px-2">CA</b>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/ca/objective">El Projecte</a>
				<a href="/ca/about">Sobre nosaltres</a>
				<a href="/ca/contribute">Contribuir</a>
				<a href="/ca/donate">Donacions</a>
				<a href="/ca/licence">Llicència</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'><div class='col-6'><a href='propagacion-adelante-atras'>&#x25C4; Propagació Cap Endavant i Cap Enrere</a></div><div class='col-6 text-end'><a href='introduccion-cnn'>Introducció a les CNN &#x25BA;</a></div></div><div class='content'></div><h1>Introducció</h1>
<div class='content'><p>En el context del Deep Learning, l'optimització i les funcions de pèrdua són components fonamentals que permeten als models aprendre de les dades. Aquest tema se centrarà en explicar què són les funcions de pèrdua, com s'utilitzen en l'entrenament de models i els mètodes d'optimització més comuns.</p>
</div><h1>Funcions de Pèrdua</h1>
<div class='content'><p>Les funcions de pèrdua, també conegudes com a funcions de cost, mesuren la discrepància entre les prediccions del model i els valors reals. L'elecció d'una funció de pèrdua adequada és crucial per al rendiment del model.</p>
</div><h2>Tipus de Funcions de Pèrdua</h2>
<div class='content'><ul>
<li><strong>Error Quadràtic Mitjà (MSE)</strong>: Utilitzat principalment en problemes de regressió.
\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</li>
<li><strong>Entropia Creuada (Cross-Entropy)</strong>: Comunament utilitzada en problemes de classificació.
\[
\text{Cross-Entropy} = -\sum_{i=1}^{n} y_i \log(\hat{y}_i)
\]</li>
<li><strong>Error Absolut Mitjà (MAE)</strong>: Una altra opció per a problemes de regressió.
\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\]</li>
</ul>
</div><h2>Exemple de Codi: Implementació de MSE en Python</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgbWVhbl9zcXVhcmVkX2Vycm9yKHlfdHJ1ZSwgeV9wcmVkKToKICAgIHJldHVybiBucC5tZWFuKCh5X3RydWUgLSB5X3ByZWQpICoqIDIpCgojIEV4ZW1wbGUgZCfDunMKeV90cnVlID0gbnAuYXJyYXkoWzEuMCwgMi4wLCAzLjBdKQp5X3ByZWQgPSBucC5hcnJheShbMS4xLCAxLjksIDMuMl0pCm1zZSA9IG1lYW5fc3F1YXJlZF9lcnJvcih5X3RydWUsIHlfcHJlZCkKcHJpbnQoZidNU0U6IHttc2V9Jyk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# Exemple d'&uacute;s
y_true = np.array([1.0, 2.0, 3.0])
y_pred = np.array([1.1, 1.9, 3.2])
mse = mean_squared_error(y_true, y_pred)
print(f'MSE: {mse}')</pre></div><div class='content'></div><h1>Mètodes d'Optimització</h1>
<div class='content'><p>Els mètodes d'optimització s'utilitzen per ajustar els paràmetres del model amb l'objectiu de minimitzar la funció de pèrdua. A continuació, es descriuen alguns dels mètodes més comuns.</p>
</div><h2>Gradient Descendent</h2>
<div class='content'><p>El Gradient Descendent és un dels algorismes d'optimització més utilitzats. Es basa en actualitzar els paràmetres del model en la direcció del gradient negatiu de la funció de pèrdua.</p>
<ul>
<li><strong>Gradient Descendent Estàndard (Batch Gradient Descent)</strong>: Utilitza tot el conjunt de dades per calcular el gradient.</li>
<li><strong>Gradient Descendent Estocàstic (SGD)</strong>: Utilitza un sol exemple d'entrenament per calcular el gradient.</li>
<li><strong>Gradient Descendent Mini-Batch</strong>: Utilitza un petit subconjunt d'exemples d'entrenament per calcular el gradient.</li>
</ul>
</div><h2>Algorismes d'Optimització Avançats</h2>
<div class='content'><ul>
<li><strong>Adam (Adaptive Moment Estimation)</strong>: Combina els avantatges d'AdaGrad i RMSProp.</li>
<li><strong>RMSProp</strong>: Ajusta la taxa d'aprenentatge per a cada paràmetre.</li>
<li><strong>AdaGrad</strong>: Ajusta la taxa d'aprenentatge en funció dels gradients acumulats.</li>
</ul>
</div><h2>Exemple de Codi: Implementació de Gradient Descendent en Python</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgZ3JhZGllbnRfZGVzY2VudChYLCB5LCBscj0wLjAxLCBlcG9jaHM9MTAwMCk6CiAgICBtLCBuID0gWC5zaGFwZQogICAgdGhldGEgPSBucC56ZXJvcyhuKQogICAgZm9yIGVwb2NoIGluIHJhbmdlKGVwb2Nocyk6CiAgICAgICAgZ3JhZGllbnQgPSAoMS9tKSAqIFguVC5kb3QoWC5kb3QodGhldGEpIC0geSkKICAgICAgICB0aGV0YSAtPSBsciAqIGdyYWRpZW50CiAgICByZXR1cm4gdGhldGEKCiMgRXhlbXBsZSBkJ8O6cwpYID0gbnAuYXJyYXkoW1sxLCAxXSwgWzEsIDJdLCBbMiwgMl0sIFsyLCAzXV0pCnkgPSBucC5hcnJheShbNiwgOCwgOSwgMTFdKQp0aGV0YSA9IGdyYWRpZW50X2Rlc2NlbnQoWCwgeSkKcHJpbnQoZidUaGV0YToge3RoZXRhfScp"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def gradient_descent(X, y, lr=0.01, epochs=1000):
    m, n = X.shape
    theta = np.zeros(n)
    for epoch in range(epochs):
        gradient = (1/m) * X.T.dot(X.dot(theta) - y)
        theta -= lr * gradient
    return theta

# Exemple d'&uacute;s
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
y = np.array([6, 8, 9, 11])
theta = gradient_descent(X, y)
print(f'Theta: {theta}')</pre></div><div class='content'></div><h1>Comparació de Mètodes d'Optimització</h1>
<div class='content'><p>| Mètode          | Avantatges                                      | Desavantatges                                  |
|-----------------|-------------------------------------------------|------------------------------------------------|
| Batch GD        | Convergència estable                            | Computacionalment costós                       |
| SGD             | Ràpid i eficient                                | Convergència sorollosa                         |
| Mini-Batch GD   | Equilibri entre eficiència i estabilitat        | Requereix ajust del tamany del mini-batch      |
| Adam            | Ràpida convergència i ajust automàtic           | Requereix més memòria                          |
| RMSProp         | Ajust dinàmic de la taxa d'aprenentatge         | Pot ser inestable en alguns casos              |
| AdaGrad         | Bo per a problemes amb característiques esparses | Pot fer que la taxa d'aprenentatge es torni molt petita |</p>
</div><h1>Conclusió</h1>
<div class='content'><p>L'optimització i les funcions de pèrdua són components essencials en l'entrenament de models de Deep Learning. L'elecció d'una funció de pèrdua adequada i un mètode d'optimització eficient pot tenir un impacte significatiu en el rendiment del model. És important comprendre les característiques i limitacions de cada mètode per prendre decisions informades durant el desenvolupament del model.</p>
</div><div class='row navigation'><div class='col-6'><a href='propagacion-adelante-atras'>&#x25C4; Propagació Cap Endavant i Cap Enrere</a></div><div class='col-6 text-end'><a href='introduccion-cnn'>Introducció a les CNN &#x25BA;</a></div></div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicitat</h1>
			<p>Aquest espai està destinat a publicitat.</p>
			<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Gràcies per col·laborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/ca/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
