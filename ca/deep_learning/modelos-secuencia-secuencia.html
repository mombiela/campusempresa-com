<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Models Seqüència a Seqüència</title>

    <link rel="alternate" href="https://campusempresa.com/es/deep_learning/modelos-secuencia-secuencia" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/deep_learning/modelos-secuencia-secuencia" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/deep_learning/modelos-secuencia-secuencia" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/ca/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construint la societat d'avui<br> i del demà</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
							<a href="/es/deep_learning/modelos-secuencia-secuencia" id="lnk_lang_es" data-lang="es" class="px-2">ES</a></b>
				|
				<b id="lit_lang_ca" class="px-2">CA</b>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/ca/objective">El Projecte</a>
				<a href="/ca/about">Sobre nosaltres</a>
				<a href="/ca/contribute">Contribuir</a>
				<a href="/ca/donate">Donacions</a>
				<a href="/ca/licence">Llicència</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'></div><h1>Introducció</h1>
<div class='content'><p>Els models seqüència a seqüència (Seq2Seq) són una classe de models de deep learning dissenyats per transformar una seqüència d'elements en una altra seqüència. Aquests models són àmpliament utilitzats en tasques com traducció automàtica, resum de text, i generació de text.</p>
</div><h1>Conceptes Clau</h1>
<div class='content'><ul>
<li><strong>Codificador (Encoder)</strong>: Processa la seqüència d'entrada i la converteix en una representació d'estat.</li>
<li><strong>Decodificador (Decoder)</strong>: Pren la representació d'estat del codificador i genera la seqüència de sortida.</li>
<li><strong>Atenció (Attention)</strong>: Mecanisme que permet al decodificador enfocar-se en diferents parts de la seqüència d'entrada en cada pas de la generació de la seqüència de sortida.</li>
<li><strong>Embedding</strong>: Representació densa i de baixa dimensió de paraules o tokens.</li>
<li><strong>RNN, LSTM, GRU</strong>: Tipus de xarxes neuronals recurrents utilitzades comunament en models Seq2Seq.</li>
</ul>
</div><h1>Arquitectura d'un Model Seq2Seq</h1>
<div class='content'></div><h2>Codificador</h2>
<div class='content'><p>El codificador pren una seqüència d'entrada i la processa a través d'una xarxa neuronal recurrent (RNN), Long Short-Term Memory (LSTM) o Gated Recurrent Unit (GRU) per produir una representació d'estat.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLmxheWVycyBpbXBvcnQgTFNUTSwgRW1iZWRkaW5nLCBEZW5zZQoKIyBEZWZpbmljacOzIGRlbCBjb2RpZmljYWRvcgplbmNvZGVyX2lucHV0cyA9IHRmLmtlcmFzLklucHV0KHNoYXBlPShOb25lLCkpCnggPSBFbWJlZGRpbmcoaW5wdXRfZGltPXZvY2FiX3NpemUsIG91dHB1dF9kaW09ZW1iZWRkaW5nX2RpbSkoZW5jb2Rlcl9pbnB1dHMpCmVuY29kZXJfb3V0cHV0cywgc3RhdGVfaCwgc3RhdGVfYyA9IExTVE0odW5pdHM9bGF0ZW50X2RpbSwgcmV0dXJuX3N0YXRlPVRydWUpKHgpCmVuY29kZXJfc3RhdGVzID0gW3N0YXRlX2gsIHN0YXRlX2Nd"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.layers import LSTM, Embedding, Dense

# Definici&oacute; del codificador
encoder_inputs = tf.keras.Input(shape=(None,))
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)
encoder_outputs, state_h, state_c = LSTM(units=latent_dim, return_state=True)(x)
encoder_states = [state_h, state_c]</pre></div><div class='content'></div><h2>Decodificador</h2>
<div class='content'><p>El decodificador utilitza la representació d'estat del codificador per generar la seqüència de sortida. En cada pas, el decodificador prediu el següent token de la seqüència de sortida.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBEZWZpbmljacOzIGRlbCBkZWNvZGlmaWNhZG9yCmRlY29kZXJfaW5wdXRzID0gdGYua2VyYXMuSW5wdXQoc2hhcGU9KE5vbmUsKSkKeCA9IEVtYmVkZGluZyhpbnB1dF9kaW09dm9jYWJfc2l6ZSwgb3V0cHV0X2RpbT1lbWJlZGRpbmdfZGltKShkZWNvZGVyX2lucHV0cykKeCA9IExTVE0odW5pdHM9bGF0ZW50X2RpbSwgcmV0dXJuX3NlcXVlbmNlcz1UcnVlKSh4LCBpbml0aWFsX3N0YXRlPWVuY29kZXJfc3RhdGVzKQpkZWNvZGVyX291dHB1dHMgPSBEZW5zZSh1bml0cz12b2NhYl9zaXplLCBhY3RpdmF0aW9uPSdzb2Z0bWF4JykoeCk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Definici&oacute; del decodificador
decoder_inputs = tf.keras.Input(shape=(None,))
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)
x = LSTM(units=latent_dim, return_sequences=True)(x, initial_state=encoder_states)
decoder_outputs = Dense(units=vocab_size, activation='softmax')(x)</pre></div><div class='content'></div><h2>Mecanisme d'Atenció</h2>
<div class='content'><p>El mecanisme d'atenció permet al decodificador enfocar-se en diferents parts de la seqüència d'entrada en cada pas de la generació de la seqüència de sortida.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSB0ZW5zb3JmbG93LmtlcmFzLmxheWVycyBpbXBvcnQgQXR0ZW50aW9uCgojIERlZmluaWNpw7MgZGVsIG1lY2FuaXNtZSBkJ2F0ZW5jacOzCmF0dGVudGlvbiA9IEF0dGVudGlvbigpCmNvbnRleHRfdmVjdG9yLCBhdHRlbnRpb25fd2VpZ2h0cyA9IGF0dGVudGlvbihbZW5jb2Rlcl9vdXRwdXRzLCBkZWNvZGVyX2lucHV0c10p"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from tensorflow.keras.layers import Attention

# Definici&oacute; del mecanisme d'atenci&oacute;
attention = Attention()
context_vector, attention_weights = attention([encoder_outputs, decoder_inputs])</pre></div><div class='content'></div><h1>Comparació de RNN, LSTM i GRU</h1>
<div class='content'><p>| Característica | RNN | LSTM | GRU |
|----------------|-----|------|-----|
| Estructura     | Simple | Complexa (amb cel·les de memòria) | Intermèdia |
| Rendiment      | Menor | Alt | Alt |
| Velocitat      | Ràpida | Llenta | Intermèdia |
| Ús de memòria  | Baixa | Alta | Intermèdia |</p>
</div><h1>Exemple Complet</h1>
<div class='content'><p>A continuació es mostra un exemple complet d'un model Seq2Seq amb un codificador i un decodificador utilitzant LSTM.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLmxheWVycyBpbXBvcnQgSW5wdXQsIExTVE0sIEVtYmVkZGluZywgRGVuc2UKCiMgUGFyw6BtZXRyZXMKdm9jYWJfc2l6ZSA9IDEwMDAwCmVtYmVkZGluZ19kaW0gPSAyNTYKbGF0ZW50X2RpbSA9IDUxMgoKIyBDb2RpZmljYWRvcgplbmNvZGVyX2lucHV0cyA9IElucHV0KHNoYXBlPShOb25lLCkpCnggPSBFbWJlZGRpbmcoaW5wdXRfZGltPXZvY2FiX3NpemUsIG91dHB1dF9kaW09ZW1iZWRkaW5nX2RpbSkoZW5jb2Rlcl9pbnB1dHMpCmVuY29kZXJfb3V0cHV0cywgc3RhdGVfaCwgc3RhdGVfYyA9IExTVE0odW5pdHM9bGF0ZW50X2RpbSwgcmV0dXJuX3N0YXRlPVRydWUpKHgpCmVuY29kZXJfc3RhdGVzID0gW3N0YXRlX2gsIHN0YXRlX2NdCgojIERlY29kaWZpY2Fkb3IKZGVjb2Rlcl9pbnB1dHMgPSBJbnB1dChzaGFwZT0oTm9uZSwpKQp4ID0gRW1iZWRkaW5nKGlucHV0X2RpbT12b2NhYl9zaXplLCBvdXRwdXRfZGltPWVtYmVkZGluZ19kaW0pKGRlY29kZXJfaW5wdXRzKQp4ID0gTFNUTSh1bml0cz1sYXRlbnRfZGltLCByZXR1cm5fc2VxdWVuY2VzPVRydWUpKHgsIGluaXRpYWxfc3RhdGU9ZW5jb2Rlcl9zdGF0ZXMpCmRlY29kZXJfb3V0cHV0cyA9IERlbnNlKHVuaXRzPXZvY2FiX3NpemUsIGFjdGl2YXRpb249J3NvZnRtYXgnKSh4KQoKIyBNb2RlbCBTZXEyU2VxCm1vZGVsID0gdGYua2VyYXMuTW9kZWwoW2VuY29kZXJfaW5wdXRzLCBkZWNvZGVyX2lucHV0c10sIGRlY29kZXJfb3V0cHV0cykKbW9kZWwuY29tcGlsZShvcHRpbWl6ZXI9J2FkYW0nLCBsb3NzPSdzcGFyc2VfY2F0ZWdvcmljYWxfY3Jvc3NlbnRyb3B5Jyk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense

# Par&agrave;metres
vocab_size = 10000
embedding_dim = 256
latent_dim = 512

# Codificador
encoder_inputs = Input(shape=(None,))
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)
encoder_outputs, state_h, state_c = LSTM(units=latent_dim, return_state=True)(x)
encoder_states = [state_h, state_c]

# Decodificador
decoder_inputs = Input(shape=(None,))
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)
x = LSTM(units=latent_dim, return_sequences=True)(x, initial_state=encoder_states)
decoder_outputs = Dense(units=vocab_size, activation='softmax')(x)

# Model Seq2Seq
model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')</pre></div><div class='content'></div><h1>Conclusió</h1>
<div class='content'><p>Els models seqüència a seqüència són eines poderoses en el camp del deep learning, especialment útils per a tasques que requereixen la transformació d'una seqüència de dades en una altra. Comprendre l'arquitectura i els components clau d'aquests models, com el codificador, el decodificador i el mecanisme d'atenció, és fonamental per a la seva implementació i optimització en diverses aplicacions.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicitat</h1>
			<p>Aquest espai està destinat a publicitat.</p>
			<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Gràcies per col·laborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/ca/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
