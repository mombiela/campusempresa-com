<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title></title>

	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construint la societat d'avui<br> i del demà</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
							<a href="https://campusempresa.com/deep_learning/perceptron-multicapa" id="lnk_lang_es" data-lang="es">Castellano</a></b>
				|
				<b id="lit_lang_ca">Catala</b>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="objective">El Projecte</a>
				<a href="about">Sobre nosaltres</a>
				<a href="contribute">Contribuir</a>
				<a href="donate">Donacions</a>
				<a href="licence">Llicència</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'></div><h1>Introducció</h1>
<div class='content'><p>En aquesta secció del curs sobre Deep Learning, explorarem els conceptes fonamentals del Perceptró i el Perceptró Multicapa (MLP). Aquests són els blocs bàsics de les xarxes neuronals i són essencials per entendre models més complexos en Deep Learning.</p>
</div><h1>Perceptró</h1>
<div class='content'><p>El Perceptró és el model més simple d'una xarxa neuronal i va ser introduït per Frank Rosenblatt el 1958. És un classificador lineal que pren una entrada, realitza una suma ponderada i aplica una funció d'activació per produir una sortida.</p>
</div><h2>Conceptes Clau</h2>
<div class='content'><ul>
<li><strong>Neurones</strong>: Unitats bàsiques de processament en una xarxa neuronal.</li>
<li><strong>Pesos (Weights)</strong>: Coeficients que multipliquen les entrades.</li>
<li><strong>Bias (Bias)</strong>: Un terme addicional que ajuda a ajustar la sortida del model.</li>
<li><strong>Funció d'Activació</strong>: Funció que decideix si una neurona ha d'activar-se o no.</li>
</ul>
</div><h2>Estructura del Perceptró</h2>
<div class='content'><p>El Perceptró pot ser representat matemàticament com:</p>
<p>\[ y = f(\sum_{i=1}^{n} w_i x_i + b) \]</p>
<p>On:</p>
<ul>
<li>\( y \) és la sortida.</li>
<li>\( w_i \) són els pesos.</li>
<li>\( x_i \) són les entrades.</li>
<li>\( b \) és el bias.</li>
<li>\( f \) és la funció d'activació (per exemple, la funció esglaó).</li>
</ul>
</div><h2>Exemple de Codi</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIEZ1bmNpw7MgZCdhY3RpdmFjacOzIChlc2dsYcOzKQpkZWYgc3RlcF9mdW5jdGlvbih4KToKICAgIHJldHVybiAxIGlmIHggPj0gMCBlbHNlIDAKCiMgUGVyY2VwdHLDsyBzaW1wbGUKY2xhc3MgUGVyY2VwdHJvbjoKICAgIGRlZiBfX2luaXRfXyhzZWxmLCBpbnB1dF9zaXplLCBsZWFybmluZ19yYXRlPTAuMSk6CiAgICAgICAgc2VsZi53ZWlnaHRzID0gbnAuemVyb3MoaW5wdXRfc2l6ZSArIDEpICAjICsxIHBlciBhbCBiaWFzCiAgICAgICAgc2VsZi5sZWFybmluZ19yYXRlID0gbGVhcm5pbmdfcmF0ZQoKICAgIGRlZiBwcmVkaWN0KHNlbGYsIHgpOgogICAgICAgIHggPSBucC5pbnNlcnQoeCwgMCwgMSkgICMgSW5zZXJpciBlbCBiaWFzCiAgICAgICAgc3VtbWF0aW9uID0gbnAuZG90KHNlbGYud2VpZ2h0cywgeCkKICAgICAgICByZXR1cm4gc3RlcF9mdW5jdGlvbihzdW1tYXRpb24pCgogICAgZGVmIHRyYWluKHNlbGYsIHRyYWluaW5nX2RhdGEsIGxhYmVscywgZXBvY2hzPTEwKToKICAgICAgICBmb3IgXyBpbiByYW5nZShlcG9jaHMpOgogICAgICAgICAgICBmb3IgeCwgbGFiZWwgaW4gemlwKHRyYWluaW5nX2RhdGEsIGxhYmVscyk6CiAgICAgICAgICAgICAgICBwcmVkaWN0aW9uID0gc2VsZi5wcmVkaWN0KHgpCiAgICAgICAgICAgICAgICBzZWxmLndlaWdodHMgKz0gc2VsZi5sZWFybmluZ19yYXRlICogKGxhYmVsIC0gcHJlZGljdGlvbikgKiBucC5pbnNlcnQoeCwgMCwgMSkKCiMgRGFkZXMgZCdlbnRyZW5hbWVudCAoQU5EIGzDsmdpYykKdHJhaW5pbmdfZGF0YSA9IG5wLmFycmF5KFtbMCwgMF0sIFswLCAxXSwgWzEsIDBdLCBbMSwgMV1dKQpsYWJlbHMgPSBucC5hcnJheShbMCwgMCwgMCwgMV0pCgojIENyZWFyIGkgZW50cmVuYXIgZWwgcGVyY2VwdHLDswpwZXJjZXB0cm9uID0gUGVyY2VwdHJvbihpbnB1dF9zaXplPTIpCnBlcmNlcHRyb24udHJhaW4odHJhaW5pbmdfZGF0YSwgbGFiZWxzKQoKIyBQcm92YXIgZWwgcGVyY2VwdHLDswpmb3IgeCBpbiB0cmFpbmluZ19kYXRhOgogICAgcHJpbnQoZiJFbnRyYWRhOiB7eH0sIFByZWRpY2Npw7M6IHtwZXJjZXB0cm9uLnByZWRpY3QoeCl9Iik="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Funci&oacute; d'activaci&oacute; (esgla&oacute;)
def step_function(x):
    return 1 if x &gt;= 0 else 0

# Perceptr&oacute; simple
class Perceptron:
    def __init__(self, input_size, learning_rate=0.1):
        self.weights = np.zeros(input_size + 1)  # +1 per al bias
        self.learning_rate = learning_rate

    def predict(self, x):
        x = np.insert(x, 0, 1)  # Inserir el bias
        summation = np.dot(self.weights, x)
        return step_function(summation)

    def train(self, training_data, labels, epochs=10):
        for _ in range(epochs):
            for x, label in zip(training_data, labels):
                prediction = self.predict(x)
                self.weights += self.learning_rate * (label - prediction) * np.insert(x, 0, 1)

# Dades d'entrenament (AND l&ograve;gic)
training_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
labels = np.array([0, 0, 0, 1])

# Crear i entrenar el perceptr&oacute;
perceptron = Perceptron(input_size=2)
perceptron.train(training_data, labels)

# Provar el perceptr&oacute;
for x in training_data:
    print(f&quot;Entrada: {x}, Predicci&oacute;: {perceptron.predict(x)}&quot;)</pre></div><div class='content'></div><h1>Perceptró Multicapa (MLP)</h1>
<div class='content'><p>El Perceptró Multicapa (MLP) és una extensió del Perceptró simple i pot resoldre problemes no lineals. Consisteix en múltiples capes de neurones: una capa d'entrada, una o més capes ocultes i una capa de sortida.</p>
</div><h2>Conceptes Clau</h2>
<div class='content'><ul>
<li><strong>Capes Ocultes</strong>: Capes intermèdies entre l'entrada i la sortida que permeten la modelització de relacions no lineals.</li>
<li><strong>Backpropagation</strong>: Algorisme per entrenar xarxes neuronals ajustant els pesos mitjançant el càlcul del gradient de l'error.</li>
</ul>
</div><h2>Estructura del MLP</h2>
<div class='content'><p>Un MLP amb una capa oculta pot ser representat com:</p>
<p>\[ y = f_2(\sum_{j=1}^{m} w_{2j} f_1(\sum_{i=1}^{n} w_{1ij} x_i + b_{1j}) + b_2) \]</p>
<p>On:</p>
<ul>
<li>\( f_1 \) i \( f_2 \) són funcions d'activació per a la capa oculta i la capa de sortida, respectivament.</li>
<li>\( w_{1ij} \) i \( w_{2j} \) són els pesos de la primera i segona capa.</li>
<li>\( b_{1j} \) i \( b_2 \) són els bias de la primera i segona capa.</li>
</ul>
</div><h2>Exemple de Codi</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Funci&oacute; d'activaci&oacute; (sigmoide)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivada de la funci&oacute; sigmoide
def sigmoid_derivative(x):
    return x * (1 - x)

# Perceptr&oacute; Multicapa
class MLP:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):
        self.weights_input_hidden = np.random.rand(input_size, hidden_size)
        self.weights_hidden_output = np.random.rand(hidden_size, output_size)
        self.learning_rate = learning_rate

    def forward(self, x):
        self.hidden_input = np.dot(x, self.weights_input_hidden)
        self.hidden_output = sigmoid(self.hidden_input)
        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output)
        self.final_output = sigmoid(self.final_input)
        return self.final_output

    def backward(self, x, y, output):
        error = y - output
        d_output = error * sigmoid_derivative(output)
        error_hidden_layer = d_output.dot(self.weights_hidden_output.T)
        d_hidden_layer = error_hidden_layer * sigmoid_derivative(self.hidden_output)

        self.weights_hidden_output += self.hidden_output.T.dot(d_output) * self.learning_rate
        self.weights_input_hidden += x.T.dot(d_hidden_layer) * self.learning_rate

    def train(self, training_data, labels, epochs=10000):
        for _ in range(epochs):
            for x, y in zip(training_data, labels):
                output = self.forward(x)
                self.backward(x, y, output)

# Dades d'entrenament (XOR l&ograve;gic)
training_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
labels = np.array([[0], [1], [1], [0]])

# Crear i entrenar el MLP
mlp = MLP(input_size=2, hidden_size=2, output_size=1)
mlp.train(training_data, labels)

# Provar el MLP
for x in training_data:
    print(f&quot;Entrada: {x}, Predicci&oacute;: {mlp.forward(x)}&quot;)</pre></div><div class='content'></div><h1>Conclusió</h1>
<div class='content'><p>En aquesta secció, hem cobert els conceptes bàsics del Perceptró i el Perceptró Multicapa. El Perceptró és un classificador lineal simple, mentre que el MLP pot manejar problemes no lineals mitjançant la inclusió de capes ocultes i l'ús de l'algorisme de backpropagation. Aquests models són fonamentals per entendre xarxes neuronals més complexes i són la base de molts algorismes de Deep Learning avançats.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicitat</h1>
			<p>Aquest espai està destinat a publicitat.</p>
			<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Gràcies per col·laborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
       <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
       <a href="cookies">Més informació</a>
   </div>	
	</div>    
</body>
</html>
