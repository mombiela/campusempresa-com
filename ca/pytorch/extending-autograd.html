<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ampliant Autograd</title>

    <link rel="alternate" href="https://campusempresa.com/es/pytorch/extending-autograd" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/pytorch/extending-autograd" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/pytorch/extending-autograd" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/ca/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construint la societat d'avui i del demà</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
							<a href="/es/pytorch/extending-autograd" id="lnk_lang_es" data-lang="es" class="px-2">ES</a></b>
				|
				<b id="lit_lang_ca" class="px-2">CA</b>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/ca/objective">El Projecte</a>
				<a href="/ca/about">Sobre nosaltres</a>
				<a href="/ca/contribute">Contribuir</a>
				<a href="/ca/donate">Donacions</a>
				<a href="/ca/licence">Llicència</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'><div class='col-6'><a href='custom-layers-and-modules'>&#x25C4; Capes i Mòduls Personalitzats</a></div><div class='col-6 text-end'><a href='pytorch-lightning'>PyTorch Lightning &#x25BA;</a></div></div><div class='content'><p>En aquesta secció, explorarem com ampliar la funcionalitat d'Autograd de PyTorch. Autograd és el motor de diferenciació automàtica de PyTorch que impulsa l'entrenament de xarxes neuronals. Ampliant Autograd, pots definir operacions i gradients personalitzats, cosa que pot ser útil per implementar nous algoritmes o optimitzar parts específiques del teu model.</p>
</div><h1>Conceptes Clau</h1>
<div class='content'><ul>
<li><strong>Autograd</strong>: El motor de diferenciació automàtica de PyTorch.</li>
<li><strong>Funcions Autograd Personalitzades</strong>: Operacions definides per l'usuari amb passos de forward i backward personalitzats.</li>
<li><strong>Tensors</strong>: L'estructura de dades fonamental a PyTorch, que suporta la diferenciació automàtica.</li>
</ul>
</div><h1>Funcions Autograd Personalitzades</h1>
<div class='content'><p>Per ampliar Autograd, necessites crear funcions autograd personalitzades. Aquestes funcions et permeten definir tant els passos de forward com de backward d'una operació. Això és particularment útil quan necessites més control sobre el càlcul del gradient.</p>
</div><h2>Creant una Funció Autograd Personalitzada</h2>
<div class='content'><p>Per crear una funció autograd personalitzada, necessites subclassificar <code>torch.autograd.Function</code> i implementar dos mètodes estàtics: <code>forward</code> i <code>backward</code>.</p>
<h4>Exemple: Funció Quadrat Personalitzada</h4>
<p>Creem una funció personalitzada que calcula el quadrat de la seva entrada i el seu gradient.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgpjbGFzcyBTcXVhcmVGdW5jdGlvbih0b3JjaC5hdXRvZ3JhZC5GdW5jdGlvbik6CiAgICBAc3RhdGljbWV0aG9kCiAgICBkZWYgZm9yd2FyZChjdHgsIGlucHV0KToKICAgICAgICAjIEd1YXJkYXIgY29udGV4dCBwZXIgYWwgcGFzIGJhY2t3YXJkCiAgICAgICAgY3R4LnNhdmVfZm9yX2JhY2t3YXJkKGlucHV0KQogICAgICAgIHJldHVybiBpbnB1dCAqKiAyCgogICAgQHN0YXRpY21ldGhvZAogICAgZGVmIGJhY2t3YXJkKGN0eCwgZ3JhZF9vdXRwdXQpOgogICAgICAgICMgUmVjdXBlcmFyIHRlbnNvciBndWFyZGF0CiAgICAgICAgaW5wdXQsID0gY3R4LnNhdmVkX3RlbnNvcnMKICAgICAgICAjIENhbGN1bGFyIGdyYWRpZW50CiAgICAgICAgZ3JhZF9pbnB1dCA9IGdyYWRfb3V0cHV0ICogMiAqIGlucHV0CiAgICAgICAgcmV0dXJuIGdyYWRfaW5wdXQKCiMgw5pzCnggPSB0b3JjaC50ZW5zb3IoWzIuMCwgMy4wXSwgcmVxdWlyZXNfZ3JhZD1UcnVlKQpzcXVhcmUgPSBTcXVhcmVGdW5jdGlvbi5hcHBseQp5ID0gc3F1YXJlKHgpCnkuYmFja3dhcmQodG9yY2gudGVuc29yKFsxLjAsIDEuMF0pKQoKcHJpbnQoeC5ncmFkKSAgIyBTb3J0aWRhOiB0ZW5zb3IoWzQuLCA2Ll0p"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

class SquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        # Guardar context per al pas backward
        ctx.save_for_backward(input)
        return input ** 2

    @staticmethod
    def backward(ctx, grad_output):
        # Recuperar tensor guardat
        input, = ctx.saved_tensors
        # Calcular gradient
        grad_input = grad_output * 2 * input
        return grad_input

# &Uacute;s
x = torch.tensor([2.0, 3.0], requires_grad=True)
square = SquareFunction.apply
y = square(x)
y.backward(torch.tensor([1.0, 1.0]))

print(x.grad)  # Sortida: tensor([4., 6.])</pre></div><div class='content'><p>En aquest exemple:</p>
<ul>
<li>El mètode <code>forward</code> calcula el quadrat del tensor d'entrada i el guarda per al pas backward.</li>
<li>El mètode <code>backward</code> recupera el tensor d'entrada guardat i calcula el gradient.</li>
</ul>
</div><h2>Utilitzant Funcions Autograd Personalitzades</h2>
<div class='content'><p>Pots utilitzar funcions autograd personalitzades igual que qualsevol altra operació de PyTorch. La clau és cridar el mètode <code>apply</code> de la teva funció personalitzada.</p>
<h4>Exemple: Funció ReLU Personalitzada</h4>
<p>Creem una funció ReLU (Rectified Linear Unit) personalitzada.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgUmVMVUZ1bmN0aW9uKHRvcmNoLmF1dG9ncmFkLkZ1bmN0aW9uKToKICAgIEBzdGF0aWNtZXRob2QKICAgIGRlZiBmb3J3YXJkKGN0eCwgaW5wdXQpOgogICAgICAgIGN0eC5zYXZlX2Zvcl9iYWNrd2FyZChpbnB1dCkKICAgICAgICByZXR1cm4gaW5wdXQuY2xhbXAobWluPTApCgogICAgQHN0YXRpY21ldGhvZAogICAgZGVmIGJhY2t3YXJkKGN0eCwgZ3JhZF9vdXRwdXQpOgogICAgICAgIGlucHV0LCA9IGN0eC5zYXZlZF90ZW5zb3JzCiAgICAgICAgZ3JhZF9pbnB1dCA9IGdyYWRfb3V0cHV0LmNsb25lKCkKICAgICAgICBncmFkX2lucHV0W2lucHV0IDwgMF0gPSAwCiAgICAgICAgcmV0dXJuIGdyYWRfaW5wdXQKCiMgw5pzCnggPSB0b3JjaC50ZW5zb3IoWy0xLjAsIDIuMCwgLTMuMCwgNC4wXSwgcmVxdWlyZXNfZ3JhZD1UcnVlKQpyZWx1ID0gUmVMVUZ1bmN0aW9uLmFwcGx5CnkgPSByZWx1KHgpCnkuYmFja3dhcmQodG9yY2gudGVuc29yKFsxLjAsIDEuMCwgMS4wLCAxLjBdKSkKCnByaW50KHguZ3JhZCkgICMgU29ydGlkYTogdGVuc29yKFswLiwgMS4sIDAuLCAxLl0p"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class ReLUFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input &lt; 0] = 0
        return grad_input

# &Uacute;s
x = torch.tensor([-1.0, 2.0, -3.0, 4.0], requires_grad=True)
relu = ReLUFunction.apply
y = relu(x)
y.backward(torch.tensor([1.0, 1.0, 1.0, 1.0]))

print(x.grad)  # Sortida: tensor([0., 1., 0., 1.])</pre></div><div class='content'><p>En aquest exemple:</p>
<ul>
<li>El mètode <code>forward</code> aplica l'operació ReLU.</li>
<li>El mètode <code>backward</code> calcula el gradient, establint-lo a zero on l'entrada era negativa.</li>
</ul>
</div><h1>Conclusió</h1>
<div class='content'><p>Ampliant Autograd a PyTorch et permet definir operacions personalitzades amb passos de forward i backward específics. Això pot ser particularment útil per implementar nous algoritmes o optimitzar certes parts del teu model. Subclassificant <code>torch.autograd.Function</code> i implementant els mètodes <code>forward</code> i <code>backward</code>, obtens control total sobre el càlcul i el flux de gradients, permetent més flexibilitat i personalització en els teus models d'aprenentatge profund.</p>
<p>En les properes seccions, aprofundirem en temes més avançats i aplicacions de funcions autograd personalitzades, incloent el seu ús en arquitectures complexes de xarxes neuronals i rutines d'optimització.</p>
</div><div class='row navigation'><div class='col-6'><a href='custom-layers-and-modules'>&#x25C4; Capes i Mòduls Personalitzats</a></div><div class='col-6 text-end'><a href='pytorch-lightning'>PyTorch Lightning &#x25BA;</a></div></div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicitat</h1>
			<p>Aquest espai està destinat a publicitat.</p>
			<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Gràcies per col·laborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/ca/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
