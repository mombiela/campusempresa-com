<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Entrenament Distribuït</title>

    <link rel="alternate" href="https://campusempresa.com/es/tensorflow/distributed-training" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/tensorflow/distributed-training" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/tensorflow/distributed-training" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/ca/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construint la societat d'avui<br> i del demà</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
							<a href="/es/tensorflow/distributed-training" id="lnk_lang_es" data-lang="es" class="px-2">ES</a></b>
				|
				<b id="lit_lang_ca" class="px-2">CA</b>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="objective">El Projecte</a>
				<a href="about">Sobre nosaltres</a>
				<a href="contribute">Contribuir</a>
				<a href="donate">Donacions</a>
				<a href="licence">Llicència</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'><p>L'entrenament distribuït és una tècnica utilitzada per entrenar models d'aprenentatge automàtic a través de múltiples dispositius o màquines. Aquest enfocament pot accelerar significativament el procés d'entrenament i gestionar conjunts de dades més grans que no cabrien a la memòria d'un sol dispositiu. TensorFlow proporciona un suport robust per a l'entrenament distribuït, facilitant l'escalabilitat dels teus models.</p>
</div><h1>Introducció a l'Entrenament Distribuït</h1>
<div class='content'><ul>
<li><strong>Definició</strong>: L'entrenament distribuït implica dividir la càrrega de treball d'entrenament entre múltiples dispositius o màquines.</li>
<li><strong>Beneficis</strong>:
<ul>
<li><strong>Velocitat</strong>: Temps d'entrenament més ràpids aprofitant múltiples processadors.</li>
<li><strong>Escalabilitat</strong>: Capacitat per entrenar amb conjunts de dades més grans.</li>
<li><strong>Utilització de Recursos</strong>: Ús eficient del maquinari disponible.</li>
</ul>
</li>
</ul>
</div><h1>Tipus d'Entrenament Distribuït</h1>
<div class='content'></div><h2>Paral·lelisme de Dades</h2>
<div class='content'><ul>
<li><strong>Concepte</strong>: Cada dispositiu obté una porció diferent de les dades però el mateix model.</li>
<li><strong>Mecanisme</strong>: Els gradients es promiguen entre els dispositius, i els pesos del model s'actualitzen de manera síncrona.</li>
<li><strong>Exemple</strong>:
<pre><code class="language-python">import tensorflow as tf

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=['accuracy'])

# Carregar i preprocessar dades
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# Entrenar el model
model.fit(x_train, y_train, epochs=5)
</code></pre>
</li>
</ul>
</div><h2>Paral·lelisme de Models</h2>
<div class='content'><ul>
<li><strong>Concepte</strong>: Diferents parts del model es distribueixen entre múltiples dispositius.</li>
<li><strong>Mecanisme</strong>: Cada dispositiu calcula una porció del model, i els resultats es combinen.</li>
<li><strong>Cas d'Ús</strong>: Útil quan el model és massa gran per cabre a la memòria d'un sol dispositiu.</li>
</ul>
</div><h1>Estratègies de TensorFlow per a l'Entrenament Distribuït</h1>
<div class='content'></div><h2>MirroredStrategy</h2>
<div class='content'><ul>
<li><strong>Descripció</strong>: Entrenament síncron en múltiples GPUs en una màquina.</li>
<li><strong>Ús</strong>:
<pre><code class="language-python">strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    # Defineix i compila el teu model aquí
</code></pre>
</li>
</ul>
</div><h2>MultiWorkerMirroredStrategy</h2>
<div class='content'><ul>
<li><strong>Descripció</strong>: Entrenament síncron en múltiples màquines.</li>
<li><strong>Ús</strong>:
<pre><code class="language-python">strategy = tf.distribute.MultiWorkerMirroredStrategy()
with strategy.scope():
    # Defineix i compila el teu model aquí
</code></pre>
</li>
</ul>
</div><h2>TPUStrategy</h2>
<div class='content'><ul>
<li><strong>Descripció</strong>: Entrenament en dispositius TPU (Unitat de Processament Tensorial).</li>
<li><strong>Ús</strong>:
<pre><code class="language-python">resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='your-tpu-address')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)

strategy = tf.distribute.TPUStrategy(resolver)
with strategy.scope():
    # Defineix i compila el teu model aquí
</code></pre>
</li>
</ul>
</div><h1>Exemple Pràctic: Entrenament Distribuït amb MirroredStrategy</h1>
<div class='content'></div><h2>Guia Pas a Pas</h2>
<div class='content'><ol>
<li>
<p><strong>Configura l'Estratègia</strong>:</p>
<pre><code class="language-python">import tensorflow as tf

strategy = tf.distribute.MirroredStrategy()
</code></pre>
</li>
<li>
<p><strong>Defineix el Model dins de l'Àmbit de l'Estratègia</strong>:</p>
<pre><code class="language-python">with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=['accuracy'])
</code></pre>
</li>
<li>
<p><strong>Carrega i Preprocessa les Dades</strong>:</p>
<pre><code class="language-python">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
</code></pre>
</li>
<li>
<p><strong>Entrena el Model</strong>:</p>
<pre><code class="language-python">model.fit(x_train, y_train, epochs=5)
</code></pre>
</li>
</ol>
</div><h2>Explicació</h2>
<div class='content'><ul>
<li><strong>Configuració de l'Estratègia</strong>: <code>MirroredStrategy</code> s'utilitza per a l'entrenament síncron en múltiples GPUs.</li>
<li><strong>Definició del Model</strong>: El model es defineix dins de l'àmbit de l'estratègia per assegurar que es distribueixi entre les GPUs disponibles.</li>
<li><strong>Càrrega de Dades</strong>: Es carrega i normalitza el conjunt de dades MNIST.</li>
<li><strong>Entrenament</strong>: El model s'entrena utilitzant el mètode <code>fit</code>, que distribuirà l'entrenament entre les GPUs.</li>
</ul>
</div><h1>Conclusió</h1>
<div class='content'><p>L'entrenament distribuït en TensorFlow et permet escalar els teus models i accelerar el procés d'entrenament aprofitant múltiples dispositius. Entenent i utilitzant diferents estratègies com <code>MirroredStrategy</code>, <code>MultiWorkerMirroredStrategy</code> i <code>TPUStrategy</code>, pots entrenar models grans de manera eficient amb conjunts de dades extensos. Comença amb estratègies simples i gradualment passa a configuracions més complexes a mesura que creixin les teves necessitats.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicitat</h1>
			<p>Aquest espai està destinat a publicitat.</p>
			<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Gràcies per col·laborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
