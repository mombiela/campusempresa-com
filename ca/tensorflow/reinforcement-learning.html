<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aprenentatge per Reforç amb TensorFlow</title>

    <link rel="alternate" href="https://campusempresa.com/es/tensorflow/reinforcement-learning" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/tensorflow/reinforcement-learning" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/tensorflow/reinforcement-learning" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/ca/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construint la societat d'avui i del demà</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
							<a href="/es/tensorflow/reinforcement-learning" id="lnk_lang_es" data-lang="es" class="px-2">ES</a></b>
				|
				<b id="lit_lang_ca" class="px-2">CA</b>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/ca/objective">El Projecte</a>
				<a href="/ca/about">Sobre nosaltres</a>
				<a href="/ca/contribute">Contribuir</a>
				<a href="/ca/donate">Donacions</a>
				<a href="/ca/licence">Llicència</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'><div class='col-6'><a href='autoencoders'>&#x25C4; Autoencoders</a></div><div class='col-6 text-end'><a href='image-classification-project'>Projecte de classificació d'imatges &#x25BA;</a></div></div><div class='content'></div><h1>Introducció a l'Aprenentatge per Reforç</h1>
<div class='content'><p>L'Aprenentatge per Reforç (RL) és un tipus d'aprenentatge automàtic on un agent aprèn a prendre decisions realitzant accions en un entorn per maximitzar la recompensa acumulada.</p>
<ul>
<li><strong>Agent</strong>: L'aprenent o el que pren decisions.</li>
<li><strong>Entorn</strong>: El sistema extern amb el qual l'agent interactua.</li>
<li><strong>Acció</strong>: El que l'agent pot fer.</li>
<li><strong>Estat</strong>: La situació actual de l'agent.</li>
<li><strong>Recompensa</strong>: El feedback de l'entorn basat en l'acció presa.</li>
</ul>
</div><h1>Conceptes Clau en l'Aprenentatge per Reforç</h1>
<div class='content'><ul>
<li><strong>Política</strong>: Una estratègia utilitzada per l'agent per decidir quines accions prendre.</li>
<li><strong>Funció de Valor</strong>: Una funció que estima la recompensa esperada dels estats o parelles estat-acció.</li>
<li><strong>Model</strong>: La representació de l'agent de l'entorn.</li>
</ul>
</div><h1>Configurant TensorFlow per a l'Aprenentatge per Reforç</h1>
<div class='content'><p>Primer, assegura't que tens TensorFlow instal·lat. Pots instal·lar-lo utilitzant pip:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgdGVuc29yZmxvdw=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install tensorflow</pre></div><div class='content'></div><h1>Construint un Model RL Simple amb TensorFlow</h1>
<h2>Pas 1: Definir l'Entorn</h2>
<div class='content'><p>Utilitzarem la biblioteca Gym d'OpenAI per crear un entorn simple. Instal·la Gym utilitzant pip:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgZ3lt"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install gym</pre></div><div class='content'></div><h2>Pas 2: Crear l'Entorn</h2>
<div class='content'><p>Aquí tens un exemple de creació d'un entorn simple CartPole:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGd5bQoKZW52ID0gZ3ltLm1ha2UoJ0NhcnRQb2xlLXYxJykKc3RhdGUgPSBlbnYucmVzZXQoKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import gym

env = gym.make('CartPole-v1')
state = env.reset()</pre></div><div class='content'></div><h2>Pas 3: Definir la Xarxa Neuronal</h2>
<div class='content'><p>Utilitzarem TensorFlow per definir una xarxa neuronal simple que actuarà com la nostra política.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzIGltcG9ydCBsYXllcnMKCm1vZGVsID0gdGYua2VyYXMuU2VxdWVudGlhbChbCiAgICBsYXllcnMuRGVuc2UoMjQsIGFjdGl2YXRpb249J3JlbHUnLCBpbnB1dF9zaGFwZT0oZW52Lm9ic2VydmF0aW9uX3NwYWNlLnNoYXBlWzBdLCkpLAogICAgbGF5ZXJzLkRlbnNlKDI0LCBhY3RpdmF0aW9uPSdyZWx1JyksCiAgICBsYXllcnMuRGVuc2UoZW52LmFjdGlvbl9zcGFjZS5uLCBhY3RpdmF0aW9uPSdsaW5lYXInKQpdKQoKbW9kZWwuY29tcGlsZShvcHRpbWl6ZXI9dGYua2VyYXMub3B0aW1pemVycy5BZGFtKGxlYXJuaW5nX3JhdGU9MC4wMDEpLCBsb3NzPSdtc2UnKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras import layers

model = tf.keras.Sequential([
    layers.Dense(24, activation='relu', input_shape=(env.observation_space.shape[0],)),
    layers.Dense(24, activation='relu'),
    layers.Dense(env.action_space.n, activation='linear')
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')</pre></div><div class='content'></div><h2>Pas 4: Implementar el Bucle d'Entrenament</h2>
<div class='content'><p>Implementarem un bucle d'entrenament simple on l'agent interactua amb l'entorn i aprèn de les recompenses.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgdHJhaW5fbW9kZWwoZW52LCBtb2RlbCwgZXBpc29kZXM9MTAwMCwgZ2FtbWE9MC45OSk6CiAgICBmb3IgZXBpc29kZSBpbiByYW5nZShlcGlzb2Rlcyk6CiAgICAgICAgc3RhdGUgPSBlbnYucmVzZXQoKQogICAgICAgIHRvdGFsX3Jld2FyZCA9IDAKICAgICAgICBkb25lID0gRmFsc2UKCiAgICAgICAgd2hpbGUgbm90IGRvbmU6CiAgICAgICAgICAgIHN0YXRlID0gc3RhdGUucmVzaGFwZShbMSwgc3RhdGUuc2hhcGVbMF1dKQogICAgICAgICAgICBxX3ZhbHVlcyA9IG1vZGVsLnByZWRpY3Qoc3RhdGUpCiAgICAgICAgICAgIGFjdGlvbiA9IG5wLmFyZ21heChxX3ZhbHVlc1swXSkKCiAgICAgICAgICAgIG5leHRfc3RhdGUsIHJld2FyZCwgZG9uZSwgXyA9IGVudi5zdGVwKGFjdGlvbikKICAgICAgICAgICAgdG90YWxfcmV3YXJkICs9IHJld2FyZAoKICAgICAgICAgICAgbmV4dF9zdGF0ZSA9IG5leHRfc3RhdGUucmVzaGFwZShbMSwgbmV4dF9zdGF0ZS5zaGFwZVswXV0pCiAgICAgICAgICAgIHFfdmFsdWVzX25leHQgPSBtb2RlbC5wcmVkaWN0KG5leHRfc3RhdGUpCiAgICAgICAgICAgIHFfdmFsdWVzWzBdW2FjdGlvbl0gPSByZXdhcmQgKyBnYW1tYSAqIG5wLm1heChxX3ZhbHVlc19uZXh0WzBdKQoKICAgICAgICAgICAgbW9kZWwuZml0KHN0YXRlLCBxX3ZhbHVlcywgdmVyYm9zZT0wKQoKICAgICAgICAgICAgc3RhdGUgPSBuZXh0X3N0YXRlCgogICAgICAgIHByaW50KGYiRXBpc29kaToge2VwaXNvZGUgKyAxfSwgUmVjb21wZW5zYSBUb3RhbDoge3RvdGFsX3Jld2FyZH0iKQoKZW52ID0gZ3ltLm1ha2UoJ0NhcnRQb2xlLXYxJykKdHJhaW5fbW9kZWwoZW52LCBtb2RlbCk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def train_model(env, model, episodes=1000, gamma=0.99):
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            state = state.reshape([1, state.shape[0]])
            q_values = model.predict(state)
            action = np.argmax(q_values[0])

            next_state, reward, done, _ = env.step(action)
            total_reward += reward

            next_state = next_state.reshape([1, next_state.shape[0]])
            q_values_next = model.predict(next_state)
            q_values[0][action] = reward + gamma * np.max(q_values_next[0])

            model.fit(state, q_values, verbose=0)

            state = next_state

        print(f&quot;Episodi: {episode + 1}, Recompensa Total: {total_reward}&quot;)

env = gym.make('CartPole-v1')
train_model(env, model)</pre></div><div class='content'></div><h1>Temes Avançats en l'Aprenentatge per Reforç</h1>
<h2>Deep Q-Learning</h2>
<div class='content'><p>Deep Q-Learning (DQN) utilitza una xarxa neuronal per aproximar la funció de valor Q.</p>
<ul>
<li><strong>Reproducció d'Experiències</strong>: Emmagatzema les experiències de l'agent per trencar la correlació entre mostres consecutives.</li>
<li><strong>Xarxa Objectiu</strong>: Una xarxa separada per estabilitzar l'entrenament.</li>
</ul>
</div><h2>Mètodes de Gradient de Política</h2>
<div class='content'><p>Els mètodes de gradient de política optimitzen la política directament.</p>
<ul>
<li><strong>Algorisme REINFORCE</strong>: Un mètode de gradient de política de Monte Carlo.</li>
<li><strong>Mètodes Actor-Critic</strong>: Combina mètodes basats en valor i basats en política.</li>
</ul>
</div><h2>Exemple de DQN amb TensorFlow</h2>
<div class='content'><p>Aquí tens un exemple més avançat utilitzant DQN amb reproducció d'experiències i una xarxa objectiu.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import random
from collections import deque

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        self.learning_rate = 0.001
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_model()

    def _build_model(self):
        model = tf.keras.Sequential([
            layers.Dense(24, activation='relu', input_shape=(self.state_size,)),
            layers.Dense(24, activation='relu'),
            layers.Dense(self.action_size, activation='linear')
        ])
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')
        return model

    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() &lt;= self.epsilon:
            return random.randrange(self.action_size)
        q_values = self.model.predict(state)
        return np.argmax(q_values[0])

    def replay(self, batch_size):
        minibatch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])
            target_f = self.model.predict(state)
            target_f[0][action] = target
            self.model.fit(state, target_f, epochs=1, verbose=0)
        if self.epsilon &gt; self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def load(self, name):
        self.model.load_weights(name)

    def save(self, name):
        self.model.save_weights(name)

env = gym.make('CartPole-v1')
state_size = env.observation_space.shape[0]
action_size = env.action_space.n
agent = DQNAgent(state_size, action_size)
episodes = 1000
batch_size = 32

for e in range(episodes):
    state = env.reset()
    state = np.reshape(state, [1, state_size])
    for time in range(500):
        action = agent.act(state)
        next_state, reward, done, _ = env.step(action)
        reward = reward si no done else -10
        next_state = np.reshape(next_state, [1, state_size])
        agent.remember(state, action, reward, next_state, done)
        state = next_state
        if done:
            agent.update_target_model()
            print(f&quot;Episodi: {e + 1}/{episodes}, Puntuaci&oacute;: {time}, Epsilon: {agent.epsilon:.2}&quot;)
            break
        if len(agent.memory) &gt; batch_size:
            agent.replay(batch_size)</pre></div><div class='content'></div><h1>Conclusió</h1>
<div class='content'><p>L'Aprenentatge per Reforç és un paradigma poderós per entrenar agents a prendre decisions. TensorFlow proporciona eines robustes per implementar tant algorismes RL simples com avançats. Entenent els conceptes clau i seguint exemples estructurats, pots construir i entrenar els teus propis models RL. Continua explorant entorns i algorismes més complexos per aprofundir en la teva comprensió i habilitats en RL amb TensorFlow.</p>
</div><div class='row navigation'><div class='col-6'><a href='autoencoders'>&#x25C4; Autoencoders</a></div><div class='col-6 text-end'><a href='image-classification-project'>Projecte de classificació d'imatges &#x25BA;</a></div></div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicitat</h1>
			<p>Aquest espai està destinat a publicitat.</p>
			<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Gràcies per col·laborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/ca/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
