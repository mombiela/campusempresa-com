<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reducció de Dimensionalitat</title>

    <link rel="alternate" href="https://campusempresa.com/es/machine_learning/reduccion-dimensionalidad" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/machine_learning/reduccion-dimensionalidad" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/machine_learning/reduccion-dimensionalidad" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/ca/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construint la societat d'avui<br> i del demà</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
							<a href="/es/machine_learning/reduccion-dimensionalidad" id="lnk_lang_es" data-lang="es" class="px-2">ES</a></b>
				|
				<b id="lit_lang_ca" class="px-2">CA</b>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="objective">El Projecte</a>
				<a href="about">Sobre nosaltres</a>
				<a href="contribute">Contribuir</a>
				<a href="donate">Donacions</a>
				<a href="licence">Llicència</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'></div><h1>Introducció</h1>
<div class='content'><p>La reducció de dimensionalitat és una tècnica crucial en Machine Learning que s'utilitza per simplificar models, millorar l'eficiència computacional i reduir el soroll en les dades. Aquest procés implica transformar dades d'alta dimensionalitat en un espai de menor dimensió, mantenint la major quantitat d'informació rellevant possible.</p>
</div><h1>Importància de la Reducció de Dimensionalitat</h1>
<div class='content'><ul>
<li><strong>Millora del rendiment del model</strong>: En reduir el nombre de característiques, es pot evitar el sobreajustament i millorar la generalització del model.</li>
<li><strong>Eficiència computacional</strong>: Menys característiques signifiquen menys càlculs, cosa que accelera l'entrenament i la predicció.</li>
<li><strong>Visualització</strong>: Facilita la visualització de dades complexes en 2D o 3D.</li>
<li><strong>Eliminació de soroll</strong>: Ajuda a eliminar característiques irrellevants o redundants que poden introduir soroll en el model.</li>
</ul>
</div><h1>Tècniques de Reducció de Dimensionalitat</h1>
<div class='content'></div><h2>Anàlisi de Components Principals (PCA)</h2>
<div class='content'><p>El PCA és una tècnica estadística que transforma les dades a un nou sistema de coordenades, on les noves variables (components principals) són combinacions lineals de les variables originals i estan ordenades per la quantitat de variància que expliquen.</p>
<h4>Passos per realitzar PCA:</h4>
<ol>
<li><strong>Estandardització de les dades</strong>: Normalitzar les dades perquè cada característica tingui una mitjana de 0 i una desviació estàndard d'1.</li>
<li><strong>Càlcul de la matriu de covariància</strong>: Determinar la relació entre les característiques.</li>
<li><strong>Càlcul dels valors i vectors propis</strong>: Identificar les direccions principals de la variància.</li>
<li><strong>Selecció de components principals</strong>: Triar els components que expliquen la major part de la variància.</li>
<li><strong>Transformació de les dades</strong>: Projectar les dades originals en el nou espai de components principals.</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmZyb20gc2tsZWFybi5kZWNvbXBvc2l0aW9uIGltcG9ydCBQQ0EKZnJvbSBza2xlYXJuLnByZXByb2Nlc3NpbmcgaW1wb3J0IFN0YW5kYXJkU2NhbGVyCgojIERhZGVzIGQnZXhlbXBsZQpYID0gbnAuYXJyYXkoW1syLjUsIDIuNF0sIFswLjUsIDAuN10sIFsyLjIsIDIuOV0sIFsxLjksIDIuMl0sIFszLjEsIDMuMF0sIFsyLjMsIDIuN10sIFsyLCAxLjZdLCBbMSwgMS4xXSwgWzEuNSwgMS42XSwgWzEuMSwgMC45XV0pCgojIEVzdGFuZGFyZGl0emFjacOzIGRlIGxlcyBkYWRlcwpzY2FsZXIgPSBTdGFuZGFyZFNjYWxlcigpClhfc2NhbGVkID0gc2NhbGVyLmZpdF90cmFuc2Zvcm0oWCkKCiMgQXBsaWNhY2nDsyBkZSBQQ0EKcGNhID0gUENBKG5fY29tcG9uZW50cz0yKQpYX3BjYSA9IHBjYS5maXRfdHJhbnNmb3JtKFhfc2NhbGVkKQoKcHJpbnQoIkNvbXBvbmVudHMgcHJpbmNpcGFsczpcbiIsIFhfcGNhKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Dades d'exemple
X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])

# Estandarditzaci&oacute; de les dades
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicaci&oacute; de PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print(&quot;Components principals:\n&quot;, X_pca)</pre></div><div class='content'></div><h2>Anàlisi Discriminant Lineal (LDA)</h2>
<div class='content'><p>El LDA és una tècnica supervisada que busca maximitzar la separació entre múltiples classes. A diferència del PCA, que no considera l'etiqueta de classe, el LDA utilitza la informació de classe per trobar les direccions que maximitzen la separació entre classes.</p>
<h4>Passos per realitzar LDA:</h4>
<ol>
<li><strong>Calcular la mitjana de cada classe</strong>.</li>
<li><strong>Calcular la matriu de dispersió dins de la classe i entre classes</strong>.</li>
<li><strong>Calcular els valors i vectors propis</strong>.</li>
<li><strong>Seleccionar els vectors propis que maximitzen la separació entre classes</strong>.</li>
<li><strong>Transformar les dades originals</strong>.</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRpc2NyaW1pbmFudF9hbmFseXNpcyBpbXBvcnQgTGluZWFyRGlzY3JpbWluYW50QW5hbHlzaXMgYXMgTERBCgojIERhZGVzIGQnZXhlbXBsZSBpIGV0aXF1ZXRlcwpYID0gbnAuYXJyYXkoW1syLjUsIDIuNF0sIFswLjUsIDAuN10sIFsyLjIsIDIuOV0sIFsxLjksIDIuMl0sIFszLjEsIDMuMF0sIFsyLjMsIDIuN10sIFsyLCAxLjZdLCBbMSwgMS4xXSwgWzEuNSwgMS42XSwgWzEuMSwgMC45XV0pCnkgPSBucC5hcnJheShbMSwgMCwgMSwgMSwgMSwgMSwgMCwgMCwgMCwgMF0pCgojIEFwbGljYWNpw7MgZGUgTERBCmxkYSA9IExEQShuX2NvbXBvbmVudHM9MSkKWF9sZGEgPSBsZGEuZml0X3RyYW5zZm9ybShYLCB5KQoKcHJpbnQoIkNvbXBvbmVudHMgZGlzY3JpbWluYW50czpcbiIsIFhfbGRhKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# Dades d'exemple i etiquetes
X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])
y = np.array([1, 0, 1, 1, 1, 1, 0, 0, 0, 0])

# Aplicaci&oacute; de LDA
lda = LDA(n_components=1)
X_lda = lda.fit_transform(X, y)

print(&quot;Components discriminants:\n&quot;, X_lda)</pre></div><div class='content'></div><h2>t-Distributed Stochastic Neighbor Embedding (t-SNE)</h2>
<div class='content'><p>El t-SNE és una tècnica no lineal que s'utilitza principalment per a la visualització de dades d'alta dimensionalitat en 2D o 3D. És especialment útil per descobrir patrons en dades complexes.</p>
<h4>Passos per realitzar t-SNE:</h4>
<ol>
<li><strong>Calcular les probabilitats de similitud entre parells de punts en l'espai d'alta dimensió</strong>.</li>
<li><strong>Calcular les probabilitats de similitud en l'espai de baixa dimensió</strong>.</li>
<li><strong>Minimitzar la divergència entre aquestes dues distribucions utilitzant un mètode de gradient descendent</strong>.</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1hbmlmb2xkIGltcG9ydCBUU05FCgojIERhZGVzIGQnZXhlbXBsZQpYID0gbnAuYXJyYXkoW1syLjUsIDIuNF0sIFswLjUsIDAuN10sIFsyLjIsIDIuOV0sIFsxLjksIDIuMl0sIFszLjEsIDMuMF0sIFsyLjMsIDIuN10sIFsyLCAxLjZdLCBbMSwgMS4xXSwgWzEuNSwgMS42XSwgWzEuMSwgMC45XV0pCgojIEFwbGljYWNpw7MgZGUgdC1TTkUKdHNuZSA9IFRTTkUobl9jb21wb25lbnRzPTIsIHJhbmRvbV9zdGF0ZT0wKQpYX3RzbmUgPSB0c25lLmZpdF90cmFuc2Zvcm0oWCkKCnByaW50KCJ0LVNORSByZXN1bHQ6XG4iLCBYX3RzbmUp"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.manifold import TSNE

# Dades d'exemple
X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])

# Aplicaci&oacute; de t-SNE
tsne = TSNE(n_components=2, random_state=0)
X_tsne = tsne.fit_transform(X)

print(&quot;t-SNE result:\n&quot;, X_tsne)</pre></div><div class='content'></div><h1>Comparació de Tècniques</h1>
<div class='content'><p>| Tècnica | Tipus | Supervisada | Propòsit Principal |
|---------|------|-------------|---------------------|
| PCA     | Lineal | No         | Reducció de dimensionalitat general |
| LDA     | Lineal | Sí         | Maximitzar la separació entre classes |
| t-SNE   | No Lineal | No      | Visualització de dades complexes |</p>
</div><h1>Conclusió</h1>
<div class='content'><p>La reducció de dimensionalitat és una eina poderosa en l'arsenal d'un científic de dades. En comprendre i aplicar tècniques com PCA, LDA i t-SNE, podem millorar significativament el rendiment dels nostres models, fer que les nostres dades siguin més manejables i descobrir patrons ocults. És essencial triar la tècnica adequada segons el problema específic i les dades disponibles.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicitat</h1>
			<p>Aquest espai està destinat a publicitat.</p>
			<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Gràcies per col·laborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
