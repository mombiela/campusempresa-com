<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bias y Fairness en Machine Learning</title>

    <link rel="alternate" href="https://campusempresa.com/mod/machine_learning/bias-fairness" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/machine_learning/bias-fairness" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/machine_learning/bias-fairness" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/machine_learning/bias-fairness" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/machine_learning/bias-fairness" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-4'>
					<a href='privacidad-seguridad'>&#x25C4;Privacidad y Seguridad de Datos</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Bias y Fairness en Machine Learning</a>
	</div>
	<div class='col-4 text-end'>
					<a href='impacto-social'>Impacto Social del Machine Learning &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introducción</h1>
<div class='content'><p>En el contexto de Machine Learning, los conceptos de bias (sesgo) y fairness (justicia) son fundamentales para desarrollar modelos que no solo sean precisos, sino también éticos y equitativos. Este tema abordará las definiciones, tipos de sesgos, métodos para detectar y mitigar sesgos, y cómo evaluar la equidad en los modelos de Machine Learning.</p>
</div><h1>¿Qué es el Bias?</h1>
<div class='content'><p>El bias en Machine Learning se refiere a cualquier error sistemático en el modelo que conduce a resultados inexactos o injustos. Puede surgir en varias etapas del proceso de desarrollo del modelo, desde la recopilación de datos hasta la implementación del modelo.</p>
</div><h2>Tipos de Bias</h2>
<div class='content'><ul>
<li><strong>Bias de Selección</strong>: Ocurre cuando los datos de entrenamiento no representan adecuadamente la población objetivo.</li>
<li><strong>Bias de Confirmación</strong>: Surge cuando se buscan o interpretan datos de manera que confirmen las creencias preexistentes.</li>
<li><strong>Bias de Exclusión</strong>: Se produce cuando ciertos grupos o características se excluyen del conjunto de datos.</li>
<li><strong>Bias de Medición</strong>: Aparece cuando las variables de entrada no se miden con precisión.</li>
</ul>
</div><h1>¿Qué es Fairness?</h1>
<div class='content'><p>La fairness en Machine Learning se refiere a la creación de modelos que traten a todos los individuos o grupos de manera equitativa. La equidad puede ser evaluada desde diferentes perspectivas, como la equidad demográfica, la equidad de oportunidades y la equidad de resultados.</p>
</div><h2>Métodos para Evaluar la Fairness</h2>
<div class='content'><ul>
<li><strong>Equidad Demográfica</strong>: Asegura que todas las clases o grupos demográficos tengan una representación equitativa en los resultados del modelo.</li>
<li><strong>Equidad de Oportunidades</strong>: Garantiza que todos los individuos tengan las mismas oportunidades de recibir un resultado positivo.</li>
<li><strong>Equidad de Resultados</strong>: Se enfoca en que los resultados finales sean equitativos para todos los grupos.</li>
</ul>
</div><h1>Ejemplos de Bias y Fairness</h1>
<div class='content'></div><h2>Ejemplo de Bias de Selección</h2>
<div class='content'><p>Supongamos que estamos desarrollando un modelo de clasificación para predecir si un candidato será contratado. Si el conjunto de datos de entrenamiento contiene principalmente datos de candidatos de una sola universidad, el modelo podría tener un bias de selección.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHBhbmRhcyBhcyBwZApmcm9tIHNrbGVhcm4ubW9kZWxfc2VsZWN0aW9uIGltcG9ydCB0cmFpbl90ZXN0X3NwbGl0CmZyb20gc2tsZWFybi5saW5lYXJfbW9kZWwgaW1wb3J0IExvZ2lzdGljUmVncmVzc2lvbgoKIyBDYXJnYXIgZGF0b3MKZGF0YSA9IHBkLnJlYWRfY3N2KCdjYW5kaWRhdGVzLmNzdicpCgojIFZlcmlmaWNhciBsYSBkaXN0cmlidWNpw7NuIGRlIGxhIHVuaXZlcnNpZGFkCnByaW50KGRhdGFbJ3VuaXZlcnNpdHknXS52YWx1ZV9jb3VudHMoKSkKCiMgRGl2aWRpciBsb3MgZGF0b3MgZW4gZW50cmVuYW1pZW50byB5IHBydWViYQpYX3RyYWluLCBYX3Rlc3QsIHlfdHJhaW4sIHlfdGVzdCA9IHRyYWluX3Rlc3Rfc3BsaXQoZGF0YS5kcm9wKCdoaXJlZCcsIGF4aXM9MSksIGRhdGFbJ2hpcmVkJ10sIHRlc3Rfc2l6ZT0wLjIsIHJhbmRvbV9zdGF0ZT00MikKCiMgRW50cmVuYXIgZWwgbW9kZWxvCm1vZGVsID0gTG9naXN0aWNSZWdyZXNzaW9uKCkKbW9kZWwuZml0KFhfdHJhaW4sIHlfdHJhaW4pCgojIEV2YWx1YXIgZWwgbW9kZWxvCmFjY3VyYWN5ID0gbW9kZWwuc2NvcmUoWF90ZXN0LCB5X3Rlc3QpCnByaW50KGYnQWNjdXJhY3k6IHthY2N1cmFjeX0nKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Cargar datos
data = pd.read_csv('candidates.csv')

# Verificar la distribuci&oacute;n de la universidad
print(data['university'].value_counts())

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(data.drop('hired', axis=1), data['hired'], test_size=0.2, random_state=42)

# Entrenar el modelo
model = LogisticRegression()
model.fit(X_train, y_train)

# Evaluar el modelo
accuracy = model.score(X_test, y_test)
print(f'Accuracy: {accuracy}')</pre></div><div class='content'></div><h2>Ejemplo de Evaluación de Fairness</h2>
<div class='content'><p>Para evaluar la equidad demográfica, podemos usar métricas como la Tasa de Falsos Positivos (FPR) y la Tasa de Falsos Negativos (FNR) para diferentes grupos demográficos.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1ldHJpY3MgaW1wb3J0IGNvbmZ1c2lvbl9tYXRyaXgKCiMgUHJlZGljY2lvbmVzIGRlbCBtb2RlbG8KeV9wcmVkID0gbW9kZWwucHJlZGljdChYX3Rlc3QpCgojIENvbmZ1c2lvbiBtYXRyaXggcGFyYSBkaWZlcmVudGVzIGdydXBvcwpjb25mX21hdHJpeF9ncm91cDEgPSBjb25mdXNpb25fbWF0cml4KHlfdGVzdFtYX3Rlc3RbJ2dyb3VwJ10gPT0gMV0sIHlfcHJlZFtYX3Rlc3RbJ2dyb3VwJ10gPT0gMV0pCmNvbmZfbWF0cml4X2dyb3VwMiA9IGNvbmZ1c2lvbl9tYXRyaXgoeV90ZXN0W1hfdGVzdFsnZ3JvdXAnXSA9PSAyXSwgeV9wcmVkW1hfdGVzdFsnZ3JvdXAnXSA9PSAyXSkKCiMgQ2FsY3VsYXIgRlBSIHkgRk5SCmRlZiBjYWxjdWxhdGVfZnByX2Zucihjb25mX21hdHJpeCk6CiAgICB0biwgZnAsIGZuLCB0cCA9IGNvbmZfbWF0cml4LnJhdmVsKCkKICAgIGZwciA9IGZwIC8gKGZwICsgdG4pCiAgICBmbnIgPSBmbiAvIChmbiArIHRwKQogICAgcmV0dXJuIGZwciwgZm5yCgpmcHJfZ3JvdXAxLCBmbnJfZ3JvdXAxID0gY2FsY3VsYXRlX2Zwcl9mbnIoY29uZl9tYXRyaXhfZ3JvdXAxKQpmcHJfZ3JvdXAyLCBmbnJfZ3JvdXAyID0gY2FsY3VsYXRlX2Zwcl9mbnIoY29uZl9tYXRyaXhfZ3JvdXAyKQoKcHJpbnQoZidHcm91cCAxIC0gRlBSOiB7ZnByX2dyb3VwMX0sIEZOUjoge2Zucl9ncm91cDF9JykKcHJpbnQoZidHcm91cCAyIC0gRlBSOiB7ZnByX2dyb3VwMn0sIEZOUjoge2Zucl9ncm91cDJ9Jyk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.metrics import confusion_matrix

# Predicciones del modelo
y_pred = model.predict(X_test)

# Confusion matrix para diferentes grupos
conf_matrix_group1 = confusion_matrix(y_test[X_test['group'] == 1], y_pred[X_test['group'] == 1])
conf_matrix_group2 = confusion_matrix(y_test[X_test['group'] == 2], y_pred[X_test['group'] == 2])

# Calcular FPR y FNR
def calculate_fpr_fnr(conf_matrix):
    tn, fp, fn, tp = conf_matrix.ravel()
    fpr = fp / (fp + tn)
    fnr = fn / (fn + tp)
    return fpr, fnr

fpr_group1, fnr_group1 = calculate_fpr_fnr(conf_matrix_group1)
fpr_group2, fnr_group2 = calculate_fpr_fnr(conf_matrix_group2)

print(f'Group 1 - FPR: {fpr_group1}, FNR: {fnr_group1}')
print(f'Group 2 - FPR: {fpr_group2}, FNR: {fnr_group2}')</pre></div><div class='content'></div><h1>Métodos para Mitigar el Bias</h1>
<div class='content'><ul>
<li><strong>Recolección de Datos Diversos</strong>: Asegurarse de que el conjunto de datos de entrenamiento sea representativo de la población objetivo.</li>
<li><strong>Preprocesamiento de Datos</strong>: Aplicar técnicas de balanceo de datos, como sobremuestreo o submuestreo.</li>
<li><strong>Regularización del Modelo</strong>: Utilizar técnicas de regularización para reducir el overfitting y el bias.</li>
<li><strong>Auditorías de Equidad</strong>: Realizar auditorías periódicas para evaluar y mitigar el bias en el modelo.</li>
</ul>
</div><h1>Conclusión</h1>
<div class='content'><p>El bias y la fairness son aspectos críticos en el desarrollo de modelos de Machine Learning. Comprender y abordar estos conceptos no solo mejora la precisión del modelo, sino que también asegura que los modelos sean justos y éticos. Al implementar técnicas para detectar y mitigar el bias, y evaluar la equidad, los profesionales pueden desarrollar modelos más robustos y equitativos.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='privacidad-seguridad'>&#x25C4;Privacidad y Seguridad de Datos</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Bias y Fairness en Machine Learning</a>
	</div>
	<div class='col-4 text-end'>
					<a href='impacto-social'>Impacto Social del Machine Learning &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
