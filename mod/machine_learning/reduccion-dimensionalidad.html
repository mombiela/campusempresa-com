<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reducción de Dimensionalidad</title>

    <link rel="alternate" href="https://campusempresa.com/mod/machine_learning/reduccion-dimensionalidad" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/machine_learning/reduccion-dimensionalidad" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/machine_learning/reduccion-dimensionalidad" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/machine_learning/reduccion-dimensionalidad" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/machine_learning/reduccion-dimensionalidad" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-4'>
					<a href='pca'>&#x25C4;Análisis de Componentes Principales (PCA)</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Reducción de Dimensionalidad</a>
	</div>
	<div class='col-4 text-end'>
					<a href='asociacion-reglas'>Asociación de Reglas &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introducción</h1>
<div class='content'><p>La reducción de dimensionalidad es una técnica crucial en Machine Learning que se utiliza para simplificar modelos, mejorar la eficiencia computacional y reducir el ruido en los datos. Este proceso implica transformar datos de alta dimensionalidad en un espacio de menor dimensión, manteniendo la mayor cantidad de información relevante posible.</p>
</div><h1>Importancia de la Reducción de Dimensionalidad</h1>
<div class='content'><ul>
<li><strong>Mejora del rendimiento del modelo</strong>: Al reducir el número de características, se puede evitar el sobreajuste y mejorar la generalización del modelo.</li>
<li><strong>Eficiencia computacional</strong>: Menos características significan menos cálculos, lo que acelera el entrenamiento y la predicción.</li>
<li><strong>Visualización</strong>: Facilita la visualización de datos complejos en 2D o 3D.</li>
<li><strong>Eliminación de ruido</strong>: Ayuda a eliminar características irrelevantes o redundantes que pueden introducir ruido en el modelo.</li>
</ul>
</div><h1>Técnicas de Reducción de Dimensionalidad</h1>
<div class='content'></div><h2>Análisis de Componentes Principales (PCA)</h2>
<div class='content'><p>El PCA es una técnica estadística que transforma los datos a un nuevo sistema de coordenadas, donde las nuevas variables (componentes principales) son combinaciones lineales de las variables originales y están ordenadas por la cantidad de varianza que explican.</p>
<h4>Pasos para realizar PCA:</h4>
<ol>
<li><strong>Estandarización de los datos</strong>: Normalizar los datos para que cada característica tenga una media de 0 y una desviación estándar de 1.</li>
<li><strong>Cálculo de la matriz de covarianza</strong>: Determinar la relación entre las características.</li>
<li><strong>Cálculo de los valores y vectores propios</strong>: Identificar las direcciones principales de la varianza.</li>
<li><strong>Selección de componentes principales</strong>: Elegir los componentes que explican la mayor parte de la varianza.</li>
<li><strong>Transformación de los datos</strong>: Proyectar los datos originales en el nuevo espacio de componentes principales.</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmZyb20gc2tsZWFybi5kZWNvbXBvc2l0aW9uIGltcG9ydCBQQ0EKZnJvbSBza2xlYXJuLnByZXByb2Nlc3NpbmcgaW1wb3J0IFN0YW5kYXJkU2NhbGVyCgojIERhdG9zIGRlIGVqZW1wbG8KWCA9IG5wLmFycmF5KFtbMi41LCAyLjRdLCBbMC41LCAwLjddLCBbMi4yLCAyLjldLCBbMS45LCAyLjJdLCBbMy4xLCAzLjBdLCBbMi4zLCAyLjddLCBbMiwgMS42XSwgWzEsIDEuMV0sIFsxLjUsIDEuNl0sIFsxLjEsIDAuOV1dKQoKIyBFc3RhbmRhcml6YWNpw7NuIGRlIGxvcyBkYXRvcwpzY2FsZXIgPSBTdGFuZGFyZFNjYWxlcigpClhfc2NhbGVkID0gc2NhbGVyLmZpdF90cmFuc2Zvcm0oWCkKCiMgQXBsaWNhY2nDs24gZGUgUENBCnBjYSA9IFBDQShuX2NvbXBvbmVudHM9MikKWF9wY2EgPSBwY2EuZml0X3RyYW5zZm9ybShYX3NjYWxlZCkKCnByaW50KCJDb21wb25lbnRlcyBwcmluY2lwYWxlczpcbiIsIFhfcGNhKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Datos de ejemplo
X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])

# Estandarizaci&oacute;n de los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicaci&oacute;n de PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print(&quot;Componentes principales:\n&quot;, X_pca)</pre></div><div class='content'></div><h2>Análisis Discriminante Lineal (LDA)</h2>
<div class='content'><p>El LDA es una técnica supervisada que busca maximizar la separación entre múltiples clases. A diferencia del PCA, que no considera la etiqueta de clase, el LDA utiliza la información de clase para encontrar las direcciones que maximizan la separación entre clases.</p>
<h4>Pasos para realizar LDA:</h4>
<ol>
<li><strong>Calcular la media de cada clase</strong>.</li>
<li><strong>Calcular la matriz de dispersión dentro de la clase y entre clases</strong>.</li>
<li><strong>Calcular los valores y vectores propios</strong>.</li>
<li><strong>Seleccionar los vectores propios que maximizan la separación entre clases</strong>.</li>
<li><strong>Transformar los datos originales</strong>.</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRpc2NyaW1pbmFudF9hbmFseXNpcyBpbXBvcnQgTGluZWFyRGlzY3JpbWluYW50QW5hbHlzaXMgYXMgTERBCgojIERhdG9zIGRlIGVqZW1wbG8geSBldGlxdWV0YXMKWCA9IG5wLmFycmF5KFtbMi41LCAyLjRdLCBbMC41LCAwLjddLCBbMi4yLCAyLjldLCBbMS45LCAyLjJdLCBbMy4xLCAzLjBdLCBbMi4zLCAyLjddLCBbMiwgMS42XSwgWzEsIDEuMV0sIFsxLjUsIDEuNl0sIFsxLjEsIDAuOV1dKQp5ID0gbnAuYXJyYXkoWzEsIDAsIDEsIDEsIDEsIDEsIDAsIDAsIDAsIDBdKQoKIyBBcGxpY2FjacOzbiBkZSBMREEKbGRhID0gTERBKG5fY29tcG9uZW50cz0xKQpYX2xkYSA9IGxkYS5maXRfdHJhbnNmb3JtKFgsIHkpCgpwcmludCgiQ29tcG9uZW50ZXMgZGlzY3JpbWluYW50ZXM6XG4iLCBYX2xkYSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# Datos de ejemplo y etiquetas
X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])
y = np.array([1, 0, 1, 1, 1, 1, 0, 0, 0, 0])

# Aplicaci&oacute;n de LDA
lda = LDA(n_components=1)
X_lda = lda.fit_transform(X, y)

print(&quot;Componentes discriminantes:\n&quot;, X_lda)</pre></div><div class='content'></div><h2>t-Distributed Stochastic Neighbor Embedding (t-SNE)</h2>
<div class='content'><p>El t-SNE es una técnica no lineal que se utiliza principalmente para la visualización de datos de alta dimensionalidad en 2D o 3D. Es especialmente útil para descubrir patrones en datos complejos.</p>
<h4>Pasos para realizar t-SNE:</h4>
<ol>
<li><strong>Calcular las probabilidades de similitud entre pares de puntos en el espacio de alta dimensión</strong>.</li>
<li><strong>Calcular las probabilidades de similitud en el espacio de baja dimensión</strong>.</li>
<li><strong>Minimizar la divergencia entre estas dos distribuciones utilizando un método de gradiente descendente</strong>.</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1hbmlmb2xkIGltcG9ydCBUU05FCgojIERhdG9zIGRlIGVqZW1wbG8KWCA9IG5wLmFycmF5KFtbMi41LCAyLjRdLCBbMC41LCAwLjddLCBbMi4yLCAyLjldLCBbMS45LCAyLjJdLCBbMy4xLCAzLjBdLCBbMi4zLCAyLjddLCBbMiwgMS42XSwgWzEsIDEuMV0sIFsxLjUsIDEuNl0sIFsxLjEsIDAuOV1dKQoKIyBBcGxpY2FjacOzbiBkZSB0LVNORQp0c25lID0gVFNORShuX2NvbXBvbmVudHM9MiwgcmFuZG9tX3N0YXRlPTApClhfdHNuZSA9IHRzbmUuZml0X3RyYW5zZm9ybShYKQoKcHJpbnQoInQtU05FIHJlc3VsdDpcbiIsIFhfdHNuZSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.manifold import TSNE

# Datos de ejemplo
X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])

# Aplicaci&oacute;n de t-SNE
tsne = TSNE(n_components=2, random_state=0)
X_tsne = tsne.fit_transform(X)

print(&quot;t-SNE result:\n&quot;, X_tsne)</pre></div><div class='content'></div><h1>Comparación de Técnicas</h1>
<div class='content'><p>| Técnica | Tipo | Supervisada | Propósito Principal |
|---------|------|-------------|---------------------|
| PCA     | Lineal | No         | Reducción de dimensionalidad general |
| LDA     | Lineal | Sí         | Maximizar la separación entre clases |
| t-SNE   | No Lineal | No      | Visualización de datos complejos |</p>
</div><h1>Conclusión</h1>
<div class='content'><p>La reducción de dimensionalidad es una herramienta poderosa en el arsenal de un científico de datos. Al comprender y aplicar técnicas como PCA, LDA y t-SNE, podemos mejorar significativamente el rendimiento de nuestros modelos, hacer que nuestros datos sean más manejables y descubrir patrones ocultos. Es esencial elegir la técnica adecuada según el problema específico y los datos disponibles.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='pca'>&#x25C4;Análisis de Componentes Principales (PCA)</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Reducción de Dimensionalidad</a>
	</div>
	<div class='col-4 text-end'>
					<a href='asociacion-reglas'>Asociación de Reglas &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
