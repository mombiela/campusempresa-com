<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Funciones de Activación en PyTorch</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/activation-functions" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/activation-functions" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/activation-functions" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/pytorch/activation-functions" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/activation-functions" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='creating-a-simple-neural-network'>&#x25C4;Creando una Red Neuronal Simple</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Funciones de Activación en PyTorch</a>
	</div>
	<div class='col-4 text-end'>
					<a href='loss-functions-and-optimization'>Funciones de Pérdida y Optimización &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Las funciones de activación juegan un papel crucial en las redes neuronales al introducir no linealidad en el modelo, permitiéndole aprender patrones complejos. En esta sección, exploraremos varias funciones de activación disponibles en PyTorch, entenderemos sus propiedades y veremos cómo implementarlas en código.</p>
</div><h1>Introducción a las Funciones de Activación</h1>
<div class='content'><ul>
<li><strong>Definición</strong>: Las funciones de activación determinan la salida de un nodo de red neuronal dado un conjunto de entradas.</li>
<li><strong>Propósito</strong>: Introducen propiedades no lineales a la red, permitiéndole aprender de los datos y realizar tareas complejas.</li>
</ul>
</div><h1>Funciones de Activación Comunes</h1>
<div class='content'></div><h2>Sigmoide</h2>
<div class='content'><ul>
<li><strong>Fórmula</strong>: \( \sigma(x) = \frac{1}{1 + e^{-x}} \)</li>
<li><strong>Rango</strong>: (0, 1)</li>
<li><strong>Propiedades</strong>:
<ul>
<li>Gradiente suave</li>
<li>Valores de salida limitados entre 0 y 1</li>
<li>Puede causar el problema del gradiente que desaparece</li>
</ul>
</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgoKIyBGdW5jacOzbiBkZSBhY3RpdmFjacOzbiBTaWdtb2lkZQpzaWdtb2lkID0gbm4uU2lnbW9pZCgpCmlucHV0X3RlbnNvciA9IHRvcmNoLnRlbnNvcihbMS4wLCAyLjAsIDMuMF0pCm91dHB1dF90ZW5zb3IgPSBzaWdtb2lkKGlucHV0X3RlbnNvcikKcHJpbnQob3V0cHV0X3RlbnNvcik="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn

# Funci&oacute;n de activaci&oacute;n Sigmoide
sigmoid = nn.Sigmoid()
input_tensor = torch.tensor([1.0, 2.0, 3.0])
output_tensor = sigmoid(input_tensor)
print(output_tensor)</pre></div><div class='content'></div><h2>Tanh</h2>
<div class='content'><ul>
<li><strong>Fórmula</strong>: \( \tanh(x) = \frac{2}{1 + e^{-2x}} - 1 \)</li>
<li><strong>Rango</strong>: (-1, 1)</li>
<li><strong>Propiedades</strong>:
<ul>
<li>Centrada en cero</li>
<li>Gradiente suave</li>
<li>También puede causar el problema del gradiente que desaparece</li>
</ul>
</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBGdW5jacOzbiBkZSBhY3RpdmFjacOzbiBUYW5oCnRhbmggPSBubi5UYW5oKCkKb3V0cHV0X3RlbnNvciA9IHRhbmgoaW5wdXRfdGVuc29yKQpwcmludChvdXRwdXRfdGVuc29yKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Funci&oacute;n de activaci&oacute;n Tanh
tanh = nn.Tanh()
output_tensor = tanh(input_tensor)
print(output_tensor)</pre></div><div class='content'></div><h2>ReLU (Unidad Lineal Rectificada)</h2>
<div class='content'><ul>
<li><strong>Fórmula</strong>: \( \text{ReLU}(x) = \max(0, x) \)</li>
<li><strong>Rango</strong>: [0, ∞)</li>
<li><strong>Propiedades</strong>:
<ul>
<li>Computacionalmente eficiente</li>
<li>Ayuda a mitigar el problema del gradiente que desaparece</li>
<li>Puede causar el problema de ReLU moribunda</li>
</ul>
</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBGdW5jacOzbiBkZSBhY3RpdmFjacOzbiBSZUxVCnJlbHUgPSBubi5SZUxVKCkKb3V0cHV0X3RlbnNvciA9IHJlbHUoaW5wdXRfdGVuc29yKQpwcmludChvdXRwdXRfdGVuc29yKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Funci&oacute;n de activaci&oacute;n ReLU
relu = nn.ReLU()
output_tensor = relu(input_tensor)
print(output_tensor)</pre></div><div class='content'></div><h2>Leaky ReLU</h2>
<div class='content'><ul>
<li><strong>Fórmula</strong>: \( \text{Leaky ReLU}(x) = \max(0.01x, x) \)</li>
<li><strong>Rango</strong>: (-∞, ∞)</li>
<li><strong>Propiedades</strong>:
<ul>
<li>Permite un pequeño gradiente cuando la unidad no está activa</li>
<li>Ayuda a mitigar el problema de ReLU moribunda</li>
</ul>
</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBGdW5jacOzbiBkZSBhY3RpdmFjacOzbiBMZWFreSBSZUxVCmxlYWt5X3JlbHUgPSBubi5MZWFreVJlTFUoMC4wMSkKb3V0cHV0X3RlbnNvciA9IGxlYWt5X3JlbHUoaW5wdXRfdGVuc29yKQpwcmludChvdXRwdXRfdGVuc29yKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Funci&oacute;n de activaci&oacute;n Leaky ReLU
leaky_relu = nn.LeakyReLU(0.01)
output_tensor = leaky_relu(input_tensor)
print(output_tensor)</pre></div><div class='content'></div><h2>Softmax</h2>
<div class='content'><ul>
<li><strong>Fórmula</strong>: \( \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \)</li>
<li><strong>Rango</strong>: (0, 1)</li>
<li><strong>Propiedades</strong>:
<ul>
<li>Convierte logits en probabilidades</li>
<li>A menudo se usa en la capa de salida para tareas de clasificación</li>
</ul>
</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBGdW5jacOzbiBkZSBhY3RpdmFjacOzbiBTb2Z0bWF4CnNvZnRtYXggPSBubi5Tb2Z0bWF4KGRpbT0wKQpvdXRwdXRfdGVuc29yID0gc29mdG1heChpbnB1dF90ZW5zb3IpCnByaW50KG91dHB1dF90ZW5zb3Ip"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Funci&oacute;n de activaci&oacute;n Softmax
softmax = nn.Softmax(dim=0)
output_tensor = softmax(input_tensor)
print(output_tensor)</pre></div><div class='content'></div><h1>Comparación de Funciones de Activación</h1>
<div class='content'><p>| Función de Activación | Fórmula | Rango | Propiedades Clave |
|-----------------------|---------|-------|-------------------|
| Sigmoide              | \( \frac{1}{1 + e^{-x}} \) | (0, 1) | Gradiente suave, problema del gradiente que desaparece |
| Tanh                  | \( \frac{2}{1 + e^{-2x}} - 1 \) | (-1, 1) | Centrada en cero, problema del gradiente que desaparece |
| ReLU                  | \( \max(0, x) \) | [0, ∞) | Computacionalmente eficiente, problema de ReLU moribunda |
| Leaky ReLU            | \( \max(0.01x, x) \) | (-∞, ∞) | Pequeño gradiente cuando inactiva, mitiga ReLU moribunda |
| Softmax               | \( \frac{e^{x_i}}{\sum_{j} e^{x_j}} \) | (0, 1) | Convierte logits en probabilidades, usada en clasificación |</p>
</div><h1>Conclusión</h1>
<div class='content'><p>Las funciones de activación son un componente fundamental de las redes neuronales, permitiéndoles aprender y modelar datos complejos. En PyTorch, varias funciones de activación están disponibles y pueden integrarse fácilmente en tus modelos. Entender las propiedades y los casos de uso apropiados para cada función de activación es esencial para construir redes neuronales efectivas.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='creating-a-simple-neural-network'>&#x25C4;Creando una Red Neuronal Simple</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Funciones de Activación en PyTorch</a>
	</div>
	<div class='col-4 text-end'>
					<a href='loss-functions-and-optimization'>Funciones de Pérdida y Optimización &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
