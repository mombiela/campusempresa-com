<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introducción a las Redes Neuronales</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/introduction-to-neural-networks" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/introduction-to-neural-networks" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/introduction-to-neural-networks" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/pytorch/introduction-to-neural-networks" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/introduction-to-neural-networks" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='autograd-automatic-differentiation'>&#x25C4;Autograd: Diferenciación Automática</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Introducción a las Redes Neuronales</a>
	</div>
	<div class='col-4 text-end'>
					<a href='creating-a-simple-neural-network'>Creando una Red Neuronal Simple &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Resumen</h1>
<div class='content'><p>Las redes neuronales son un concepto fundamental en el aprendizaje profundo y son la columna vertebral de muchas aplicaciones modernas de aprendizaje automático. Esta sección te introducirá a los conceptos básicos de las redes neuronales y cómo implementarlas usando PyTorch.</p>
</div><h1>Conceptos Clave</h1>
<div class='content'><ul>
<li><strong>Neuronas y Capas</strong>: Los bloques de construcción básicos de las redes neuronales.</li>
<li><strong>Funciones de Activación</strong>: Funciones que introducen no linealidad en la red.</li>
<li><strong>Funciones de Pérdida</strong>: Métricas para evaluar el rendimiento de la red.</li>
<li><strong>Algoritmos de Optimización</strong>: Métodos para minimizar la función de pérdida.</li>
<li><strong>Propagación Hacia Adelante y Hacia Atrás</strong>: Mecanismos para entrenar la red.</li>
</ul>
</div><h1>Neuronas y Capas</h1>
<div class='content'><p>Una red neuronal consiste en capas de neuronas. Cada neurona recibe una entrada, la procesa y pasa la salida a la siguiente capa.</p>
</div><h2>Ejemplo: Neurona Única</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIERlZmluaXIgdW5hIG5ldXJvbmEgw7puaWNhIGNvbiB1biBwZXNvIHkgdW4gc2VzZ28Kd2VpZ2h0ID0gdG9yY2gudGVuc29yKDAuNSwgcmVxdWlyZXNfZ3JhZD1UcnVlKQpiaWFzID0gdG9yY2gudGVuc29yKDAuMSwgcmVxdWlyZXNfZ3JhZD1UcnVlKQoKIyBWYWxvciBkZSBlbnRyYWRhCnggPSB0b3JjaC50ZW5zb3IoMS4wKQoKIyBUcmFuc2Zvcm1hY2nDs24gbGluZWFsCnkgPSB3ZWlnaHQgKiB4ICsgYmlhcwoKcHJpbnQoeSkgICMgU2FsaWRhOiB0ZW5zb3IoMC42MDAwLCBncmFkX2ZuPTxBZGRCYWNrd2FyZDA+KQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Definir una neurona &uacute;nica con un peso y un sesgo
weight = torch.tensor(0.5, requires_grad=True)
bias = torch.tensor(0.1, requires_grad=True)

# Valor de entrada
x = torch.tensor(1.0)

# Transformaci&oacute;n lineal
y = weight * x + bias

print(y)  # Salida: tensor(0.6000, grad_fn=&lt;AddBackward0&gt;)</pre></div><div class='content'></div><h2>Ejemplo: Red Neuronal Simple</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoLm5uIGFzIG5uCgojIERlZmluaXIgdW5hIHJlZCBuZXVyb25hbCBzaW1wbGUgY29uIHVuYSBjYXBhIG9jdWx0YQpjbGFzcyBTaW1wbGVOTihubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYpOgogICAgICAgIHN1cGVyKFNpbXBsZU5OLCBzZWxmKS5fX2luaXRfXygpCiAgICAgICAgc2VsZi5oaWRkZW4gPSBubi5MaW5lYXIoMSwgMTApICAjIENhcGEgb2N1bHRhIGNvbiAxMCBuZXVyb25hcwogICAgICAgIHNlbGYub3V0cHV0ID0gbm4uTGluZWFyKDEwLCAxKSAgIyBDYXBhIGRlIHNhbGlkYQoKICAgIGRlZiBmb3J3YXJkKHNlbGYsIHgpOgogICAgICAgIHggPSB0b3JjaC5yZWx1KHNlbGYuaGlkZGVuKHgpKSAgIyBBcGxpY2FyIGZ1bmNpw7NuIGRlIGFjdGl2YWNpw7NuIFJlTFUKICAgICAgICB4ID0gc2VsZi5vdXRwdXQoeCkKICAgICAgICByZXR1cm4geAoKIyBJbnN0YW5jaWFyIGxhIHJlZApuZXQgPSBTaW1wbGVOTigpCnByaW50KG5ldCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch.nn as nn

# Definir una red neuronal simple con una capa oculta
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.hidden = nn.Linear(1, 10)  # Capa oculta con 10 neuronas
        self.output = nn.Linear(10, 1)  # Capa de salida

    def forward(self, x):
        x = torch.relu(self.hidden(x))  # Aplicar funci&oacute;n de activaci&oacute;n ReLU
        x = self.output(x)
        return x

# Instanciar la red
net = SimpleNN()
print(net)</pre></div><div class='content'></div><h1>Funciones de Activación</h1>
<div class='content'><p>Las funciones de activación introducen no linealidad en la red, permitiéndole aprender patrones complejos.</p>
</div><h2>Funciones de Activación Comunes</h2>
<div class='content'><ul>
<li><strong>ReLU (Unidad Lineal Rectificada)</strong>: <code>torch.relu</code></li>
<li><strong>Sigmoide</strong>: <code>torch.sigmoid</code></li>
<li><strong>Tanh</strong>: <code>torch.tanh</code></li>
</ul>
</div><h2>Ejemplo: Aplicando Funciones de Activación</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIFRlbnNvciBkZSBlbnRyYWRhCnggPSB0b3JjaC50ZW5zb3IoWy0xLjAsIDAuMCwgMS4wXSkKCiMgQXBsaWNhciBmdW5jacOzbiBkZSBhY3RpdmFjacOzbiBSZUxVCnJlbHVfb3V0cHV0ID0gdG9yY2gucmVsdSh4KQpwcmludChyZWx1X291dHB1dCkgICMgU2FsaWRhOiB0ZW5zb3IoWzAuLCAwLiwgMS5dKQoKIyBBcGxpY2FyIGZ1bmNpw7NuIGRlIGFjdGl2YWNpw7NuIFNpZ21vaWRlCnNpZ21vaWRfb3V0cHV0ID0gdG9yY2guc2lnbW9pZCh4KQpwcmludChzaWdtb2lkX291dHB1dCkgICMgU2FsaWRhOiB0ZW5zb3IoWzAuMjY4OSwgMC41MDAwLCAwLjczMTFdKQoKIyBBcGxpY2FyIGZ1bmNpw7NuIGRlIGFjdGl2YWNpw7NuIFRhbmgKdGFuaF9vdXRwdXQgPSB0b3JjaC50YW5oKHgpCnByaW50KHRhbmhfb3V0cHV0KSAgIyBTYWxpZGE6IHRlbnNvcihbLTAuNzYxNiwgIDAuMDAwMCwgIDAuNzYxNl0p"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Tensor de entrada
x = torch.tensor([-1.0, 0.0, 1.0])

# Aplicar funci&oacute;n de activaci&oacute;n ReLU
relu_output = torch.relu(x)
print(relu_output)  # Salida: tensor([0., 0., 1.])

# Aplicar funci&oacute;n de activaci&oacute;n Sigmoide
sigmoid_output = torch.sigmoid(x)
print(sigmoid_output)  # Salida: tensor([0.2689, 0.5000, 0.7311])

# Aplicar funci&oacute;n de activaci&oacute;n Tanh
tanh_output = torch.tanh(x)
print(tanh_output)  # Salida: tensor([-0.7616,  0.0000,  0.7616])</pre></div><div class='content'></div><h1>Funciones de Pérdida</h1>
<div class='content'><p>Las funciones de pérdida miden qué tan bien está funcionando la red neuronal. Las funciones de pérdida comunes incluyen el Error Cuadrático Medio (MSE) y la Pérdida de Entropía Cruzada.</p>
</div><h2>Ejemplo: Error Cuadrático Medio</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoLm5uLmZ1bmN0aW9uYWwgYXMgRgoKIyBWYWxvcmVzIHByZWRpY2hvcyB5IG9iamV0aXZvcwpwcmVkaWN0ZWQgPSB0b3JjaC50ZW5zb3IoWzAuNSwgMC44LCAwLjNdKQp0YXJnZXQgPSB0b3JjaC50ZW5zb3IoWzEuMCwgMC4wLCAwLjBdKQoKIyBDYWxjdWxhciBlbCBFcnJvciBDdWFkcsOhdGljbyBNZWRpbwpsb3NzID0gRi5tc2VfbG9zcyhwcmVkaWN0ZWQsIHRhcmdldCkKcHJpbnQobG9zcykgICMgU2FsaWRhOiB0ZW5zb3IoMC4zOTMzKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch.nn.functional as F

# Valores predichos y objetivos
predicted = torch.tensor([0.5, 0.8, 0.3])
target = torch.tensor([1.0, 0.0, 0.0])

# Calcular el Error Cuadr&aacute;tico Medio
loss = F.mse_loss(predicted, target)
print(loss)  # Salida: tensor(0.3933)</pre></div><div class='content'></div><h1>Algoritmos de Optimización</h1>
<div class='content'><p>Los algoritmos de optimización ajustan los pesos y sesgos para minimizar la función de pérdida. El algoritmo de optimización más común es el Descenso de Gradiente Estocástico (SGD).</p>
</div><h2>Ejemplo: Descenso de Gradiente Estocástico</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoLm9wdGltIGFzIG9wdGltCgojIERlZmluaXIgdW5hIHJlZCBuZXVyb25hbCBzaW1wbGUKbmV0ID0gU2ltcGxlTk4oKQoKIyBEZWZpbmlyIGVsIG9wdGltaXphZG9yCm9wdGltaXplciA9IG9wdGltLlNHRChuZXQucGFyYW1ldGVycygpLCBscj0wLjAxKQoKIyBFbnRyYWRhIHkgb2JqZXRpdm8gZmljdGljaW9zCmlucHV0ID0gdG9yY2gudGVuc29yKFtbMS4wXV0pCnRhcmdldCA9IHRvcmNoLnRlbnNvcihbWzAuMF1dKQoKIyBQYXNvIGhhY2lhIGFkZWxhbnRlCm91dHB1dCA9IG5ldChpbnB1dCkKbG9zcyA9IEYubXNlX2xvc3Mob3V0cHV0LCB0YXJnZXQpCgojIFBhc28gaGFjaWEgYXRyw6FzIHkgb3B0aW1pemFjacOzbgpvcHRpbWl6ZXIuemVyb19ncmFkKCkgICMgUG9uZXIgYSBjZXJvIGxvcyBncmFkaWVudGVzCmxvc3MuYmFja3dhcmQoKSAgICAgICAgIyBQYXNvIGhhY2lhIGF0csOhcwpvcHRpbWl6ZXIuc3RlcCgpICAgICAgICMgQWN0dWFsaXphciBsb3MgcGVzb3MKCnByaW50KGxvc3MpICAjIFNhbGlkYTogdGVuc29yKDAuMjUwMCwgZ3JhZF9mbj08TXNlTG9zc0JhY2t3YXJkPik="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch.optim as optim

# Definir una red neuronal simple
net = SimpleNN()

# Definir el optimizador
optimizer = optim.SGD(net.parameters(), lr=0.01)

# Entrada y objetivo ficticios
input = torch.tensor([[1.0]])
target = torch.tensor([[0.0]])

# Paso hacia adelante
output = net(input)
loss = F.mse_loss(output, target)

# Paso hacia atr&aacute;s y optimizaci&oacute;n
optimizer.zero_grad()  # Poner a cero los gradientes
loss.backward()        # Paso hacia atr&aacute;s
optimizer.step()       # Actualizar los pesos

print(loss)  # Salida: tensor(0.2500, grad_fn=&lt;MseLossBackward&gt;)</pre></div><div class='content'></div><h1>Propagación Hacia Adelante y Hacia Atrás</h1>
<div class='content'><p>La propagación hacia adelante implica pasar la entrada a través de la red para obtener la salida. La propagación hacia atrás implica calcular los gradientes y actualizar los pesos.</p>
</div><h2>Ejemplo: Bucle de Entrenamiento Completo</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBEZWZpbmlyIGxhIHJlZCwgbGEgZnVuY2nDs24gZGUgcMOpcmRpZGEgeSBlbCBvcHRpbWl6YWRvcgpuZXQgPSBTaW1wbGVOTigpCmNyaXRlcmlvbiA9IG5uLk1TRUxvc3MoKQpvcHRpbWl6ZXIgPSBvcHRpbS5TR0QobmV0LnBhcmFtZXRlcnMoKSwgbHI9MC4wMSkKCiMgRGF0b3MgZmljdGljaW9zCmlucHV0cyA9IHRvcmNoLnRlbnNvcihbWzEuMF0sIFsyLjBdLCBbMy4wXV0pCnRhcmdldHMgPSB0b3JjaC50ZW5zb3IoW1swLjBdLCBbMC4wXSwgWzEuMF1dKQoKIyBCdWNsZSBkZSBlbnRyZW5hbWllbnRvCmZvciBlcG9jaCBpbiByYW5nZSgxMDApOgogICAgZm9yIGkgaW4gcmFuZ2UobGVuKGlucHV0cykpOgogICAgICAgIGlucHV0ID0gaW5wdXRzW2ldCiAgICAgICAgdGFyZ2V0ID0gdGFyZ2V0c1tpXQoKICAgICAgICAjIFBhc28gaGFjaWEgYWRlbGFudGUKICAgICAgICBvdXRwdXQgPSBuZXQoaW5wdXQpCiAgICAgICAgbG9zcyA9IGNyaXRlcmlvbihvdXRwdXQsIHRhcmdldCkKCiAgICAgICAgIyBQYXNvIGhhY2lhIGF0csOhcyB5IG9wdGltaXphY2nDs24KICAgICAgICBvcHRpbWl6ZXIuemVyb19ncmFkKCkKICAgICAgICBsb3NzLmJhY2t3YXJkKCkKICAgICAgICBvcHRpbWl6ZXIuc3RlcCgpCgogICAgaWYgZXBvY2ggJSAxMCA9PSAwOgogICAgICAgIHByaW50KGYnRXBvY2gge2Vwb2NofSwgTG9zczoge2xvc3MuaXRlbSgpfScp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Definir la red, la funci&oacute;n de p&eacute;rdida y el optimizador
net = SimpleNN()
criterion = nn.MSELoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)

# Datos ficticios
inputs = torch.tensor([[1.0], [2.0], [3.0]])
targets = torch.tensor([[0.0], [0.0], [1.0]])

# Bucle de entrenamiento
for epoch in range(100):
    for i in range(len(inputs)):
        input = inputs[i]
        target = targets[i]

        # Paso hacia adelante
        output = net(input)
        loss = criterion(output, target)

        # Paso hacia atr&aacute;s y optimizaci&oacute;n
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item()}')</pre></div><div class='content'></div><h1>Conclusión</h1>
<div class='content'><p>En esta sección, aprendiste los conceptos básicos de las redes neuronales, incluyendo neuronas, capas, funciones de activación, funciones de pérdida, algoritmos de optimización y los conceptos de propagación hacia adelante y hacia atrás. Con esta base, ahora estás listo para construir y entrenar redes neuronales más complejas usando PyTorch.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='autograd-automatic-differentiation'>&#x25C4;Autograd: Diferenciación Automática</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Introducción a las Redes Neuronales</a>
	</div>
	<div class='col-4 text-end'>
					<a href='creating-a-simple-neural-network'>Creando una Red Neuronal Simple &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
