<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Redes de Memoria a Largo Corto Plazo (LSTMs)</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/long-short-term-memory-networks-lstms" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/long-short-term-memory-networks-lstms" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/long-short-term-memory-networks-lstms" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/pytorch/long-short-term-memory-networks-lstms" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/long-short-term-memory-networks-lstms" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='recurrent-neural-networks-rnns'>&#x25C4;Redes Neuronales Recurrentes (RNNs)</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Redes de Memoria a Largo Corto Plazo (LSTMs)</a>
	</div>
	<div class='col-4 text-end'>
					<a href='transformers'>Transformers &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introducción a las LSTMs</h1>
<div class='content'><p>Las redes de Memoria a Largo Corto Plazo (LSTM) son un tipo de red neuronal recurrente (RNN) diseñada para manejar el problema del gradiente que desaparece, lo que las hace efectivas para aprender dependencias a largo plazo. Se utilizan ampliamente en tareas como la predicción de series temporales, el procesamiento del lenguaje natural y el reconocimiento de voz.</p>
</div><h2>Conceptos Clave</h2>
<div class='content'><ul>
<li><strong>Redes Neuronales Recurrentes (RNNs):</strong> Redes con bucles que permiten que la información persista.</li>
<li><strong>Problema del Gradiente que Desaparece:</strong> Dificultad en el entrenamiento de RNNs debido a que los gradientes se vuelven muy pequeños.</li>
<li><strong>Célula LSTM:</strong> Una unidad especial en las LSTMs diseñada para mantener dependencias a largo plazo.</li>
</ul>
</div><h1>Entendiendo la Arquitectura de las LSTM</h1>
<div class='content'><p>Las LSTMs tienen una estructura única que incluye varias puertas para controlar el flujo de información.</p>
</div><h2>Componentes de una Célula LSTM</h2>
<div class='content'><ul>
<li><strong>Puerta de Olvido:</strong> Decide qué información descartar del estado de la célula.</li>
<li><strong>Puerta de Entrada:</strong> Decide qué valores del input actualizarán el estado de la célula.</li>
<li><strong>Estado de la Célula:</strong> La memoria de la red.</li>
<li><strong>Puerta de Salida:</strong> Decide cuál debería ser el próximo estado oculto.</li>
</ul>
</div><h2>Diagrama de la Célula LSTM</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("W0lucHV0XSAtLT4gW0ZvcmdldCBHYXRlXSAtLT4gW0NlbGwgU3RhdGVdIC0tPiBbT3V0cHV0IEdhdGVdIC0tPiBbSGlkZGVuIFN0YXRlXQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>[Input] --&gt; [Forget Gate] --&gt; [Cell State] --&gt; [Output Gate] --&gt; [Hidden State]</pre></div><div class='content'></div><h1>Implementando LSTMs en PyTorch</h1>
<div class='content'><p>Vamos a profundizar en la implementación de LSTMs usando PyTorch.</p>
</div><h2>Paso 1: Importando Librerías</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgppbXBvcnQgdG9yY2gub3B0aW0gYXMgb3B0aW0="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn
import torch.optim as optim</pre></div><div class='content'></div><h2>Paso 2: Definiendo el Modelo LSTM</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgTFNUTU1vZGVsKG5uLk1vZHVsZSk6CiAgICBkZWYgX19pbml0X18oc2VsZiwgaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemUsIG91dHB1dF9zaXplLCBudW1fbGF5ZXJzPTEpOgogICAgICAgIHN1cGVyKExTVE1Nb2RlbCwgc2VsZikuX19pbml0X18oKQogICAgICAgIHNlbGYuaGlkZGVuX3NpemUgPSBoaWRkZW5fc2l6ZQogICAgICAgIHNlbGYubnVtX2xheWVycyA9IG51bV9sYXllcnMKICAgICAgICBzZWxmLmxzdG0gPSBubi5MU1RNKGlucHV0X3NpemUsIGhpZGRlbl9zaXplLCBudW1fbGF5ZXJzLCBiYXRjaF9maXJzdD1UcnVlKQogICAgICAgIHNlbGYuZmMgPSBubi5MaW5lYXIoaGlkZGVuX3NpemUsIG91dHB1dF9zaXplKQogICAgCiAgICBkZWYgZm9yd2FyZChzZWxmLCB4KToKICAgICAgICBoMCA9IHRvcmNoLnplcm9zKHNlbGYubnVtX2xheWVycywgeC5zaXplKDApLCBzZWxmLmhpZGRlbl9zaXplKS50byh4LmRldmljZSkKICAgICAgICBjMCA9IHRvcmNoLnplcm9zKHNlbGYubnVtX2xheWVycywgeC5zaXplKDApLCBzZWxmLmhpZGRlbl9zaXplKS50byh4LmRldmljZSkKICAgICAgICAKICAgICAgICBvdXQsIF8gPSBzZWxmLmxzdG0oeCwgKGgwLCBjMCkpCiAgICAgICAgb3V0ID0gc2VsZi5mYyhvdXRbOiwgLTEsIDpdKQogICAgICAgIHJldHVybiBvdXQ="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out</pre></div><div class='content'></div><h2>Paso 3: Entrenando el Modelo LSTM</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBIaXBlcnBhcsOhbWV0cm9zCmlucHV0X3NpemUgPSAxMApoaWRkZW5fc2l6ZSA9IDUwCm91dHB1dF9zaXplID0gMQpudW1fbGF5ZXJzID0gMgpudW1fZXBvY2hzID0gMTAwCmxlYXJuaW5nX3JhdGUgPSAwLjAwMQoKIyBNb2RlbG8sIFDDqXJkaWRhLCBPcHRpbWl6YWRvcgptb2RlbCA9IExTVE1Nb2RlbChpbnB1dF9zaXplLCBoaWRkZW5fc2l6ZSwgb3V0cHV0X3NpemUsIG51bV9sYXllcnMpCmNyaXRlcmlvbiA9IG5uLk1TRUxvc3MoKQpvcHRpbWl6ZXIgPSBvcHRpbS5BZGFtKG1vZGVsLnBhcmFtZXRlcnMoKSwgbHI9bGVhcm5pbmdfcmF0ZSkKCiMgRGF0b3MgZGUgUHJ1ZWJhCnhfdHJhaW4gPSB0b3JjaC5yYW5kbigxMDAsIDEwLCBpbnB1dF9zaXplKQp5X3RyYWluID0gdG9yY2gucmFuZG4oMTAwLCBvdXRwdXRfc2l6ZSkKCiMgQnVjbGUgZGUgRW50cmVuYW1pZW50bwpmb3IgZXBvY2ggaW4gcmFuZ2UobnVtX2Vwb2Nocyk6CiAgICBtb2RlbC50cmFpbigpCiAgICBvdXRwdXRzID0gbW9kZWwoeF90cmFpbikKICAgIG9wdGltaXplci56ZXJvX2dyYWQoKQogICAgbG9zcyA9IGNyaXRlcmlvbihvdXRwdXRzLCB5X3RyYWluKQogICAgbG9zcy5iYWNrd2FyZCgpCiAgICBvcHRpbWl6ZXIuc3RlcCgpCiAgICAKICAgIGlmIChlcG9jaCsxKSAlIDEwID09IDA6CiAgICAgICAgcHJpbnQoZidFcG9jaCBbe2Vwb2NoKzF9L3tudW1fZXBvY2hzfV0sIExvc3M6IHtsb3NzLml0ZW0oKTouNGZ9Jyk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Hiperpar&aacute;metros
input_size = 10
hidden_size = 50
output_size = 1
num_layers = 2
num_epochs = 100
learning_rate = 0.001

# Modelo, P&eacute;rdida, Optimizador
model = LSTMModel(input_size, hidden_size, output_size, num_layers)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Datos de Prueba
x_train = torch.randn(100, 10, input_size)
y_train = torch.randn(100, output_size)

# Bucle de Entrenamiento
for epoch in range(num_epochs):
    model.train()
    outputs = model(x_train)
    optimizer.zero_grad()
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')</pre></div><div class='content'></div><h1>Técnicas Avanzadas de LSTM</h1>
<h2>LSTMs Bidireccionales</h2>
<div class='content'><p>Las LSTMs bidireccionales procesan datos en ambas direcciones, proporcionando contexto adicional.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgQmlMU1RNTW9kZWwobm4uTW9kdWxlKToKICAgIGRlZiBfX2luaXRfXyhzZWxmLCBpbnB1dF9zaXplLCBoaWRkZW5fc2l6ZSwgb3V0cHV0X3NpemUsIG51bV9sYXllcnM9MSk6CiAgICAgICAgc3VwZXIoQmlMU1RNTW9kZWwsIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmhpZGRlbl9zaXplID0gaGlkZGVuX3NpemUKICAgICAgICBzZWxmLm51bV9sYXllcnMgPSBudW1fbGF5ZXJzCiAgICAgICAgc2VsZi5sc3RtID0gbm4uTFNUTShpbnB1dF9zaXplLCBoaWRkZW5fc2l6ZSwgbnVtX2xheWVycywgYmF0Y2hfZmlyc3Q9VHJ1ZSwgYmlkaXJlY3Rpb25hbD1UcnVlKQogICAgICAgIHNlbGYuZmMgPSBubi5MaW5lYXIoaGlkZGVuX3NpemUqMiwgb3V0cHV0X3NpemUpCiAgICAKICAgIGRlZiBmb3J3YXJkKHNlbGYsIHgpOgogICAgICAgIGgwID0gdG9yY2guemVyb3Moc2VsZi5udW1fbGF5ZXJzKjIsIHguc2l6ZSgwKSwgc2VsZi5oaWRkZW5fc2l6ZSkudG8oeC5kZXZpY2UpCiAgICAgICAgYzAgPSB0b3JjaC56ZXJvcyhzZWxmLm51bV9sYXllcnMqMiwgeC5zaXplKDApLCBzZWxmLmhpZGRlbl9zaXplKS50byh4LmRldmljZSkKICAgICAgICAKICAgICAgICBvdXQsIF8gPSBzZWxmLmxzdG0oeCwgKGgwLCBjMCkpCiAgICAgICAgb3V0ID0gc2VsZi5mYyhvdXRbOiwgLTEsIDpdKQogICAgICAgIHJldHVybiBvdXQ="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class BiLSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(BiLSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size*2, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out</pre></div><div class='content'></div><h2>LSTM con Dropout</h2>
<div class='content'><p>Agregar dropout a las capas LSTM ayuda a prevenir el sobreajuste.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgTFNUTVdpdGhEcm9wb3V0KG5uLk1vZHVsZSk6CiAgICBkZWYgX19pbml0X18oc2VsZiwgaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemUsIG91dHB1dF9zaXplLCBudW1fbGF5ZXJzPTEsIGRyb3BvdXQ9MC41KToKICAgICAgICBzdXBlcihMU1RNV2l0aERyb3BvdXQsIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmhpZGRlbl9zaXplID0gaGlkZGVuX3NpemUKICAgICAgICBzZWxmLm51bV9sYXllcnMgPSBudW1fbGF5ZXJzCiAgICAgICAgc2VsZi5sc3RtID0gbm4uTFNUTShpbnB1dF9zaXplLCBoaWRkZW5fc2l6ZSwgbnVtX2xheWVycywgYmF0Y2hfZmlyc3Q9VHJ1ZSwgZHJvcG91dD1kcm9wb3V0KQogICAgICAgIHNlbGYuZmMgPSBubi5MaW5lYXIoaGlkZGVuX3NpemUsIG91dHB1dF9zaXplKQogICAgCiAgICBkZWYgZm9yd2FyZChzZWxmLCB4KToKICAgICAgICBoMCA9IHRvcmNoLnplcm9zKHNlbGYubnVtX2xheWVycywgeC5zaXplKDApLCBzZWxmLmhpZGRlbl9zaXplKS50byh4LmRldmljZSkKICAgICAgICBjMCA9IHRvcmNoLnplcm9zKHNlbGYubnVtX2xheWVycywgeC5zaXplKDApLCBzZWxmLmhpZGRlbl9zaXplKS50byh4LmRldmljZSkKICAgICAgICAKICAgICAgICBvdXQsIF8gPSBzZWxmLmxzdG0oeCwgKGgwLCBjMCkpCiAgICAgICAgb3V0ID0gc2VsZi5mYyhvdXRbOiwgLTEsIDpdKQogICAgICAgIHJldHVybiBvdXQ="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class LSTMWithDropout(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.5):
        super(LSTMWithDropout, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out</pre></div><div class='content'></div><h1>Conclusión</h1>
<div class='content'><p>Las LSTMs son herramientas poderosas para tareas de predicción de secuencias, superando las limitaciones de las RNNs tradicionales. Al entender su arquitectura y aprender a implementarlas en PyTorch, puedes aprovechar sus capacidades para diversas aplicaciones. Técnicas avanzadas como las LSTMs bidireccionales y el dropout pueden mejorar aún más el rendimiento y la robustez del modelo.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='recurrent-neural-networks-rnns'>&#x25C4;Redes Neuronales Recurrentes (RNNs)</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Redes de Memoria a Largo Corto Plazo (LSTMs)</a>
	</div>
	<div class='col-4 text-end'>
					<a href='transformers'>Transformers &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
