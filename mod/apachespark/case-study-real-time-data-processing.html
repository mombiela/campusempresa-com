<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Estudio de Caso: Procesamiento de Datos en Tiempo Real</title>

    <link rel="alternate" href="https://campusempresa.com/mod/apachespark/case-study-real-time-data-processing" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/apachespark/case-study-real-time-data-processing" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/apachespark/case-study-real-time-data-processing" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/apachespark/case-study-real-time-data-processing" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/apachespark/case-study-real-time-data-processing" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='integrating-spark-with-other-big-data-tools'>&#x25C4;Integración de Spark con Otras Herramientas de Big Data</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Estudio de Caso: Procesamiento de Datos en Tiempo Real</a>
	</div>
	<div class='col-4 text-end'>
					<a href='case-study-machine-learning-pipeline'>Estudio de Caso: Pipeline de Aprendizaje Automático &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introducción</h1>
<div class='content'><p>El procesamiento de datos en tiempo real es un requisito crítico para muchas aplicaciones modernas, como sistemas de comercio financiero, plataformas de redes sociales y dispositivos IoT. Apache Spark, con sus potentes capacidades de transmisión, es una herramienta ideal para manejar datos en tiempo real. Este estudio de caso te guiará a través del proceso de configuración de una canalización de procesamiento de datos en tiempo real utilizando Apache Spark, desde niveles principiantes hasta avanzados.</p>
</div><h1>Conceptos Clave</h1>
<div class='content'><ul>
<li><strong>Procesamiento de Datos en Tiempo Real</strong>: La entrada, procesamiento y salida continua de datos en tiempo real.</li>
<li><strong>Apache Spark</strong>: Un motor de análisis unificado de código abierto para el procesamiento de datos a gran escala.</li>
<li><strong>Spark Streaming</strong>: Una extensión de la API central de Spark que permite el procesamiento de flujos de datos en vivo de manera escalable, con alto rendimiento y tolerancia a fallos.</li>
</ul>
</div><h1>Configuración del Entorno</h1>
<div class='content'><p>Antes de sumergirte en el estudio de caso, asegúrate de tener lo siguiente:</p>
<ul>
<li>Apache Spark instalado.</li>
<li>Un entorno de desarrollo (por ejemplo, IntelliJ IDEA, PyCharm o Jupyter Notebook).</li>
<li>Comprensión básica de Spark y sus componentes.</li>
</ul>
</div><h1>Nivel Principiante: Transmisión Básica con Spark</h1>
<h2>Configuración de un Flujo Simple</h2>
<div class='content'><p>Comenzaremos configurando un flujo simple que lee datos de un socket.</p>
<h4>Ejemplo de Código: Flujo de Socket Simple</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBweXNwYXJrLnNxbCBpbXBvcnQgU3BhcmtTZXNzaW9uCmZyb20gcHlzcGFyay5zdHJlYW1pbmcgaW1wb3J0IFN0cmVhbWluZ0NvbnRleHQKCiMgSW5pY2lhbGl6YXIgU3BhcmsgU2Vzc2lvbgpzcGFyayA9IFNwYXJrU2Vzc2lvbi5idWlsZGVyLmFwcE5hbWUoIlNpbXBsZVN0cmVhbSIpLmdldE9yQ3JlYXRlKCkKCiMgSW5pY2lhbGl6YXIgU3RyZWFtaW5nIENvbnRleHQgY29uIHVuIGludGVydmFsbyBkZSBsb3RlIGRlIDEgc2VndW5kbwpzc2MgPSBTdHJlYW1pbmdDb250ZXh0KHNwYXJrLnNwYXJrQ29udGV4dCwgMSkKCiMgQ3JlYXIgdW4gRFN0cmVhbSBxdWUgc2UgY29uZWN0YSBhIGxvY2FsaG9zdDo5OTk5CmxpbmVzID0gc3NjLnNvY2tldFRleHRTdHJlYW0oImxvY2FsaG9zdCIsIDk5OTkpCgojIEltcHJpbWlyIGNhZGEgbMOtbmVhCmxpbmVzLnBwcmludCgpCgojIEluaWNpYXIgZWwgY8OhbGN1bG8Kc3NjLnN0YXJ0KCkKCiMgRXNwZXJhciBhIHF1ZSBlbCBjw6FsY3VsbyB0ZXJtaW5lCnNzYy5hd2FpdFRlcm1pbmF0aW9uKCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from pyspark.sql import SparkSession
from pyspark.streaming import StreamingContext

# Inicializar Spark Session
spark = SparkSession.builder.appName(&quot;SimpleStream&quot;).getOrCreate()

# Inicializar Streaming Context con un intervalo de lote de 1 segundo
ssc = StreamingContext(spark.sparkContext, 1)

# Crear un DStream que se conecta a localhost:9999
lines = ssc.socketTextStream(&quot;localhost&quot;, 9999)

# Imprimir cada l&iacute;nea
lines.pprint()

# Iniciar el c&aacute;lculo
ssc.start()

# Esperar a que el c&aacute;lculo termine
ssc.awaitTermination()</pre></div><div class='content'><p><strong>Explicación</strong>:</p>
<ul>
<li><strong>SparkSession</strong>: Punto de entrada a la funcionalidad de Spark.</li>
<li><strong>StreamingContext</strong>: Punto de entrada principal para la funcionalidad de Spark Streaming.</li>
<li><strong>socketTextStream</strong>: Se conecta a un socket para leer datos.</li>
</ul>
</div><h2>Ejecutando el Flujo</h2>
<div class='content'><ol>
<li>Inicia un servidor de socket en tu máquina:
<pre><code class="language-bash">nc -lk 9999
</code></pre>
</li>
<li>Ejecuta la aplicación de Spark Streaming.</li>
<li>Escribe mensajes en la terminal que ejecuta el servidor de socket y observa la salida en tu aplicación de Spark.</li>
</ol>
</div><h1>Nivel Intermedio: Transformaciones y Acciones</h1>
<h2>Aplicando Transformaciones</h2>
<div class='content'><p>Las transformaciones te permiten modificar el flujo de datos. Vamos a filtrar las líneas que contienen una palabra clave específica.</p>
<h4>Ejemplo de Código: Filtrando Líneas</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBGaWx0cmFyIGzDrW5lYXMgcXVlIGNvbnRpZW5lbiBsYSBwYWxhYnJhICJlcnJvciIKZXJyb3JfbGluZXMgPSBsaW5lcy5maWx0ZXIobGFtYmRhIGxpbmU6ICJlcnJvciIgaW4gbGluZSkKCiMgSW1wcmltaXIgbMOtbmVhcyBmaWx0cmFkYXMKZXJyb3JfbGluZXMucHByaW50KCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Filtrar l&iacute;neas que contienen la palabra &quot;error&quot;
error_lines = lines.filter(lambda line: &quot;error&quot; in line)

# Imprimir l&iacute;neas filtradas
error_lines.pprint()</pre></div><div class='content'><p><strong>Explicación</strong>:</p>
<ul>
<li><strong>filter</strong>: Transformación que filtra el DStream basado en una condición.</li>
</ul>
</div><h2>Realizando Acciones</h2>
<div class='content'><p>Las acciones desencadenan la ejecución de las transformaciones.</p>
<h4>Ejemplo de Código: Contando Palabras</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBEaXZpZGlyIGNhZGEgbMOtbmVhIGVuIHBhbGFicmFzCndvcmRzID0gbGluZXMuZmxhdE1hcChsYW1iZGEgbGluZTogbGluZS5zcGxpdCgiICIpKQoKIyBDb250YXIgY2FkYSBwYWxhYnJhIGVuIGNhZGEgbG90ZQpwYWlycyA9IHdvcmRzLm1hcChsYW1iZGEgd29yZDogKHdvcmQsIDEpKQp3b3JkX2NvdW50cyA9IHBhaXJzLnJlZHVjZUJ5S2V5KGxhbWJkYSB4LCB5OiB4ICsgeSkKCiMgSW1wcmltaXIgZWwgY29udGVvIGRlIHBhbGFicmFzCndvcmRfY291bnRzLnBwcmludCgp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Dividir cada l&iacute;nea en palabras
words = lines.flatMap(lambda line: line.split(&quot; &quot;))

# Contar cada palabra en cada lote
pairs = words.map(lambda word: (word, 1))
word_counts = pairs.reduceByKey(lambda x, y: x + y)

# Imprimir el conteo de palabras
word_counts.pprint()</pre></div><div class='content'><p><strong>Explicación</strong>:</p>
<ul>
<li><strong>flatMap</strong>: Transformación que divide cada línea en palabras.</li>
<li><strong>map</strong>: Transformación que mapea cada palabra a un par (palabra, 1).</li>
<li><strong>reduceByKey</strong>: Agrega los conteos de cada palabra.</li>
</ul>
</div><h1>Nivel Avanzado: Integración con Kafka</h1>
<h2>Configuración de Kafka</h2>
<div class='content'><p>Kafka es una plataforma de transmisión distribuida que se puede utilizar para construir canalizaciones de datos en tiempo real.</p>
<h4>Ejemplo de Código: Integración con Kafka</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBweXNwYXJrLnN0cmVhbWluZy5rYWZrYSBpbXBvcnQgS2Fma2FVdGlscwoKIyBDcmVhciB1biBEU3RyZWFtIHF1ZSBzZSBjb25lY3RhIGEgS2Fma2EKa2Fma2Ffc3RyZWFtID0gS2Fma2FVdGlscy5jcmVhdGVTdHJlYW0oc3NjLCAibG9jYWxob3N0OjIxODEiLCAic3Bhcmstc3RyZWFtaW5nIiwgeyJ0ZXN0LXRvcGljIjogMX0pCgojIEV4dHJhZXIgbG9zIG1lbnNhamVzIGRlIEthZmthCm1lc3NhZ2VzID0ga2Fma2Ffc3RyZWFtLm1hcChsYW1iZGEgbXNnOiBtc2dbMV0pCgojIEltcHJpbWlyIGxvcyBtZW5zYWplcwptZXNzYWdlcy5wcHJpbnQoKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from pyspark.streaming.kafka import KafkaUtils

# Crear un DStream que se conecta a Kafka
kafka_stream = KafkaUtils.createStream(ssc, &quot;localhost:2181&quot;, &quot;spark-streaming&quot;, {&quot;test-topic&quot;: 1})

# Extraer los mensajes de Kafka
messages = kafka_stream.map(lambda msg: msg[1])

# Imprimir los mensajes
messages.pprint()</pre></div><div class='content'><p><strong>Explicación</strong>:</p>
<ul>
<li><strong>KafkaUtils.createStream</strong>: Se conecta a un tema de Kafka.</li>
<li><strong>map</strong>: Extrae la parte del mensaje del registro de Kafka.</li>
</ul>
</div><h2>Transformaciones Avanzadas</h2>
<div class='content'><p>Vamos a realizar operaciones con ventanas para calcular estadísticas sobre una ventana deslizante de datos.</p>
<h4>Ejemplo de Código: Operaciones con Ventanas</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBDYWxjdWxhciBlbCBjb250ZW8gZGUgcGFsYWJyYXMgc29icmUgdW5hIHZlbnRhbmEgZGUgMTAgc2VndW5kb3MgZGVzbGl6w6FuZG9zZSBjYWRhIDUgc2VndW5kb3MKd2luZG93ZWRfd29yZF9jb3VudHMgPSB3b3JkX2NvdW50cy5yZWR1Y2VCeUtleUFuZFdpbmRvdyhsYW1iZGEgeCwgeTogeCArIHksIDEwLCA1KQoKIyBJbXByaW1pciBlbCBjb250ZW8gZGUgcGFsYWJyYXMgZW4gbGEgdmVudGFuYQp3aW5kb3dlZF93b3JkX2NvdW50cy5wcHJpbnQoKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Calcular el conteo de palabras sobre una ventana de 10 segundos desliz&aacute;ndose cada 5 segundos
windowed_word_counts = word_counts.reduceByKeyAndWindow(lambda x, y: x + y, 10, 5)

# Imprimir el conteo de palabras en la ventana
windowed_word_counts.pprint()</pre></div><div class='content'><p><strong>Explicación</strong>:</p>
<ul>
<li><strong>reduceByKeyAndWindow</strong>: Agrega datos sobre una ventana deslizante.</li>
</ul>
</div><h1>Conclusión</h1>
<div class='content'><p>En este estudio de caso, exploramos los conceptos básicos del procesamiento de datos en tiempo real utilizando Apache Spark, comenzando desde la configuración de un flujo simple hasta la integración con Kafka y la realización de transformaciones avanzadas. El procesamiento de datos en tiempo real con Spark es una herramienta poderosa que puede manejar datos a gran escala de manera eficiente y efectiva. Al comprender estos conceptos y practicar con los ejemplos proporcionados, puedes construir canalizaciones de procesamiento de datos en tiempo real robustas para diversas aplicaciones.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='integrating-spark-with-other-big-data-tools'>&#x25C4;Integración de Spark con Otras Herramientas de Big Data</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Estudio de Caso: Procesamiento de Datos en Tiempo Real</a>
	</div>
	<div class='col-4 text-end'>
					<a href='case-study-machine-learning-pipeline'>Estudio de Caso: Pipeline de Aprendizaje Automático &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
