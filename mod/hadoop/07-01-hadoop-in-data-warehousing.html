<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hadoop en Almacenamiento de Datos</title>

    <link rel="alternate" href="https://campusempresa.com/mod/hadoop/07-01-hadoop-in-data-warehousing" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/hadoop/07-01-hadoop-in-data-warehousing" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/hadoop/07-01-hadoop-in-data-warehousing" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-2 p-md-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/hadoop/07-01-hadoop-in-data-warehousing" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/hadoop/07-01-hadoop-in-data-warehousing" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='06-04-hadoop-data-serialization' title="Serialización de Datos en Hadoop">&#x25C4;Anterior</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Hadoop en Almacenamiento de Datos</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='07-02-hadoop-in-machine-learning' title="Hadoop en Aprendizaje Automático">Siguiente &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1><p>Introducción</p>
</h1>
<div class='content'><p>El almacenamiento de datos es una parte crucial de la gestión de grandes volúmenes de información en las organizaciones modernas. Hadoop, con su capacidad para manejar grandes cantidades de datos distribuidos, se ha convertido en una herramienta esencial para el almacenamiento de datos. En esta sección, exploraremos cómo Hadoop se utiliza en el almacenamiento de datos, sus ventajas y cómo implementarlo.</p>
</div><h1><p>¿Qué es el Almacenamiento de Datos?</p>
</h1>
<div class='content'><p>El almacenamiento de datos (Data Warehousing) es el proceso de recopilar y gestionar datos de diversas fuentes para proporcionar información significativa y útil para la toma de decisiones empresariales. Los almacenes de datos son sistemas diseñados para el análisis y la consulta de datos, en lugar de la transacción de datos.</p>
</div><h2><p>Características Clave del Almacenamiento de Datos:</p>
</h2>
<div class='content'><ul>
<li><strong>Integración de Datos</strong>: Combina datos de múltiples fuentes.</li>
<li><strong>Historización</strong>: Almacena datos históricos para análisis a largo plazo.</li>
<li><strong>Optimización para Consultas</strong>: Diseñado para consultas rápidas y complejas.</li>
<li><strong>Consolidación de Datos</strong>: Proporciona una vista unificada de los datos.</li>
</ul>
</div><h1><p>Ventajas de Usar Hadoop en el Almacenamiento de Datos</p>
</h1>
<div class='content'><p>Hadoop ofrece varias ventajas significativas cuando se utiliza para el almacenamiento de datos:</p>
<ol>
<li><strong>Escalabilidad</strong>: Hadoop puede escalar horizontalmente añadiendo más nodos al clúster, lo que permite manejar grandes volúmenes de datos.</li>
<li><strong>Costo-Efectividad</strong>: Utiliza hardware común y de bajo costo, lo que reduce significativamente los costos en comparación con soluciones tradicionales.</li>
<li><strong>Flexibilidad</strong>: Puede manejar datos estructurados, semiestructurados y no estructurados.</li>
<li><strong>Alta Disponibilidad y Tolerancia a Fallos</strong>: HDFS (Hadoop Distributed File System) replica los datos en múltiples nodos, asegurando la disponibilidad y la tolerancia a fallos.</li>
<li><strong>Procesamiento Distribuido</strong>: MapReduce permite el procesamiento paralelo de grandes conjuntos de datos.</li>
</ol>
</div><h1><p>Componentes de Hadoop en el Almacenamiento de Datos</p>
</h1>
<div class='content'></div><h2><p>HDFS (Hadoop Distributed File System)</p>
</h2>
<div class='content'><p>HDFS es el sistema de archivos distribuido de Hadoop que almacena datos en bloques distribuidos a través de múltiples nodos. Proporciona alta disponibilidad y tolerancia a fallos mediante la replicación de datos.</p>
</div><h2><p>MapReduce</p>
</h2>
<div class='content'><p>MapReduce es el modelo de programación de Hadoop que permite el procesamiento paralelo de grandes conjuntos de datos. Divide las tareas en subtareas más pequeñas que se ejecutan en paralelo en diferentes nodos.</p>
</div><h2><p>Apache Hive</p>
</h2>
<div class='content'><p>Hive es una herramienta de almacenamiento de datos que proporciona una interfaz SQL para consultar y gestionar grandes conjuntos de datos almacenados en Hadoop. Permite a los usuarios escribir consultas en un lenguaje similar a SQL llamado HiveQL.</p>
</div><h2><p>Apache HBase</p>
</h2>
<div class='content'><p>HBase es una base de datos NoSQL distribuida que se ejecuta sobre HDFS. Es adecuada para aplicaciones que requieren acceso en tiempo real a grandes cantidades de datos.</p>
</div><h2><p>Apache Sqoop</p>
</h2>
<div class='content'><p>Sqoop es una herramienta que permite la transferencia eficiente de datos entre Hadoop y bases de datos relacionales. Es útil para importar datos desde bases de datos tradicionales a Hadoop y viceversa.</p>
</div><h2><p>Apache Flume</p>
</h2>
<div class='content'><p>Flume es una herramienta de ingesta de datos que permite la recolección, agregación y movimiento de grandes cantidades de datos de múltiples fuentes a Hadoop.</p>
</div><h1><p>Implementación de un Almacén de Datos con Hadoop</p>
</h1>
<div class='content'></div><h2><p>Paso 1: Configuración del Entorno Hadoop</p>
</h2>
<div class='content'><p>Antes de comenzar, asegúrate de tener un clúster de Hadoop configurado. Puedes seguir las instrucciones del módulo &quot;Configuración del Entorno Hadoop&quot; para configurar tu clúster.</p>
</div><h2><p>Paso 2: Ingesta de Datos</p>
</h2>
<div class='content'><p>Utiliza Apache Sqoop para importar datos desde una base de datos relacional a HDFS. Aquí hay un ejemplo de cómo importar datos desde MySQL a HDFS:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("c3Fvb3AgaW1wb3J0IC0tY29ubmVjdCBqZGJjOm15c3FsOi8vbG9jYWxob3N0L2RibmFtZSAtLXVzZXJuYW1lIHVzZXIgLS1wYXNzd29yZCBwYXNzIC0tdGFibGUgdGFibGVuYW1lIC0tdGFyZ2V0LWRpciAvdXNlci9oYWRvb3AvdGFibGVuYW1l"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>sqoop import --connect jdbc:mysql://localhost/dbname --username user --password pass --table tablename --target-dir /user/hadoop/tablename</pre></div><div class='content'></div><h2><p>Paso 3: Almacenamiento de Datos en HDFS</p>
</h2>
<div class='content'><p>Los datos importados se almacenarán en HDFS. Puedes verificar los datos utilizando comandos de HDFS:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aGRmcyBkZnMgLWxzIC91c2VyL2hhZG9vcC90YWJsZW5hbWU="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>hdfs dfs -ls /user/hadoop/tablename</pre></div><div class='content'></div><h2><p>Paso 4: Procesamiento de Datos con MapReduce</p>
</h2>
<div class='content'><p>Escribe un programa MapReduce para procesar los datos almacenados en HDFS. Aquí hay un ejemplo básico de un programa MapReduce en Java:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class WordCount {

    public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            String[] tokens = value.toString().split(&quot;\\s+&quot;);
            for (String token : tokens) {
                word.set(token);
                context.write(word, one);
            }
        }
    }

    public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, &quot;word count&quot;);
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}</pre></div><div class='content'></div><h2><p>Paso 5: Consulta de Datos con Hive</p>
</h2>
<div class='content'><p>Utiliza Apache Hive para consultar los datos procesados. Aquí hay un ejemplo de cómo crear una tabla en Hive y ejecutar una consulta:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Q1JFQVRFIFRBQkxFIHdvcmRfY291bnQgKHdvcmQgU1RSSU5HLCBjb3VudCBJTlQpIFJPVyBGT1JNQVQgREVMSU1JVEVEIEZJRUxEUyBURVJNSU5BVEVEIEJZICdcdCcgU1RPUkVEIEFTIFRFWFRGSUxFOwoKTE9BRCBEQVRBIElOUEFUSCAnL3VzZXIvaGFkb29wL291dHB1dCcgSU5UTyBUQUJMRSB3b3JkX2NvdW50OwoKU0VMRUNUICogRlJPTSB3b3JkX2NvdW50IFdIRVJFIGNvdW50ID4gMTA7"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>CREATE TABLE word_count (word STRING, count INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;

LOAD DATA INPATH '/user/hadoop/output' INTO TABLE word_count;

SELECT * FROM word_count WHERE count &gt; 10;</pre></div><div class='content'></div><h1><p>Ejercicio Práctico</p>
</h1>
<div class='content'></div><h2><p>Ejercicio 1: Importar y Procesar Datos</p>
</h2>
<div class='content'><ol>
<li><strong>Importar Datos</strong>: Utiliza Apache Sqoop para importar datos desde una base de datos relacional a HDFS.</li>
<li><strong>Procesar Datos</strong>: Escribe un programa MapReduce para contar la frecuencia de palabras en los datos importados.</li>
<li><strong>Consultar Datos</strong>: Utiliza Apache Hive para consultar los resultados del programa MapReduce.</li>
</ol>
</div><h2><p>Solución del Ejercicio</p>
</h2>
<div class='content'><ol>
<li>
<p><strong>Importar Datos</strong>:</p>
<pre><code class="language-bash">sqoop import --connect jdbc:mysql://localhost/dbname --username user --password pass --table tablename --target-dir /user/hadoop/tablename
</code></pre>
</li>
<li>
<p><strong>Procesar Datos</strong>: Utiliza el programa MapReduce proporcionado anteriormente.</p>
</li>
<li>
<p><strong>Consultar Datos</strong>:</p>
<pre><code class="language-sql">CREATE TABLE word_count (word STRING, count INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;

LOAD DATA INPATH '/user/hadoop/output' INTO TABLE word_count;

SELECT * FROM word_count WHERE count &gt; 10;
</code></pre>
</li>
</ol>
</div><h1><p>Conclusión</p>
</h1>
<div class='content'><p>En esta sección, hemos explorado cómo Hadoop se utiliza en el almacenamiento de datos, sus ventajas y cómo implementarlo utilizando componentes clave como HDFS, MapReduce, Hive, HBase, Sqoop y Flume. También hemos proporcionado un ejercicio práctico para reforzar los conceptos aprendidos. En la siguiente sección, profundizaremos en el uso de Hadoop en el aprendizaje automático.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='06-04-hadoop-data-serialization' title="Serialización de Datos en Hadoop">&#x25C4;Anterior</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='07-02-hadoop-in-machine-learning' title="Hadoop en Aprendizaje Automático">Siguiente &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Usamos cookies para mejorar tu experiencia de uso y ofrecer contenidos adaptados a tus intereses.
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
