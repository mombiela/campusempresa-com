<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Análisis Multivariante</title>

    <link rel="alternate" href="https://campusempresa.com/mod/estadistica/07-02-analisis-multivariante" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/estadistica/07-02-analisi-multivariant" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/estadistica/07-02-multivariate-analysis" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/estadistica/07-02-multivariate-analysis" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/estadistica/07-02-analisi-multivariant" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='07-01-series-temporales' title="Series Temporales">&#x25C4;Anterior</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Análisis Multivariante</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='07-03-metodos-no-parametricos' title="Métodos No Paramétricos">Siguiente &#x25BA;</a>
			</div>
</div>
<div class='content'><p>El análisis multivariante es una técnica estadística que se utiliza para analizar datos que involucran múltiples variables al mismo tiempo. Este tipo de análisis es esencial cuando se quiere entender las relaciones complejas entre variables y cómo estas pueden influir en un resultado o conjunto de resultados.</p>
</div><h1>Conceptos Clave</h1>
<div class='content'><ol>
<li>
<p><strong>Variables Dependientes e Independientes</strong>:</p>
<ul>
<li><strong>Variables Dependientes</strong>: Son las variables que se intentan predecir o explicar.</li>
<li><strong>Variables Independientes</strong>: Son las variables que se utilizan para predecir o explicar las variables dependientes.</li>
</ul>
</li>
<li>
<p><strong>Matriz de Datos</strong>:</p>
<ul>
<li>Una matriz de datos es una representación tabular donde las filas representan observaciones y las columnas representan variables.</li>
</ul>
</li>
<li>
<p><strong>Dimensionalidad</strong>:</p>
<ul>
<li>Se refiere al número de variables en el análisis. El análisis multivariante maneja datos de alta dimensionalidad.</li>
</ul>
</li>
</ol>
</div><h1>Técnicas de Análisis Multivariante</h1>
<div class='content'></div><h2>1. Análisis de Componentes Principales (PCA)</h2>
<div class='content'><p>El PCA es una técnica utilizada para reducir la dimensionalidad de un conjunto de datos, transformando las variables originales en un nuevo conjunto de variables no correlacionadas llamadas componentes principales.</p>
<h4>Pasos del PCA:</h4>
<ol>
<li><strong>Estandarización de Datos</strong>: Asegurarse de que todas las variables tengan la misma escala.</li>
<li><strong>Cálculo de la Matriz de Covarianza</strong>: Evaluar cómo varían las variables juntas.</li>
<li><strong>Cálculo de los Valores y Vectores Propios</strong>: Determinar las direcciones principales de variación.</li>
<li><strong>Selección de Componentes Principales</strong>: Elegir los componentes que explican la mayor parte de la variación.</li>
</ol>
<h4>Ejemplo en Python:</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmZyb20gc2tsZWFybi5kZWNvbXBvc2l0aW9uIGltcG9ydCBQQ0EKaW1wb3J0IG1hdHBsb3RsaWIucHlwbG90IGFzIHBsdAoKIyBEYXRvcyBkZSBlamVtcGxvClggPSBucC5hcnJheShbWzIuNSwgMi40XSwKICAgICAgICAgICAgICBbMC41LCAwLjddLAogICAgICAgICAgICAgIFsyLjIsIDIuOV0sCiAgICAgICAgICAgICAgWzEuOSwgMi4yXSwKICAgICAgICAgICAgICBbMy4xLCAzLjBdLAogICAgICAgICAgICAgIFsyLjMsIDIuN10sCiAgICAgICAgICAgICAgWzIsIDEuNl0sCiAgICAgICAgICAgICAgWzEsIDEuMV0sCiAgICAgICAgICAgICAgWzEuNSwgMS42XSwKICAgICAgICAgICAgICBbMS4xLCAwLjldXSkKCiMgRXN0YW5kYXJpemFjacOzbiBkZSBkYXRvcwpYX21lYW5lZCA9IFggLSBucC5tZWFuKFgsIGF4aXM9MCkKCiMgQ8OhbGN1bG8gZGUgbGEgbWF0cml6IGRlIGNvdmFyaWFuemEKY292X21hdHJpeCA9IG5wLmNvdihYX21lYW5lZCwgcm93dmFyPUZhbHNlKQoKIyBDw6FsY3VsbyBkZSBsb3MgdmFsb3JlcyB5IHZlY3RvcmVzIHByb3Bpb3MKZWlnZW5fdmFsdWVzLCBlaWdlbl92ZWN0b3JzID0gbnAubGluYWxnLmVpZ2goY292X21hdHJpeCkKCiMgT3JkZW5hciBsb3MgdmFsb3JlcyBwcm9waW9zIGVuIG9yZGVuIGRlc2NlbmRlbnRlCnNvcnRlZF9pbmRleCA9IG5wLmFyZ3NvcnQoZWlnZW5fdmFsdWVzKVs6Oi0xXQpzb3J0ZWRfZWlnZW52YWx1ZSA9IGVpZ2VuX3ZhbHVlc1tzb3J0ZWRfaW5kZXhdCnNvcnRlZF9laWdlbnZlY3RvcnMgPSBlaWdlbl92ZWN0b3JzWzosIHNvcnRlZF9pbmRleF0KCiMgU2VsZWNjacOzbiBkZSBjb21wb25lbnRlcyBwcmluY2lwYWxlcwpuX2NvbXBvbmVudHMgPSAyCmVpZ2VudmVjdG9yX3N1YnNldCA9IHNvcnRlZF9laWdlbnZlY3RvcnNbOiwgMDpuX2NvbXBvbmVudHNdCgojIFRyYW5zZm9ybWFjacOzbiBkZSBkYXRvcwpYX3JlZHVjZWQgPSBucC5kb3QoZWlnZW52ZWN0b3Jfc3Vic2V0LnRyYW5zcG9zZSgpLCBYX21lYW5lZC50cmFuc3Bvc2UoKSkudHJhbnNwb3NlKCkKCiMgVmlzdWFsaXphY2nDs24gZGUgbG9zIGRhdG9zIHRyYW5zZm9ybWFkb3MKcGx0LnNjYXR0ZXIoWF9yZWR1Y2VkWzosIDBdLCBYX3JlZHVjZWRbOiwgMV0pCnBsdC54bGFiZWwoJ0NvbXBvbmVudGUgUHJpbmNpcGFsIDEnKQpwbHQueWxhYmVsKCdDb21wb25lbnRlIFByaW5jaXBhbCAyJykKcGx0LnRpdGxlKCdQQ0EgLSBSZWR1Y2Npw7NuIGRlIERpbWVuc2lvbmFsaWRhZCcpCnBsdC5zaG93KCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Datos de ejemplo
X = np.array([[2.5, 2.4],
              [0.5, 0.7],
              [2.2, 2.9],
              [1.9, 2.2],
              [3.1, 3.0],
              [2.3, 2.7],
              [2, 1.6],
              [1, 1.1],
              [1.5, 1.6],
              [1.1, 0.9]])

# Estandarizaci&oacute;n de datos
X_meaned = X - np.mean(X, axis=0)

# C&aacute;lculo de la matriz de covarianza
cov_matrix = np.cov(X_meaned, rowvar=False)

# C&aacute;lculo de los valores y vectores propios
eigen_values, eigen_vectors = np.linalg.eigh(cov_matrix)

# Ordenar los valores propios en orden descendente
sorted_index = np.argsort(eigen_values)[::-1]
sorted_eigenvalue = eigen_values[sorted_index]
sorted_eigenvectors = eigen_vectors[:, sorted_index]

# Selecci&oacute;n de componentes principales
n_components = 2
eigenvector_subset = sorted_eigenvectors[:, 0:n_components]

# Transformaci&oacute;n de datos
X_reduced = np.dot(eigenvector_subset.transpose(), X_meaned.transpose()).transpose()

# Visualizaci&oacute;n de los datos transformados
plt.scatter(X_reduced[:, 0], X_reduced[:, 1])
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.title('PCA - Reducci&oacute;n de Dimensionalidad')
plt.show()</pre></div><div class='content'></div><h2>2. Análisis de Clústeres</h2>
<div class='content'><p>El análisis de clústeres agrupa un conjunto de objetos de tal manera que los objetos en el mismo grupo (o clúster) son más similares entre sí que con los de otros grupos.</p>
<h4>Algoritmos Comunes:</h4>
<ul>
<li><strong>K-Means</strong>: Agrupa los datos en k clústeres basándose en la minimización de la varianza dentro de cada clúster.</li>
<li><strong>Jerárquico</strong>: Construye una jerarquía de clústeres mediante la combinación o división de clústeres existentes.</li>
</ul>
<h4>Ejemplo en Python (K-Means):</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmNsdXN0ZXIgaW1wb3J0IEtNZWFucwppbXBvcnQgbWF0cGxvdGxpYi5weXBsb3QgYXMgcGx0CgojIERhdG9zIGRlIGVqZW1wbG8KWCA9IG5wLmFycmF5KFtbMSwgMl0sIFsxLjUsIDEuOF0sIFs1LCA4XSwgWzgsIDhdLCBbMSwgMC42XSwgWzksIDExXV0pCgojIEFwbGljYWNpw7NuIGRlIEstTWVhbnMKa21lYW5zID0gS01lYW5zKG5fY2x1c3RlcnM9MikKa21lYW5zLmZpdChYKQoKIyBQcmVkaWNjacOzbiBkZSBjbMO6c3RlcmVzCnlfa21lYW5zID0ga21lYW5zLnByZWRpY3QoWCkKCiMgVmlzdWFsaXphY2nDs24gZGUgY2zDunN0ZXJlcwpwbHQuc2NhdHRlcihYWzosIDBdLCBYWzosIDFdLCBjPXlfa21lYW5zLCBzPTUwLCBjbWFwPSd2aXJpZGlzJykKY2VudGVycyA9IGttZWFucy5jbHVzdGVyX2NlbnRlcnNfCnBsdC5zY2F0dGVyKGNlbnRlcnNbOiwgMF0sIGNlbnRlcnNbOiwgMV0sIGM9J3JlZCcsIHM9MjAwLCBhbHBoYT0wLjc1KQpwbHQudGl0bGUoJ0stTWVhbnMgQ2x1c3RlcmluZycpCnBsdC5zaG93KCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Datos de ejemplo
X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])

# Aplicaci&oacute;n de K-Means
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)

# Predicci&oacute;n de cl&uacute;steres
y_kmeans = kmeans.predict(X)

# Visualizaci&oacute;n de cl&uacute;steres
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75)
plt.title('K-Means Clustering')
plt.show()</pre></div><div class='content'></div><h2>3. Análisis Discriminante Lineal (LDA)</h2>
<div class='content'><p>El LDA es una técnica utilizada para encontrar una combinación lineal de características que separa dos o más clases de objetos o eventos.</p>
<h4>Pasos del LDA:</h4>
<ol>
<li><strong>Cálculo de la Media de Cada Clase</strong>.</li>
<li><strong>Cálculo de la Matriz de Dispersión Dentro de la Clase y Entre Clases</strong>.</li>
<li><strong>Cálculo de los Vectores Propios y Valores Propios</strong>.</li>
<li><strong>Proyección de los Datos en el Nuevo Espacio</strong>.</li>
</ol>
<h4>Ejemplo en Python:</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRpc2NyaW1pbmFudF9hbmFseXNpcyBpbXBvcnQgTGluZWFyRGlzY3JpbWluYW50QW5hbHlzaXMgYXMgTERBCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKCiMgRGF0b3MgZGUgZWplbXBsbwpYID0gbnAuYXJyYXkoW1s0LCAyXSwgWzIsIDRdLCBbMiwgM10sIFszLCA2XSwgWzQsIDRdLCBbOSwgMTBdLCBbNiwgOF0sIFs5LCA1XSwgWzgsIDddLCBbMTAsIDhdXSkKeSA9IG5wLmFycmF5KFswLCAwLCAwLCAwLCAwLCAxLCAxLCAxLCAxLCAxXSkKCiMgQXBsaWNhY2nDs24gZGUgTERBCmxkYSA9IExEQShuX2NvbXBvbmVudHM9MSkKWF9yMiA9IGxkYS5maXQoWCwgeSkudHJhbnNmb3JtKFgpCgojIFZpc3VhbGl6YWNpw7NuIGRlIGxvcyBkYXRvcyB0cmFuc2Zvcm1hZG9zCnBsdC5zY2F0dGVyKFhfcjIsIG5wLnplcm9zX2xpa2UoWF9yMiksIGM9eSwgY21hcD0ndmlyaWRpcycpCnBsdC54bGFiZWwoJ0NvbXBvbmVudGUgRGlzY3JpbWluYW50ZSAxJykKcGx0LnRpdGxlKCdMREEgLSBSZWR1Y2Npw7NuIGRlIERpbWVuc2lvbmFsaWRhZCcpCnBsdC5zaG93KCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
import matplotlib.pyplot as plt

# Datos de ejemplo
X = np.array([[4, 2], [2, 4], [2, 3], [3, 6], [4, 4], [9, 10], [6, 8], [9, 5], [8, 7], [10, 8]])
y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

# Aplicaci&oacute;n de LDA
lda = LDA(n_components=1)
X_r2 = lda.fit(X, y).transform(X)

# Visualizaci&oacute;n de los datos transformados
plt.scatter(X_r2, np.zeros_like(X_r2), c=y, cmap='viridis')
plt.xlabel('Componente Discriminante 1')
plt.title('LDA - Reducci&oacute;n de Dimensionalidad')
plt.show()</pre></div><div class='content'></div><h1>Ejercicios Prácticos</h1>
<div class='content'></div><h2>Ejercicio 1: Aplicación de PCA</h2>
<div class='content'><p><strong>Instrucciones</strong>:</p>
<ol>
<li>Utiliza el conjunto de datos <code>Iris</code> de <code>sklearn.datasets</code>.</li>
<li>Aplica PCA para reducir las dimensiones a 2 componentes principales.</li>
<li>Visualiza los datos transformados en un gráfico de dispersión.</li>
</ol>
<p><strong>Solución</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBsb2FkX2lyaXMKZnJvbSBza2xlYXJuLmRlY29tcG9zaXRpb24gaW1wb3J0IFBDQQppbXBvcnQgbWF0cGxvdGxpYi5weXBsb3QgYXMgcGx0CgojIENhcmdhciBkYXRvcyBJcmlzCmlyaXMgPSBsb2FkX2lyaXMoKQpYID0gaXJpcy5kYXRhCnkgPSBpcmlzLnRhcmdldAoKIyBBcGxpY2FjacOzbiBkZSBQQ0EKcGNhID0gUENBKG5fY29tcG9uZW50cz0yKQpYX3BjYSA9IHBjYS5maXRfdHJhbnNmb3JtKFgpCgojIFZpc3VhbGl6YWNpw7NuIGRlIGxvcyBkYXRvcyB0cmFuc2Zvcm1hZG9zCnBsdC5zY2F0dGVyKFhfcGNhWzosIDBdLCBYX3BjYVs6LCAxXSwgYz15LCBjbWFwPSd2aXJpZGlzJykKcGx0LnhsYWJlbCgnQ29tcG9uZW50ZSBQcmluY2lwYWwgMScpCnBsdC55bGFiZWwoJ0NvbXBvbmVudGUgUHJpbmNpcGFsIDInKQpwbHQudGl0bGUoJ1BDQSAtIElyaXMgRGF0YXNldCcpCnBsdC5zaG93KCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Cargar datos Iris
iris = load_iris()
X = iris.data
y = iris.target

# Aplicaci&oacute;n de PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Visualizaci&oacute;n de los datos transformados
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.title('PCA - Iris Dataset')
plt.show()</pre></div><div class='content'></div><h2>Ejercicio 2: Análisis de Clústeres con K-Means</h2>
<div class='content'><p><strong>Instrucciones</strong>:</p>
<ol>
<li>Genera un conjunto de datos sintético utilizando <code>make_blobs</code> de <code>sklearn.datasets</code>.</li>
<li>Aplica K-Means para agrupar los datos en 3 clústeres.</li>
<li>Visualiza los clústeres y sus centros.</li>
</ol>
<p><strong>Solución</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBtYWtlX2Jsb2JzCmZyb20gc2tsZWFybi5jbHVzdGVyIGltcG9ydCBLTWVhbnMKaW1wb3J0IG1hdHBsb3RsaWIucHlwbG90IGFzIHBsdAoKIyBHZW5lcmFyIGRhdG9zIHNpbnTDqXRpY29zClgsIF8gPSBtYWtlX2Jsb2JzKG5fc2FtcGxlcz0zMDAsIGNlbnRlcnM9MywgY2x1c3Rlcl9zdGQ9MC42MCwgcmFuZG9tX3N0YXRlPTApCgojIEFwbGljYWNpw7NuIGRlIEstTWVhbnMKa21lYW5zID0gS01lYW5zKG5fY2x1c3RlcnM9MykKa21lYW5zLmZpdChYKQp5X2ttZWFucyA9IGttZWFucy5wcmVkaWN0KFgpCgojIFZpc3VhbGl6YWNpw7NuIGRlIGNsw7pzdGVyZXMKcGx0LnNjYXR0ZXIoWFs6LCAwXSwgWFs6LCAxXSwgYz15X2ttZWFucywgcz01MCwgY21hcD0ndmlyaWRpcycpCmNlbnRlcnMgPSBrbWVhbnMuY2x1c3Rlcl9jZW50ZXJzXwpwbHQuc2NhdHRlcihjZW50ZXJzWzosIDBdLCBjZW50ZXJzWzosIDFdLCBjPSdyZWQnLCBzPTIwMCwgYWxwaGE9MC43NSkKcGx0LnRpdGxlKCdLLU1lYW5zIENsdXN0ZXJpbmcnKQpwbHQuc2hvdygp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Generar datos sint&eacute;ticos
X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)

# Aplicaci&oacute;n de K-Means
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)

# Visualizaci&oacute;n de cl&uacute;steres
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75)
plt.title('K-Means Clustering')
plt.show()</pre></div><div class='content'></div><h1>Conclusión</h1>
<div class='content'><p>El análisis multivariante es una herramienta poderosa para entender y explorar datos complejos con múltiples variables. Técnicas como PCA, análisis de clústeres y LDA permiten reducir la dimensionalidad, agrupar datos y discriminar entre diferentes clases, respectivamente. La práctica con ejemplos y ejercicios ayuda a consolidar estos conceptos y a aplicarlos en situaciones del mundo real.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='07-01-series-temporales' title="Series Temporales">&#x25C4;Anterior</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='07-03-metodos-no-parametricos' title="Métodos No Paramétricos">Siguiente &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
