<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Funciones de Activación</title>

    <link rel="alternate" href="https://campusempresa.com/mod/tensorflow/04-03-activation-functions" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/tensorflow/04-03-activation-functions" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/tensorflow/04-03-activation-functions" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
				<a href="https://enterprisecampus.net/mod/tensorflow/04-03-activation-functions" class="px-2">EN</a></b>
	|
	<b class="px-2">ES</b>
	|
	<a href="https://campusempresa.cat/mod/tensorflow/04-03-activation-functions" class="px-2">CA</a>
<br>
			<cite>Construyendo la sociedad de hoy y del mañana</cite>
		</div>
	</div>
</div>
<div id="subheader" class="container-xxl">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Proyecto</a> | 
<a href="/about">Sobre nosotros</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donaciones</a> | 
<a href="/licence">Licencia</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				 					<a href="/categ/languages">Lenguajes de Programación</a>
				 					<a href="/categ/frameworks">Frameworks y Librerías</a>
				 					<a href="/categ/tech-tools">Herramientas Técnicas</a>
				 					<a href="/categ/foundations">Fundamentos Teóricos</a>
				 					<a href="/categ/soft-skills">Habilidades Sociales</a>
							</div>
		</div>
	</div>
</div>
		
<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='04-02-creating-a-simple-neural-network' title="Creando una Red Neuronal Simple">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Funciones de Activación</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='04-04-loss-functions-and-optimizers' title="Funciones de Pérdida y Optimizadores">
				<span class="d-none d-md-inline">Siguiente &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>Las funciones de activación son un componente crucial en las redes neuronales, ya que introducen no linealidades en el modelo, permitiendo que la red aprenda y represente relaciones complejas en los datos. En esta sección, exploraremos las funciones de activación más comunes y cómo implementarlas en TensorFlow.</p>
</div><h1><p>¿Qué es una Función de Activación?</p>
</h1>
<div class='content'><p>Una función de activación es una función matemática que se aplica a la salida de una neurona en una red neuronal. Su propósito principal es introducir no linealidades en el modelo, lo que permite a la red neuronal aprender y modelar datos complejos.</p>
</div><h2><p>Propiedades Clave de las Funciones de Activación</p>
</h2>
<div class='content'><ol>
<li><strong>No Linealidad</strong>: Permite que la red neuronal aprenda relaciones complejas.</li>
<li><strong>Derivabilidad</strong>: La función debe ser diferenciable para permitir la retropropagación.</li>
<li><strong>Rango de Salida</strong>: El rango de valores que la función puede producir.</li>
<li><strong>Saturación</strong>: Algunas funciones pueden saturarse, es decir, sus derivadas se vuelven muy pequeñas para ciertos rangos de entrada.</li>
</ol>
</div><h1><p>Funciones de Activación Comunes</p>
</h1>
<div class='content'></div><h2><ol>
<li>Sigmoide</li>
</ol>
</h2>
<div class='content'><p>La función sigmoide es una de las funciones de activación más antiguas y se define como:</p>
<p>\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<ul>
<li><strong>Rango de Salida</strong>: (0, 1)</li>
<li><strong>Ventajas</strong>: Suaviza la salida y es útil para modelos probabilísticos.</li>
<li><strong>Desventajas</strong>: Puede sufrir de saturación y el problema del gradiente desvanecido.</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgRWplbXBsbyBkZSB1c28gZGUgbGEgZnVuY2nDs24gc2lnbW9pZGUgZW4gVGVuc29yRmxvdwp4ID0gdGYuY29uc3RhbnQoWy0xLjAsIDAuMCwgMS4wXSwgZHR5cGU9dGYuZmxvYXQzMikKc2lnbW9pZCA9IHRmLm5uLnNpZ21vaWQoeCkKcHJpbnQoc2lnbW9pZC5udW1weSgpKSAgIyBTYWxpZGE6IFswLjI2ODk0MTQzIDAuNSAwLjczMTA1ODZd"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Ejemplo de uso de la funci&oacute;n sigmoide en TensorFlow
x = tf.constant([-1.0, 0.0, 1.0], dtype=tf.float32)
sigmoid = tf.nn.sigmoid(x)
print(sigmoid.numpy())  # Salida: [0.26894143 0.5 0.7310586]</pre></div><div class='content'></div><h2><ol start="2">
<li>Tanh (Tangente Hiperbólica)</li>
</ol>
</h2>
<div class='content'><p>La función tanh es similar a la sigmoide pero escala la salida entre -1 y 1:</p>
<p>\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<ul>
<li><strong>Rango de Salida</strong>: (-1, 1)</li>
<li><strong>Ventajas</strong>: Centra los datos alrededor de cero, lo que puede acelerar el aprendizaje.</li>
<li><strong>Desventajas</strong>: También puede sufrir de saturación y el problema del gradiente desvanecido.</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFamVtcGxvIGRlIHVzbyBkZSBsYSBmdW5jacOzbiB0YW5oIGVuIFRlbnNvckZsb3cKdGFuaCA9IHRmLm5uLnRhbmgoeCkKcHJpbnQodGFuaC5udW1weSgpKSAgIyBTYWxpZGE6IFstMC43NjE1OTQyIDAuIDAuNzYxNTk0Ml0="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Ejemplo de uso de la funci&oacute;n tanh en TensorFlow
tanh = tf.nn.tanh(x)
print(tanh.numpy())  # Salida: [-0.7615942 0. 0.7615942]</pre></div><div class='content'></div><h2><ol start="3">
<li>ReLU (Rectified Linear Unit)</li>
</ol>
</h2>
<div class='content'><p>La función ReLU es actualmente la más popular debido a su simplicidad y efectividad:</p>
<p>\[ \text{ReLU}(x) = \max(0, x) \]</p>
<ul>
<li><strong>Rango de Salida</strong>: [0, ∞)</li>
<li><strong>Ventajas</strong>: No sufre de saturación en la región positiva y es computacionalmente eficiente.</li>
<li><strong>Desventajas</strong>: Puede causar el problema de &quot;neurona muerta&quot; donde algunas neuronas nunca se activan.</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFamVtcGxvIGRlIHVzbyBkZSBsYSBmdW5jacOzbiBSZUxVIGVuIFRlbnNvckZsb3cKcmVsdSA9IHRmLm5uLnJlbHUoeCkKcHJpbnQocmVsdS5udW1weSgpKSAgIyBTYWxpZGE6IFswLiAwLiAxLl0="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Ejemplo de uso de la funci&oacute;n ReLU en TensorFlow
relu = tf.nn.relu(x)
print(relu.numpy())  # Salida: [0. 0. 1.]</pre></div><div class='content'></div><h2><ol start="4">
<li>Leaky ReLU</li>
</ol>
</h2>
<div class='content'><p>Leaky ReLU es una variante de ReLU que permite un pequeño gradiente cuando la entrada es negativa:</p>
<p>\[ \text{Leaky ReLU}(x) = \max(0.01x, x) \]</p>
<ul>
<li><strong>Rango de Salida</strong>: (-∞, ∞)</li>
<li><strong>Ventajas</strong>: Mitiga el problema de &quot;neurona muerta&quot;.</li>
<li><strong>Desventajas</strong>: Introduce un hiperparámetro adicional (la pendiente negativa).</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFamVtcGxvIGRlIHVzbyBkZSBsYSBmdW5jacOzbiBMZWFreSBSZUxVIGVuIFRlbnNvckZsb3cKbGVha3lfcmVsdSA9IHRmLm5uLmxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSkKcHJpbnQobGVha3lfcmVsdS5udW1weSgpKSAgIyBTYWxpZGE6IFstMC4wMSAwLiAxLl0="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Ejemplo de uso de la funci&oacute;n Leaky ReLU en TensorFlow
leaky_relu = tf.nn.leaky_relu(x, alpha=0.01)
print(leaky_relu.numpy())  # Salida: [-0.01 0. 1.]</pre></div><div class='content'></div><h2><ol start="5">
<li>Softmax</li>
</ol>
</h2>
<div class='content'><p>La función softmax se utiliza principalmente en la capa de salida de una red neuronal para clasificación multiclase:</p>
<p>\[ \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]</p>
<ul>
<li><strong>Rango de Salida</strong>: (0, 1) y la suma de todas las salidas es 1.</li>
<li><strong>Ventajas</strong>: Proporciona una interpretación probabilística de las salidas.</li>
<li><strong>Desventajas</strong>: No es adecuada para capas ocultas debido a su complejidad computacional.</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFamVtcGxvIGRlIHVzbyBkZSBsYSBmdW5jacOzbiBzb2Z0bWF4IGVuIFRlbnNvckZsb3cKbG9naXRzID0gdGYuY29uc3RhbnQoWzEuMCwgMi4wLCAzLjBdLCBkdHlwZT10Zi5mbG9hdDMyKQpzb2Z0bWF4ID0gdGYubm4uc29mdG1heChsb2dpdHMpCnByaW50KHNvZnRtYXgubnVtcHkoKSkgICMgU2FsaWRhOiBbMC4wOTAwMzA1NyAwLjI0NDcyODQ4IDAuNjY1MjQwOTRd"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Ejemplo de uso de la funci&oacute;n softmax en TensorFlow
logits = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)
softmax = tf.nn.softmax(logits)
print(softmax.numpy())  # Salida: [0.09003057 0.24472848 0.66524094]</pre></div><div class='content'></div><h1><p>Ejercicio Práctico</p>
</h1>
<div class='content'></div><h2><p>Ejercicio 1: Implementar y Comparar Funciones de Activación</p>
</h2>
<div class='content'><ol>
<li><strong>Objetivo</strong>: Implementar las funciones de activación mencionadas y comparar sus salidas para un conjunto de datos de entrada.</li>
<li><strong>Instrucciones</strong>:
<ul>
<li>Crear un tensor de entrada con valores entre -10 y 10.</li>
<li>Aplicar cada una de las funciones de activación a este tensor.</li>
<li>Graficar las salidas para visualizar las diferencias.</li>
</ul>
</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKCiMgQ3JlYXIgdW4gdGVuc29yIGRlIGVudHJhZGEKeCA9IHRmLmxpbnNwYWNlKC0xMC4wLCAxMC4wLCAxMDApCgojIEFwbGljYXIgZnVuY2lvbmVzIGRlIGFjdGl2YWNpw7NuCnNpZ21vaWQgPSB0Zi5ubi5zaWdtb2lkKHgpCnRhbmggPSB0Zi5ubi50YW5oKHgpCnJlbHUgPSB0Zi5ubi5yZWx1KHgpCmxlYWt5X3JlbHUgPSB0Zi5ubi5sZWFreV9yZWx1KHgsIGFscGhhPTAuMDEpCnNvZnRtYXggPSB0Zi5ubi5zb2Z0bWF4KHRmLnN0YWNrKFt4LCB4XSwgYXhpcz0xKSwgYXhpcz0xKVs6LCAwXQoKIyBHcmFmaWNhciBsYXMgc2FsaWRhcwpwbHQuZmlndXJlKGZpZ3NpemU9KDEyLCA4KSkKcGx0LnBsb3QoeCwgc2lnbW9pZCwgbGFiZWw9J1NpZ21vaWQnKQpwbHQucGxvdCh4LCB0YW5oLCBsYWJlbD0nVGFuaCcpCnBsdC5wbG90KHgsIHJlbHUsIGxhYmVsPSdSZUxVJykKcGx0LnBsb3QoeCwgbGVha3lfcmVsdSwgbGFiZWw9J0xlYWt5IFJlTFUnKQpwbHQucGxvdCh4LCBzb2Z0bWF4LCBsYWJlbD0nU29mdG1heCcpCnBsdC5sZWdlbmQoKQpwbHQudGl0bGUoJ0NvbXBhcmFjacOzbiBkZSBGdW5jaW9uZXMgZGUgQWN0aXZhY2nDs24nKQpwbHQueGxhYmVsKCdFbnRyYWRhJykKcGx0LnlsYWJlbCgnU2FsaWRhJykKcGx0LmdyaWQoVHJ1ZSkKcGx0LnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import matplotlib.pyplot as plt

# Crear un tensor de entrada
x = tf.linspace(-10.0, 10.0, 100)

# Aplicar funciones de activaci&oacute;n
sigmoid = tf.nn.sigmoid(x)
tanh = tf.nn.tanh(x)
relu = tf.nn.relu(x)
leaky_relu = tf.nn.leaky_relu(x, alpha=0.01)
softmax = tf.nn.softmax(tf.stack([x, x], axis=1), axis=1)[:, 0]

# Graficar las salidas
plt.figure(figsize=(12, 8))
plt.plot(x, sigmoid, label='Sigmoid')
plt.plot(x, tanh, label='Tanh')
plt.plot(x, relu, label='ReLU')
plt.plot(x, leaky_relu, label='Leaky ReLU')
plt.plot(x, softmax, label='Softmax')
plt.legend()
plt.title('Comparaci&oacute;n de Funciones de Activaci&oacute;n')
plt.xlabel('Entrada')
plt.ylabel('Salida')
plt.grid(True)
plt.show()</pre></div><div class='content'></div><h2><p>Solución del Ejercicio</p>
</h2>
<div class='content'><p>El código anterior crea un tensor de entrada y aplica las funciones de activación mencionadas. Luego, grafica las salidas para comparar visualmente cómo cada función transforma los datos de entrada.</p>
</div><h1><p>Conclusión</p>
</h1>
<div class='content'><p>En esta sección, hemos explorado varias funciones de activación comunes y sus implementaciones en TensorFlow. Cada función tiene sus propias ventajas y desventajas, y la elección de la función de activación adecuada puede tener un impacto significativo en el rendimiento de la red neuronal. En el próximo módulo, aplicaremos estos conocimientos para construir redes neuronales más complejas y eficientes.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='04-02-creating-a-simple-neural-network' title="Creando una Red Neuronal Simple">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='04-04-loss-functions-and-optimizers' title="Funciones de Pérdida y Optimizadores">
				<span class="d-none d-md-inline">Siguiente &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Usamos cookies para mejorar tu experiencia de uso y ofrecer contenidos adaptados a tus intereses.
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
