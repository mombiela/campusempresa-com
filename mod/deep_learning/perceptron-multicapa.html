<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Perceptrón y Perceptrón Multicapa</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/perceptron-multicapa" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/perceptron-multicapa" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/perceptron-multicapa" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/deep_learning/perceptron-multicapa" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/deep_learning/perceptron-multicapa" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-4'>
					<a href='aplicaciones-deep-learning'>&#x25C4;Aplicaciones del Deep Learning</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Perceptrón y Perceptrón Multicapa</a>
	</div>
	<div class='col-4 text-end'>
					<a href='funciones-activacion'>Funciones de Activación &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introducción</h1>
<div class='content'><p>En esta sección del curso sobre Deep Learning, exploraremos los conceptos fundamentales del Perceptrón y el Perceptrón Multicapa (MLP). Estos son los bloques básicos de las redes neuronales y son esenciales para entender modelos más complejos en Deep Learning.</p>
</div><h1>Perceptrón</h1>
<div class='content'><p>El Perceptrón es el modelo más simple de una red neuronal y fue introducido por Frank Rosenblatt en 1958. Es un clasificador lineal que toma una entrada, realiza una suma ponderada y aplica una función de activación para producir una salida.</p>
</div><h2>Conceptos Clave</h2>
<div class='content'><ul>
<li><strong>Neuronas</strong>: Unidades básicas de procesamiento en una red neuronal.</li>
<li><strong>Pesos (Weights)</strong>: Coeficientes que multiplican las entradas.</li>
<li><strong>Sesgo (Bias)</strong>: Un término adicional que ayuda a ajustar la salida del modelo.</li>
<li><strong>Función de Activación</strong>: Función que decide si una neurona debe activarse o no.</li>
</ul>
</div><h2>Estructura del Perceptrón</h2>
<div class='content'><p>El Perceptrón puede ser representado matemáticamente como:</p>
<p>\[ y = f(\sum_{i=1}^{n} w_i x_i + b) \]</p>
<p>Donde:</p>
<ul>
<li>\( y \) es la salida.</li>
<li>\( w_i \) son los pesos.</li>
<li>\( x_i \) son las entradas.</li>
<li>\( b \) es el sesgo.</li>
<li>\( f \) es la función de activación (por ejemplo, la función escalón).</li>
</ul>
</div><h2>Ejemplo de Código</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIEZ1bmNpw7NuIGRlIGFjdGl2YWNpw7NuIChlc2NhbMOzbikKZGVmIHN0ZXBfZnVuY3Rpb24oeCk6CiAgICByZXR1cm4gMSBpZiB4ID49IDAgZWxzZSAwCgojIFBlcmNlcHRyw7NuIHNpbXBsZQpjbGFzcyBQZXJjZXB0cm9uOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIGlucHV0X3NpemUsIGxlYXJuaW5nX3JhdGU9MC4xKToKICAgICAgICBzZWxmLndlaWdodHMgPSBucC56ZXJvcyhpbnB1dF9zaXplICsgMSkgICMgKzEgcGFyYSBlbCBzZXNnbwogICAgICAgIHNlbGYubGVhcm5pbmdfcmF0ZSA9IGxlYXJuaW5nX3JhdGUKCiAgICBkZWYgcHJlZGljdChzZWxmLCB4KToKICAgICAgICB4ID0gbnAuaW5zZXJ0KHgsIDAsIDEpICAjIEluc2VydGFyIGVsIHNlc2dvCiAgICAgICAgc3VtbWF0aW9uID0gbnAuZG90KHNlbGYud2VpZ2h0cywgeCkKICAgICAgICByZXR1cm4gc3RlcF9mdW5jdGlvbihzdW1tYXRpb24pCgogICAgZGVmIHRyYWluKHNlbGYsIHRyYWluaW5nX2RhdGEsIGxhYmVscywgZXBvY2hzPTEwKToKICAgICAgICBmb3IgXyBpbiByYW5nZShlcG9jaHMpOgogICAgICAgICAgICBmb3IgeCwgbGFiZWwgaW4gemlwKHRyYWluaW5nX2RhdGEsIGxhYmVscyk6CiAgICAgICAgICAgICAgICBwcmVkaWN0aW9uID0gc2VsZi5wcmVkaWN0KHgpCiAgICAgICAgICAgICAgICBzZWxmLndlaWdodHMgKz0gc2VsZi5sZWFybmluZ19yYXRlICogKGxhYmVsIC0gcHJlZGljdGlvbikgKiBucC5pbnNlcnQoeCwgMCwgMSkKCiMgRGF0b3MgZGUgZW50cmVuYW1pZW50byAoQU5EIGzDs2dpY28pCnRyYWluaW5nX2RhdGEgPSBucC5hcnJheShbWzAsIDBdLCBbMCwgMV0sIFsxLCAwXSwgWzEsIDFdXSkKbGFiZWxzID0gbnAuYXJyYXkoWzAsIDAsIDAsIDFdKQoKIyBDcmVhciB5IGVudHJlbmFyIGVsIHBlcmNlcHRyw7NuCnBlcmNlcHRyb24gPSBQZXJjZXB0cm9uKGlucHV0X3NpemU9MikKcGVyY2VwdHJvbi50cmFpbih0cmFpbmluZ19kYXRhLCBsYWJlbHMpCgojIFByb2JhciBlbCBwZXJjZXB0csOzbgpmb3IgeCBpbiB0cmFpbmluZ19kYXRhOgogICAgcHJpbnQoZiJFbnRyYWRhOiB7eH0sIFByZWRpY2Npw7NuOiB7cGVyY2VwdHJvbi5wcmVkaWN0KHgpfSIp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Funci&oacute;n de activaci&oacute;n (escal&oacute;n)
def step_function(x):
    return 1 if x &gt;= 0 else 0

# Perceptr&oacute;n simple
class Perceptron:
    def __init__(self, input_size, learning_rate=0.1):
        self.weights = np.zeros(input_size + 1)  # +1 para el sesgo
        self.learning_rate = learning_rate

    def predict(self, x):
        x = np.insert(x, 0, 1)  # Insertar el sesgo
        summation = np.dot(self.weights, x)
        return step_function(summation)

    def train(self, training_data, labels, epochs=10):
        for _ in range(epochs):
            for x, label in zip(training_data, labels):
                prediction = self.predict(x)
                self.weights += self.learning_rate * (label - prediction) * np.insert(x, 0, 1)

# Datos de entrenamiento (AND l&oacute;gico)
training_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
labels = np.array([0, 0, 0, 1])

# Crear y entrenar el perceptr&oacute;n
perceptron = Perceptron(input_size=2)
perceptron.train(training_data, labels)

# Probar el perceptr&oacute;n
for x in training_data:
    print(f&quot;Entrada: {x}, Predicci&oacute;n: {perceptron.predict(x)}&quot;)</pre></div><div class='content'></div><h1>Perceptrón Multicapa (MLP)</h1>
<div class='content'><p>El Perceptrón Multicapa (MLP) es una extensión del Perceptrón simple y puede resolver problemas no lineales. Consiste en múltiples capas de neuronas: una capa de entrada, una o más capas ocultas y una capa de salida.</p>
</div><h2>Conceptos Clave</h2>
<div class='content'><ul>
<li><strong>Capas Ocultas</strong>: Capas intermedias entre la entrada y la salida que permiten la modelización de relaciones no lineales.</li>
<li><strong>Backpropagation</strong>: Algoritmo para entrenar redes neuronales ajustando los pesos mediante el cálculo del gradiente del error.</li>
</ul>
</div><h2>Estructura del MLP</h2>
<div class='content'><p>Un MLP con una capa oculta puede ser representado como:</p>
<p>\[ y = f_2(\sum_{j=1}^{m} w_{2j} f_1(\sum_{i=1}^{n} w_{1ij} x_i + b_{1j}) + b_2) \]</p>
<p>Donde:</p>
<ul>
<li>\( f_1 \) y \( f_2 \) son funciones de activación para la capa oculta y la capa de salida, respectivamente.</li>
<li>\( w_{1ij} \) y \( w_{2j} \) son los pesos de la primera y segunda capa.</li>
<li>\( b_{1j} \) y \( b_2 \) son los sesgos de la primera y segunda capa.</li>
</ul>
</div><h2>Ejemplo de Código</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Funci&oacute;n de activaci&oacute;n (sigmoide)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivada de la funci&oacute;n sigmoide
def sigmoid_derivative(x):
    return x * (1 - x)

# Perceptr&oacute;n Multicapa
class MLP:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):
        self.weights_input_hidden = np.random.rand(input_size, hidden_size)
        self.weights_hidden_output = np.random.rand(hidden_size, output_size)
        self.learning_rate = learning_rate

    def forward(self, x):
        self.hidden_input = np.dot(x, self.weights_input_hidden)
        self.hidden_output = sigmoid(self.hidden_input)
        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output)
        self.final_output = sigmoid(self.final_input)
        return self.final_output

    def backward(self, x, y, output):
        error = y - output
        d_output = error * sigmoid_derivative(output)
        error_hidden_layer = d_output.dot(self.weights_hidden_output.T)
        d_hidden_layer = error_hidden_layer * sigmoid_derivative(self.hidden_output)

        self.weights_hidden_output += self.hidden_output.T.dot(d_output) * self.learning_rate
        self.weights_input_hidden += x.T.dot(d_hidden_layer) * self.learning_rate

    def train(self, training_data, labels, epochs=10000):
        for _ in range(epochs):
            for x, y in zip(training_data, labels):
                output = self.forward(x)
                self.backward(x, y, output)

# Datos de entrenamiento (XOR l&oacute;gico)
training_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
labels = np.array([[0], [1], [1], [0]])

# Crear y entrenar el MLP
mlp = MLP(input_size=2, hidden_size=2, output_size=1)
mlp.train(training_data, labels)

# Probar el MLP
for x in training_data:
    print(f&quot;Entrada: {x}, Predicci&oacute;n: {mlp.forward(x)}&quot;)</pre></div><div class='content'></div><h1>Conclusión</h1>
<div class='content'><p>En esta sección, hemos cubierto los conceptos básicos del Perceptrón y el Perceptrón Multicapa. El Perceptrón es un clasificador lineal simple, mientras que el MLP puede manejar problemas no lineales mediante la inclusión de capas ocultas y el uso del algoritmo de backpropagation. Estos modelos son fundamentales para entender redes neuronales más complejas y son la base de muchos algoritmos de Deep Learning avanzados.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='aplicaciones-deep-learning'>&#x25C4;Aplicaciones del Deep Learning</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Perceptrón y Perceptrón Multicapa</a>
	</div>
	<div class='col-4 text-end'>
					<a href='funciones-activacion'>Funciones de Activación &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
