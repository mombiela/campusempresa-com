<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Funciones de Activación</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/funciones-activacion" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/funciones-activacion" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/funciones-activacion" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/deep_learning/funciones-activacion" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/deep_learning/funciones-activacion" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='perceptron-multicapa'>&#x25C4;Perceptrón y Perceptrón Multicapa</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Funciones de Activación</a>
	</div>
	<div class='col-4 text-end'>
					<a href='propagacion-adelante-atras'>Propagación Hacia Adelante y Hacia Atrás &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introducción</h1>
<div class='content'><p>En el contexto del Deep Learning, las funciones de activación juegan un papel crucial en la capacidad de las redes neuronales para aprender y modelar datos complejos. Estas funciones determinan si una neurona debe activarse o no, introduciendo no linealidades que permiten a la red neuronal aprender relaciones complejas en los datos.</p>
</div><h1>Tipos de Funciones de Activación</h1>
<div class='content'><p>Existen varias funciones de activación que se utilizan comúnmente en las redes neuronales. A continuación, se describen algunas de las más importantes:</p>
</div><h2>Función Sigmoide</h2>
<div class='content'><p>La función sigmoide es una de las funciones de activación más antiguas y se define como:</p>
<p>\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<ul>
<li><strong>Rango de salida:</strong> (0, 1)</li>
<li><strong>Ventajas:</strong>
<ul>
<li>Suave y diferenciable.</li>
<li>Salida en un rango limitado (0 a 1), útil para probabilidades.</li>
</ul>
</li>
<li><strong>Desventajas:</strong>
<ul>
<li>Problema de desvanecimiento del gradiente.</li>
<li>Salidas no centradas en cero.</li>
</ul>
</li>
</ul>
<h4>Ejemplo de Código</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgc2lnbW9pZCh4KToKICAgIHJldHVybiAxIC8gKDEgKyBucC5leHAoLXgpKQoKIyBFamVtcGxvIGRlIHVzbwp4ID0gbnAuYXJyYXkoWy0xLjAsIDAuMCwgMS4wXSkKcHJpbnQoc2lnbW9pZCh4KSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Ejemplo de uso
x = np.array([-1.0, 0.0, 1.0])
print(sigmoid(x))</pre></div><div class='content'></div><h2>Función Tanh</h2>
<div class='content'><p>La función tangente hiperbólica (tanh) es similar a la sigmoide pero sus salidas están centradas en cero:</p>
<p>\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<ul>
<li><strong>Rango de salida:</strong> (-1, 1)</li>
<li><strong>Ventajas:</strong>
<ul>
<li>Salidas centradas en cero.</li>
<li>Suave y diferenciable.</li>
</ul>
</li>
<li><strong>Desventajas:</strong>
<ul>
<li>Aún puede sufrir del problema de desvanecimiento del gradiente.</li>
</ul>
</li>
</ul>
<h4>Ejemplo de Código</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHRhbmgoeCk6CiAgICByZXR1cm4gbnAudGFuaCh4KQoKIyBFamVtcGxvIGRlIHVzbwp4ID0gbnAuYXJyYXkoWy0xLjAsIDAuMCwgMS4wXSkKcHJpbnQodGFuaCh4KSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def tanh(x):
    return np.tanh(x)

# Ejemplo de uso
x = np.array([-1.0, 0.0, 1.0])
print(tanh(x))</pre></div><div class='content'></div><h2>Función ReLU (Rectified Linear Unit)</h2>
<div class='content'><p>La función ReLU es actualmente una de las funciones de activación más populares:</p>
<p>\[ \text{ReLU}(x) = \max(0, x) \]</p>
<ul>
<li><strong>Rango de salida:</strong> [0, ∞)</li>
<li><strong>Ventajas:</strong>
<ul>
<li>Simple y eficiente.</li>
<li>Mitiga el problema de desvanecimiento del gradiente.</li>
</ul>
</li>
<li><strong>Desventajas:</strong>
<ul>
<li>Problema de neuronas muertas (cuando los valores negativos se convierten en cero).</li>
</ul>
</li>
</ul>
<h4>Ejemplo de Código</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHJlbHUoeCk6CiAgICByZXR1cm4gbnAubWF4aW11bSgwLCB4KQoKIyBFamVtcGxvIGRlIHVzbwp4ID0gbnAuYXJyYXkoWy0xLjAsIDAuMCwgMS4wXSkKcHJpbnQocmVsdSh4KSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def relu(x):
    return np.maximum(0, x)

# Ejemplo de uso
x = np.array([-1.0, 0.0, 1.0])
print(relu(x))</pre></div><div class='content'></div><h2>Función Leaky ReLU</h2>
<div class='content'><p>La función Leaky ReLU es una variante de ReLU que intenta solucionar el problema de las neuronas muertas:</p>
<p>\[ \text{Leaky ReLU}(x) = \begin{cases}
x &amp; \text{si } x \geq 0 <br>\alpha x &amp; \text{si } x &lt; 0
\end{cases} \]</p>
<p>donde \( \alpha \) es un pequeño valor positivo.</p>
<ul>
<li><strong>Rango de salida:</strong> (-∞, ∞)</li>
<li><strong>Ventajas:</strong>
<ul>
<li>Mitiga el problema de neuronas muertas.</li>
</ul>
</li>
<li><strong>Desventajas:</strong>
<ul>
<li>Introduce un pequeño sesgo en las salidas negativas.</li>
</ul>
</li>
</ul>
<h4>Ejemplo de Código</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIGxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSk6CiAgICByZXR1cm4gbnAud2hlcmUoeCA+IDAsIHgsIGFscGhhICogeCkKCiMgRWplbXBsbyBkZSB1c28KeCA9IG5wLmFycmF5KFstMS4wLCAwLjAsIDEuMF0pCnByaW50KGxlYWt5X3JlbHUoeCkp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

# Ejemplo de uso
x = np.array([-1.0, 0.0, 1.0])
print(leaky_relu(x))</pre></div><div class='content'></div><h1>Comparación de Funciones de Activación</h1>
<div class='content'><p>| Función       | Rango de Salida | Ventajas                              | Desventajas                          |
|---------------|-----------------|---------------------------------------|--------------------------------------|
| Sigmoide      | (0, 1)          | Suave, diferenciable, útil para probabilidades | Desvanecimiento del gradiente, no centrada en cero |
| Tanh          | (-1, 1)         | Salidas centradas en cero, suave      | Desvanecimiento del gradiente        |
| ReLU          | [0, ∞)          | Simple, eficiente, mitiga desvanecimiento del gradiente | Neuronas muertas                     |
| Leaky ReLU    | (-∞, ∞)         | Mitiga neuronas muertas               | Introduce sesgo en salidas negativas |</p>
</div><h1>Conclusión</h1>
<div class='content'><p>Las funciones de activación son componentes esenciales en las redes neuronales, ya que permiten la introducción de no linealidades y, por lo tanto, la capacidad de modelar relaciones complejas en los datos. La elección de la función de activación adecuada puede tener un impacto significativo en el rendimiento y la eficiencia del modelo. Es importante entender las características, ventajas y desventajas de cada función para tomar decisiones informadas en el diseño de redes neuronales.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='perceptron-multicapa'>&#x25C4;Perceptrón y Perceptrón Multicapa</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Funciones de Activación</a>
	</div>
	<div class='col-4 text-end'>
					<a href='propagacion-adelante-atras'>Propagación Hacia Adelante y Hacia Atrás &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
