<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LSTM y GRU</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/04-02-lstm-gru" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/04-02-lstm-gru" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/04-02-lstm-gru" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
				<a href="https://enterprisecampus.net/mod/deep_learning/04-02-lstm-gru" class="px-2">EN</a></b>
	|
	<b class="px-2">ES</b>
	|
	<a href="https://campusempresa.cat/mod/deep_learning/04-02-lstm-gru" class="px-2">CA</a>
<br>
			<cite>Construyendo la sociedad de hoy y del mañana</cite>
		</div>
	</div>
</div>
<div id="subheader" class="container-xxl">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Proyecto</a> | 
<a href="/about">Sobre nosotros</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donaciones</a> | 
<a href="/licence">Licencia</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				 					<a href="/categ/languages">Lenguajes de Programación</a>
				 					<a href="/categ/frameworks">Frameworks y Librerías</a>
				 					<a href="/categ/tech-tools">Herramientas Técnicas</a>
				 					<a href="/categ/foundations">Fundamentos Teóricos</a>
				 					<a href="/categ/soft-skills">Habilidades Sociales</a>
							</div>
		</div>
	</div>
</div>
		
<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='04-01-introduccion-rnn' title="Introducción a las RNN">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">LSTM y GRU</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='04-03-aplicaciones-rnn-pln' title="Aplicaciones de RNN en procesamiento del lenguaje natural">
				<span class="d-none d-md-inline">Siguiente &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'></div><h1><p>Introducción</p>
</h1>
<div class='content'><p>En este tema, exploraremos dos tipos de redes neuronales recurrentes (RNN) avanzadas: Long Short-Term Memory (LSTM) y Gated Recurrent Units (GRU). Ambas arquitecturas están diseñadas para abordar el problema del desvanecimiento del gradiente y mejorar la capacidad de las RNN para capturar dependencias a largo plazo en secuencias de datos.</p>
</div><h1><p>Objetivos de Aprendizaje</p>
</h1>
<div class='content'><p>Al finalizar este tema, deberías ser capaz de:</p>
<ol>
<li>Comprender la estructura y funcionamiento de las LSTM.</li>
<li>Comprender la estructura y funcionamiento de las GRU.</li>
<li>Comparar las diferencias y similitudes entre LSTM y GRU.</li>
<li>Implementar LSTM y GRU en un entorno de programación.</li>
</ol>
</div><h1><ol>
<li>Long Short-Term Memory (LSTM)</li>
</ol>
</h1>
<div class='content'></div><h2><p>Estructura de una LSTM</p>
</h2>
<div class='content'><p>Una LSTM es una variante de las RNN que introduce una estructura de memoria más compleja para mantener información durante largos períodos de tiempo. La unidad LSTM consta de tres puertas principales:</p>
<ol>
<li><strong>Puerta de entrada (Input Gate):</strong> Controla la cantidad de nueva información que se almacena en la celda de memoria.</li>
<li><strong>Puerta de olvido (Forget Gate):</strong> Decide cuánta información de la celda de memoria anterior debe olvidarse.</li>
<li><strong>Puerta de salida (Output Gate):</strong> Determina qué parte de la información de la celda de memoria se utiliza para la salida.</li>
</ol>
</div><h2><p>Fórmulas Matemáticas</p>
</h2>
<div class='content'><p>Las operaciones dentro de una unidad LSTM se pueden describir mediante las siguientes ecuaciones:</p>
<ol>
<li>
<p><strong>Puerta de olvido:</strong>
\[
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
\]</p>
</li>
<li>
<p><strong>Puerta de entrada:</strong>
\[
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
\]
\[
\tilde{C}<em>t = \tanh(W_C \cdot [h</em>{t-1}, x_t] + b_C)
\]</p>
</li>
<li>
<p><strong>Actualización de la celda de memoria:</strong>
\[
C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t
\]</p>
</li>
<li>
<p><strong>Puerta de salida:</strong>
\[
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
\]
\[
h_t = o_t \cdot \tanh(C_t)
\]</p>
</li>
</ol>
</div><h2><p>Ejemplo de Implementación en Python con TensorFlow</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLm1vZGVscyBpbXBvcnQgU2VxdWVudGlhbApmcm9tIHRlbnNvcmZsb3cua2VyYXMubGF5ZXJzIGltcG9ydCBMU1RNLCBEZW5zZQoKIyBEZWZpbmlyIGVsIG1vZGVsbwptb2RlbCA9IFNlcXVlbnRpYWwoKQptb2RlbC5hZGQoTFNUTSg1MCwgaW5wdXRfc2hhcGU9KHRpbWVzdGVwcywgZmVhdHVyZXMpKSkKbW9kZWwuYWRkKERlbnNlKDEpKQoKIyBDb21waWxhciBlbCBtb2RlbG8KbW9kZWwuY29tcGlsZShvcHRpbWl6ZXI9J2FkYW0nLCBsb3NzPSdtc2UnKQoKIyBSZXN1bWVuIGRlbCBtb2RlbG8KbW9kZWwuc3VtbWFyeSgp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Definir el modelo
model = Sequential()
model.add(LSTM(50, input_shape=(timesteps, features)))
model.add(Dense(1))

# Compilar el modelo
model.compile(optimizer='adam', loss='mse')

# Resumen del modelo
model.summary()</pre></div><div class='content'></div><h1><ol start="2">
<li>Gated Recurrent Units (GRU)</li>
</ol>
</h1>
<div class='content'></div><h2><p>Estructura de una GRU</p>
</h2>
<div class='content'><p>Las GRU son una simplificación de las LSTM que combinan la puerta de entrada y la puerta de olvido en una sola puerta llamada &quot;puerta de actualización&quot;. Además, las GRU tienen una &quot;puerta de reinicio&quot; que controla cuánta información pasada se olvida.</p>
</div><h2><p>Fórmulas Matemáticas</p>
</h2>
<div class='content'><p>Las operaciones dentro de una unidad GRU se pueden describir mediante las siguientes ecuaciones:</p>
<ol>
<li>
<p><strong>Puerta de actualización:</strong>
\[
z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)
\]</p>
</li>
<li>
<p><strong>Puerta de reinicio:</strong>
\[
r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)
\]</p>
</li>
<li>
<p><strong>Cálculo de la nueva memoria:</strong>
\[
\tilde{h}<em>t = \tanh(W \cdot [r_t \cdot h</em>{t-1}, x_t] + b)
\]</p>
</li>
<li>
<p><strong>Actualización del estado oculto:</strong>
\[
h_t = (1 - z_t) \cdot h_{t-1} + z_t \cdot \tilde{h}_t
\]</p>
</li>
</ol>
</div><h2><p>Ejemplo de Implementación en Python con TensorFlow</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLm1vZGVscyBpbXBvcnQgU2VxdWVudGlhbApmcm9tIHRlbnNvcmZsb3cua2VyYXMubGF5ZXJzIGltcG9ydCBHUlUsIERlbnNlCgojIERlZmluaXIgZWwgbW9kZWxvCm1vZGVsID0gU2VxdWVudGlhbCgpCm1vZGVsLmFkZChHUlUoNTAsIGlucHV0X3NoYXBlPSh0aW1lc3RlcHMsIGZlYXR1cmVzKSkpCm1vZGVsLmFkZChEZW5zZSgxKSkKCiMgQ29tcGlsYXIgZWwgbW9kZWxvCm1vZGVsLmNvbXBpbGUob3B0aW1pemVyPSdhZGFtJywgbG9zcz0nbXNlJykKCiMgUmVzdW1lbiBkZWwgbW9kZWxvCm1vZGVsLnN1bW1hcnkoKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense

# Definir el modelo
model = Sequential()
model.add(GRU(50, input_shape=(timesteps, features)))
model.add(Dense(1))

# Compilar el modelo
model.compile(optimizer='adam', loss='mse')

# Resumen del modelo
model.summary()</pre></div><div class='content'></div><h1><ol start="3">
<li>Comparación entre LSTM y GRU</li>
</ol>
</h1>
<div class='content'><table>
<thead>
<tr>
<th>Característica</th>
<th>LSTM</th>
<th>GRU</th>
</tr>
</thead>
<tbody>
<tr>
<td>Complejidad</td>
<td>Más compleja debido a tres puertas (entrada, olvido, salida)</td>
<td>Menos compleja con dos puertas (actualización, reinicio)</td>
</tr>
<tr>
<td>Rendimiento</td>
<td>Mejor en tareas con dependencias a largo plazo</td>
<td>Similar rendimiento con menos parámetros</td>
</tr>
<tr>
<td>Tiempo de entrenamiento</td>
<td>Más lento debido a la complejidad</td>
<td>Más rápido debido a la simplicidad</td>
</tr>
</tbody>
</table>
</div><h1><ol start="4">
<li>Ejercicio Práctico</li>
</ol>
</h1>
<div class='content'></div><h2><p>Ejercicio</p>
</h2>
<div class='content'><p>Implementa una red LSTM y una red GRU para predecir una serie temporal de datos de ventas. Compara el rendimiento de ambos modelos.</p>
<h4>Solución</h4>
<ol>
<li><strong>Preparar los datos:</strong></li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBwYW5kYXMgYXMgcGQKZnJvbSBza2xlYXJuLnByZXByb2Nlc3NpbmcgaW1wb3J0IE1pbk1heFNjYWxlcgoKIyBDYXJnYXIgZGF0b3MKZGF0YSA9IHBkLnJlYWRfY3N2KCdzYWxlc19kYXRhLmNzdicpCnNhbGVzID0gZGF0YVsnc2FsZXMnXS52YWx1ZXMucmVzaGFwZSgtMSwgMSkKCiMgRXNjYWxhciBkYXRvcwpzY2FsZXIgPSBNaW5NYXhTY2FsZXIoKQpzYWxlc19zY2FsZWQgPSBzY2FsZXIuZml0X3RyYW5zZm9ybShzYWxlcykKCiMgQ3JlYXIgc2VjdWVuY2lhcyBkZSBlbnRyZW5hbWllbnRvCnRpbWVzdGVwcyA9IDEwClgsIHkgPSBbXSwgW10KZm9yIGkgaW4gcmFuZ2UobGVuKHNhbGVzX3NjYWxlZCkgLSB0aW1lc3RlcHMpOgogICAgWC5hcHBlbmQoc2FsZXNfc2NhbGVkW2k6aSt0aW1lc3RlcHNdKQogICAgeS5hcHBlbmQoc2FsZXNfc2NhbGVkW2krdGltZXN0ZXBzXSkKWCwgeSA9IG5wLmFycmF5KFgpLCBucC5hcnJheSh5KQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Cargar datos
data = pd.read_csv('sales_data.csv')
sales = data['sales'].values.reshape(-1, 1)

# Escalar datos
scaler = MinMaxScaler()
sales_scaled = scaler.fit_transform(sales)

# Crear secuencias de entrenamiento
timesteps = 10
X, y = [], []
for i in range(len(sales_scaled) - timesteps):
    X.append(sales_scaled[i:i+timesteps])
    y.append(sales_scaled[i+timesteps])
X, y = np.array(X), np.array(y)</pre></div><div class='content'><ol start="2">
<li><strong>Definir y entrenar el modelo LSTM:</strong></li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBNb2RlbG8gTFNUTQptb2RlbF9sc3RtID0gU2VxdWVudGlhbCgpCm1vZGVsX2xzdG0uYWRkKExTVE0oNTAsIGlucHV0X3NoYXBlPSh0aW1lc3RlcHMsIDEpKSkKbW9kZWxfbHN0bS5hZGQoRGVuc2UoMSkpCm1vZGVsX2xzdG0uY29tcGlsZShvcHRpbWl6ZXI9J2FkYW0nLCBsb3NzPSdtc2UnKQoKIyBFbnRyZW5hciBlbCBtb2RlbG8KbW9kZWxfbHN0bS5maXQoWCwgeSwgZXBvY2hzPTIwLCBiYXRjaF9zaXplPTMyKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Modelo LSTM
model_lstm = Sequential()
model_lstm.add(LSTM(50, input_shape=(timesteps, 1)))
model_lstm.add(Dense(1))
model_lstm.compile(optimizer='adam', loss='mse')

# Entrenar el modelo
model_lstm.fit(X, y, epochs=20, batch_size=32)</pre></div><div class='content'><ol start="3">
<li><strong>Definir y entrenar el modelo GRU:</strong></li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBNb2RlbG8gR1JVCm1vZGVsX2dydSA9IFNlcXVlbnRpYWwoKQptb2RlbF9ncnUuYWRkKEdSVSg1MCwgaW5wdXRfc2hhcGU9KHRpbWVzdGVwcywgMSkpKQptb2RlbF9ncnUuYWRkKERlbnNlKDEpKQptb2RlbF9ncnUuY29tcGlsZShvcHRpbWl6ZXI9J2FkYW0nLCBsb3NzPSdtc2UnKQoKIyBFbnRyZW5hciBlbCBtb2RlbG8KbW9kZWxfZ3J1LmZpdChYLCB5LCBlcG9jaHM9MjAsIGJhdGNoX3NpemU9MzIp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Modelo GRU
model_gru = Sequential()
model_gru.add(GRU(50, input_shape=(timesteps, 1)))
model_gru.add(Dense(1))
model_gru.compile(optimizer='adam', loss='mse')

# Entrenar el modelo
model_gru.fit(X, y, epochs=20, batch_size=32)</pre></div><div class='content'><ol start="4">
<li><strong>Evaluar y comparar los modelos:</strong></li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFdmFsdWFyIGVsIG1vZGVsbyBMU1RNCmxvc3NfbHN0bSA9IG1vZGVsX2xzdG0uZXZhbHVhdGUoWCwgeSkKcHJpbnQoZidQw6lyZGlkYSBkZWwgbW9kZWxvIExTVE06IHtsb3NzX2xzdG19JykKCiMgRXZhbHVhciBlbCBtb2RlbG8gR1JVCmxvc3NfZ3J1ID0gbW9kZWxfZ3J1LmV2YWx1YXRlKFgsIHkpCnByaW50KGYnUMOpcmRpZGEgZGVsIG1vZGVsbyBHUlU6IHtsb3NzX2dydX0nKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Evaluar el modelo LSTM
loss_lstm = model_lstm.evaluate(X, y)
print(f'P&eacute;rdida del modelo LSTM: {loss_lstm}')

# Evaluar el modelo GRU
loss_gru = model_gru.evaluate(X, y)
print(f'P&eacute;rdida del modelo GRU: {loss_gru}')</pre></div><div class='content'></div><h1><p>Conclusión</p>
</h1>
<div class='content'><p>En este tema, hemos explorado las arquitecturas LSTM y GRU, sus diferencias y similitudes, y cómo implementarlas en TensorFlow. Las LSTM y GRU son herramientas poderosas para manejar secuencias de datos y capturar dependencias a largo plazo, cada una con sus propias ventajas y desventajas. Con esta base, estás preparado para aplicar estas técnicas en problemas de series temporales y procesamiento del lenguaje natural.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='04-01-introduccion-rnn' title="Introducción a las RNN">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='04-03-aplicaciones-rnn-pln' title="Aplicaciones de RNN en procesamiento del lenguaje natural">
				<span class="d-none d-md-inline">Siguiente &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Usamos cookies para mejorar tu experiencia de uso y ofrecer contenidos adaptados a tus intereses.
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
