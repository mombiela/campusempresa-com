<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow, noarchive">
    <title>Propagación hacia adelante y hacia atrás</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/02-03-propagacion-hacia-adelante-atras" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/02-03-propagacion-hacia-adelante-atras" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/02-03-forward-backward-propagation" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css?v=3" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
	</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
			<span>	<a href="https://enterprisecampus.net/mod/deep_learning/02-03-forward-backward-propagation" class="px-2">EN</a></b>
	|
	<b class="px-2">ES</b>
	|
	<a href="https://campusempresa.cat/mod/deep_learning/02-03-propagacion-hacia-adelante-atras" class="px-2">CA</a>
</span>
			<span class="d-none d-md-inline"><br><cite>Construyendo la sociedad de hoy y del mañana</cite></span>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Proyecto</a> | 
<a href="/about">Sobre nosotros</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donaciones</a> | 
<a href="/licence">Licencia</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				 					<a href="/categ/languages">Lenguajes</a>
				 					<a href="/categ/frameworks">Frameworks</a>
				 					<a href="/categ/tech-tools">Herramientas</a>
				 					<a href="/categ/foundations">Fundamentos</a>
				 					<a href="/categ/soft-skills">Competencias</a>
							</div>
		</div>
	</div>
</div>
		
<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='02-02-funcion-de-activacion' title="Función de activación">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Propagación hacia adelante y hacia atrás</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-04-optimizacion-funcion-de-perdida' title="Optimización y función de pérdida">
				<span class="d-none d-md-inline">Siguiente &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'></div><h1><p>Introducción</p>
</h1>
<div class='content'><p>La propagación hacia adelante y hacia atrás son dos procesos fundamentales en el entrenamiento de redes neuronales. La propagación hacia adelante se refiere al cálculo de la salida de la red dado un conjunto de entradas, mientras que la propagación hacia atrás se utiliza para ajustar los pesos de la red minimizando el error de predicción.</p>
</div><h1><p>Propagación hacia adelante (Forward Propagation)</p>
</h1>
<div class='content'></div><h2><p>Conceptos Clave</p>
</h2>
<div class='content'><ol>
<li><strong>Entrada (Input):</strong> Los datos que se alimentan a la red neuronal.</li>
<li><strong>Pesos (Weights):</strong> Parámetros ajustables que determinan la importancia de cada entrada.</li>
<li><strong>Sesgo (Bias):</strong> Un término adicional que permite ajustar la salida de la red.</li>
<li><strong>Función de activación (Activation Function):</strong> Una función no lineal que introduce no linealidades en la red.</li>
</ol>
</div><h2><p>Proceso</p>
</h2>
<div class='content'><ol>
<li>
<p><strong>Cálculo de la entrada ponderada:</strong>
\[
z = W \cdot X + b
\]
Donde \( W \) son los pesos, \( X \) es la entrada y \( b \) es el sesgo.</p>
</li>
<li>
<p><strong>Aplicación de la función de activación:</strong>
\[
a = \sigma(z)
\]
Donde \( \sigma \) es la función de activación.</p>
</li>
<li>
<p><strong>Repetición del proceso para cada capa:</strong> El resultado \( a \) se convierte en la entrada para la siguiente capa.</p>
</li>
</ol>
</div><h2><p>Ejemplo</p>
</h2>
<div class='content'><p>Supongamos una red neuronal con una sola capa oculta:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIERlZmluaWNpw7NuIGRlIGxhIGZ1bmNpw7NuIGRlIGFjdGl2YWNpw7NuIChzaWdtb2lkZSkKZGVmIHNpZ21vaWQoeik6CiAgICByZXR1cm4gMSAvICgxICsgbnAuZXhwKC16KSkKCiMgRW50cmFkYXMKWCA9IG5wLmFycmF5KFswLjUsIDAuMV0pCgojIFBlc29zIHkgc2VzZ29zClcxID0gbnAuYXJyYXkoW1swLjIsIDAuOF0sIFswLjUsIDAuM11dKQpiMSA9IG5wLmFycmF5KFswLjEsIDAuMl0pCgpXMiA9IG5wLmFycmF5KFswLjYsIDAuOV0pCmIyID0gMC4zCgojIFByb3BhZ2FjacOzbiBoYWNpYSBhZGVsYW50ZQp6MSA9IG5wLmRvdChXMSwgWCkgKyBiMQphMSA9IHNpZ21vaWQoejEpCgp6MiA9IG5wLmRvdChXMiwgYTEpICsgYjIKYTIgPSBzaWdtb2lkKHoyKQoKcHJpbnQoIlNhbGlkYSBkZSBsYSByZWQ6IiwgYTIp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Definici&oacute;n de la funci&oacute;n de activaci&oacute;n (sigmoide)
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Entradas
X = np.array([0.5, 0.1])

# Pesos y sesgos
W1 = np.array([[0.2, 0.8], [0.5, 0.3]])
b1 = np.array([0.1, 0.2])

W2 = np.array([0.6, 0.9])
b2 = 0.3

# Propagaci&oacute;n hacia adelante
z1 = np.dot(W1, X) + b1
a1 = sigmoid(z1)

z2 = np.dot(W2, a1) + b2
a2 = sigmoid(z2)

print(&quot;Salida de la red:&quot;, a2)</pre></div><div class='content'></div><h2><p>Explicación del Código</p>
</h2>
<div class='content'><ol>
<li><strong>Definición de la función de activación:</strong> En este caso, se utiliza la función sigmoide.</li>
<li><strong>Entradas, pesos y sesgos:</strong> Se definen las entradas \( X \), los pesos \( W1 \) y \( W2 \), y los sesgos \( b1 \) y \( b2 \).</li>
<li><strong>Cálculo de la entrada ponderada y aplicación de la función de activación:</strong> Se realiza para cada capa de la red.</li>
<li><strong>Salida de la red:</strong> La salida final \( a2 \) es el resultado de la propagación hacia adelante.</li>
</ol>
</div><h1><p>Propagación hacia atrás (Backpropagation)</p>
</h1>
<div class='content'></div><h2><p>Conceptos Clave</p>
</h2>
<div class='content'><ol>
<li><strong>Función de pérdida (Loss Function):</strong> Mide el error entre la salida predicha y la salida real.</li>
<li><strong>Gradiente (Gradient):</strong> La derivada de la función de pérdida con respecto a los pesos.</li>
<li><strong>Actualización de pesos:</strong> Ajuste de los pesos utilizando el gradiente para minimizar el error.</li>
</ol>
</div><h2><p>Proceso</p>
</h2>
<div class='content'><ol>
<li>
<p><strong>Cálculo del error:</strong>
\[
\text{Error} = \text{Salida predicha} - \text{Salida real}
\]</p>
</li>
<li>
<p><strong>Cálculo del gradiente:</strong>
\[
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial W}
\]
Donde \( L \) es la función de pérdida, \( a \) es la activación y \( z \) es la entrada ponderada.</p>
</li>
<li>
<p><strong>Actualización de pesos:</strong>
\[
W = W - \eta \cdot \frac{\partial L}{\partial W}
\]
Donde \( \eta \) es la tasa de aprendizaje.</p>
</li>
</ol>
</div><h2><p>Ejemplo</p>
</h2>
<div class='content'><p>Continuando con el ejemplo anterior, supongamos que la salida real es \( y = 0.7 \):</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBTYWxpZGEgcmVhbAp5ID0gMC43CgojIEZ1bmNpw7NuIGRlIHDDqXJkaWRhIChlcnJvciBjdWFkcsOhdGljbyBtZWRpbykKZGVmIGxvc3MoYTIsIHkpOgogICAgcmV0dXJuIDAuNSAqIChhMiAtIHkpICoqIDIKCiMgRGVyaXZhZGEgZGUgbGEgZnVuY2nDs24gZGUgcMOpcmRpZGEgY29uIHJlc3BlY3RvIGEgbGEgc2FsaWRhCmRMX2RhMiA9IGEyIC0geQoKIyBEZXJpdmFkYSBkZSBsYSBmdW5jacOzbiBkZSBhY3RpdmFjacOzbiBzaWdtb2lkZQpkZWYgc2lnbW9pZF9kZXJpdmF0aXZlKGEpOgogICAgcmV0dXJuIGEgKiAoMSAtIGEpCgojIEdyYWRpZW50ZSBkZSBsYSBjYXBhIGRlIHNhbGlkYQpkTF9kejIgPSBkTF9kYTIgKiBzaWdtb2lkX2Rlcml2YXRpdmUoYTIpCmRMX2RXMiA9IGRMX2R6MiAqIGExCgojIEdyYWRpZW50ZSBkZSBsYSBjYXBhIG9jdWx0YQpkTF9kYTEgPSBkTF9kejIgKiBXMgpkTF9kejEgPSBkTF9kYTEgKiBzaWdtb2lkX2Rlcml2YXRpdmUoYTEpCmRMX2RXMSA9IG5wLm91dGVyKGRMX2R6MSwgWCkKCiMgQWN0dWFsaXphY2nDs24gZGUgcGVzb3MKZXRhID0gMC4xClcyIC09IGV0YSAqIGRMX2RXMgpiMiAtPSBldGEgKiBkTF9kejIKClcxIC09IGV0YSAqIGRMX2RXMQpiMSAtPSBldGEgKiBkTF9kejEKCnByaW50KCJQZXNvcyBhY3R1YWxpemFkb3MgVzE6IiwgVzEpCnByaW50KCJQZXNvcyBhY3R1YWxpemFkb3MgVzI6IiwgVzIp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Salida real
y = 0.7

# Funci&oacute;n de p&eacute;rdida (error cuadr&aacute;tico medio)
def loss(a2, y):
    return 0.5 * (a2 - y) ** 2

# Derivada de la funci&oacute;n de p&eacute;rdida con respecto a la salida
dL_da2 = a2 - y

# Derivada de la funci&oacute;n de activaci&oacute;n sigmoide
def sigmoid_derivative(a):
    return a * (1 - a)

# Gradiente de la capa de salida
dL_dz2 = dL_da2 * sigmoid_derivative(a2)
dL_dW2 = dL_dz2 * a1

# Gradiente de la capa oculta
dL_da1 = dL_dz2 * W2
dL_dz1 = dL_da1 * sigmoid_derivative(a1)
dL_dW1 = np.outer(dL_dz1, X)

# Actualizaci&oacute;n de pesos
eta = 0.1
W2 -= eta * dL_dW2
b2 -= eta * dL_dz2

W1 -= eta * dL_dW1
b1 -= eta * dL_dz1

print(&quot;Pesos actualizados W1:&quot;, W1)
print(&quot;Pesos actualizados W2:&quot;, W2)</pre></div><div class='content'></div><h2><p>Explicación del Código</p>
</h2>
<div class='content'><ol>
<li><strong>Cálculo del error:</strong> Se calcula la diferencia entre la salida predicha \( a2 \) y la salida real \( y \).</li>
<li><strong>Cálculo del gradiente:</strong> Se calcula el gradiente de la función de pérdida con respecto a los pesos de cada capa.</li>
<li><strong>Actualización de pesos:</strong> Se ajustan los pesos utilizando el gradiente y una tasa de aprendizaje \( \eta \).</li>
</ol>
</div><h1><p>Ejercicio Práctico</p>
</h1>
<div class='content'></div><h2><p>Ejercicio</p>
</h2>
<div class='content'><p>Implementa una red neuronal con una capa oculta que realice la clasificación binaria de un conjunto de datos simple. Utiliza la propagación hacia adelante y hacia atrás para entrenar la red.</p>
</div><h2><p>Solución</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Funci&oacute;n de activaci&oacute;n (sigmoide)
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Derivada de la funci&oacute;n de activaci&oacute;n sigmoide
def sigmoid_derivative(a):
    return a * (1 - a)

# Funci&oacute;n de p&eacute;rdida (error cuadr&aacute;tico medio)
def loss(a, y):
    return 0.5 * (a - y) ** 2

# Datos de entrenamiento
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 1, 1, 0])  # XOR

# Inicializaci&oacute;n de pesos y sesgos
np.random.seed(42)
W1 = np.random.rand(2, 2)
b1 = np.random.rand(2)
W2 = np.random.rand(2)
b2 = np.random.rand(1)

# Tasa de aprendizaje
eta = 0.1

# Entrenamiento
for epoch in range(10000):
    for i in range(len(X)):
        # Propagaci&oacute;n hacia adelante
        z1 = np.dot(W1, X[i]) + b1
        a1 = sigmoid(z1)
        
        z2 = np.dot(W2, a1) + b2
        a2 = sigmoid(z2)
        
        # C&aacute;lculo del error
        error = loss(a2, y[i])
        
        # Propagaci&oacute;n hacia atr&aacute;s
        dL_da2 = a2 - y[i]
        dL_dz2 = dL_da2 * sigmoid_derivative(a2)
        dL_dW2 = dL_dz2 * a1
        
        dL_da1 = dL_dz2 * W2
        dL_dz1 = dL_da1 * sigmoid_derivative(a1)
        dL_dW1 = np.outer(dL_dz1, X[i])
        
        # Actualizaci&oacute;n de pesos
        W2 -= eta * dL_dW2
        b2 -= eta * dL_dz2
        
        W1 -= eta * dL_dW1
        b1 -= eta * dL_dz1

# Prueba de la red entrenada
for i in range(len(X)):
    z1 = np.dot(W1, X[i]) + b1
    a1 = sigmoid(z1)
    
    z2 = np.dot(W2, a1) + b2
    a2 = sigmoid(z2)
    
    print(f&quot;Entrada: {X[i]}, Salida predicha: {a2}, Salida real: {y[i]}&quot;)</pre></div><div class='content'></div><h2><p>Explicación del Código</p>
</h2>
<div class='content'><ol>
<li><strong>Inicialización de pesos y sesgos:</strong> Se inicializan aleatoriamente los pesos y sesgos.</li>
<li><strong>Entrenamiento:</strong> Se realiza la propagación hacia adelante y hacia atrás para cada muestra de entrenamiento, ajustando los pesos en cada iteración.</li>
<li><strong>Prueba:</strong> Se prueba la red entrenada con los datos de entrada para verificar la precisión de las predicciones.</li>
</ol>
</div><h1><p>Conclusión</p>
</h1>
<div class='content'><p>En esta sección, hemos cubierto los conceptos y procesos de la propagación hacia adelante y hacia atrás en redes neuronales. Estos procesos son fundamentales para el entrenamiento de redes neuronales, permitiendo ajustar los pesos para minimizar el error de predicción. Con la práctica y la implementación de estos conceptos, podrás desarrollar y entrenar redes neuronales para una variedad de aplicaciones en Deep Learning.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='02-02-funcion-de-activacion' title="Función de activación">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-04-optimizacion-funcion-de-perdida' title="Optimización y función de pérdida">
				<span class="d-none d-md-inline">Siguiente &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<!-- 
<h1>Publicidad</h1>
<p>Este espacio está destinado a publicidad.</p>
<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
<p>¡Gracias por colaborar!</p>
-->

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725"
     crossorigin="anonymous"></script>
<!-- enterprise_campus -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-0611338592562725"
     data-ad-slot="6914733106"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</div>
</div>

<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Proyecto</a> | 
<a href="/about">Sobre nosotros</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donaciones</a> | 
<a href="/licence">Licencia</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Usamos cookies para mejorar tu experiencia de uso y ofrecer contenidos adaptados a tus intereses.
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
