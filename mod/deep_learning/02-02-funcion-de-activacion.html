<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Función de Activación</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/02-02-funcion-de-activacion" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/02-02-funcion-de-activacion" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/02-02-activation-function" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-2 p-md-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/deep_learning/02-02-activation-function" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/deep_learning/02-02-funcion-de-activacion" class="px-2">CA</a>
								</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<!-- <a href="/">Home</a>  -->
									<a href="./">Contenido del curso</a>
					<span class="sep">|</span>
								<a href="/all/competencias">Competencias técnicas</a>
				<a href="/all/conocimientos">Conocimientos</a>
				<a href="/all/soft_skills">Competencias sociales</a>
			</div>
		</div>
	</div>
</div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='02-01-perceptron-perceptron-multicapa' title="Perceptrón y Perceptrón Multicapa">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Función de Activación</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-03-propagacion-hacia-adelante-atras' title="Propagación hacia adelante y hacia atrás">
				<span class="d-none d-md-inline">Siguiente &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'></div><h1><p>Introducción</p>
</h1>
<div class='content'><p>En este tema, exploraremos las funciones de activación, un componente crucial en las redes neuronales. Las funciones de activación determinan si una neurona debe activarse o no, introduciendo no linealidades en el modelo que permiten a la red aprender y representar datos complejos.</p>
</div><h1><p>Objetivos</p>
</h1>
<div class='content'><ul>
<li>Comprender el propósito y la importancia de las funciones de activación.</li>
<li>Conocer las funciones de activación más comunes y sus características.</li>
<li>Implementar funciones de activación en código.</li>
<li>Analizar las ventajas y desventajas de cada función de activación.</li>
</ul>
</div><h1><p>¿Qué es una Función de Activación?</p>
</h1>
<div class='content'><p>Una función de activación es una función matemática que se aplica a la salida de una neurona. Su propósito principal es introducir no linealidades en la red neuronal, permitiendo que la red aprenda y modele relaciones complejas en los datos.</p>
</div><h2><p>Propósito de las Funciones de Activación</p>
</h2>
<div class='content'><ol>
<li><strong>Introducir No Linealidad</strong>: Sin funciones de activación, una red neuronal sería simplemente una combinación lineal de sus entradas, limitando su capacidad para resolver problemas complejos.</li>
<li><strong>Controlar la Activación de Neuronas</strong>: Determinan si una neurona debe activarse (es decir, si su salida debe ser transmitida a la siguiente capa).</li>
</ol>
</div><h1><p>Tipos de Funciones de Activación</p>
</h1>
<div class='content'></div><h2><ol>
<li>Sigmoide</li>
</ol>
</h2>
<div class='content'><p>La función sigmoide es una de las funciones de activación más antiguas y se define como:</p>
<p>\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<h4>Características:</h4>
<ul>
<li><strong>Rango</strong>: (0, 1)</li>
<li><strong>Ventajas</strong>: Suave y diferenciable.</li>
<li><strong>Desventajas</strong>: Problema de desvanecimiento del gradiente, salida no centrada en cero.</li>
</ul>
<h4>Ejemplo en Python:</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKCmRlZiBzaWdtb2lkKHgpOgogICAgcmV0dXJuIDEgLyAoMSArIG5wLmV4cCgteCkpCgp4ID0gbnAubGluc3BhY2UoLTEwLCAxMCwgMTAwKQp5ID0gc2lnbW9pZCh4KQoKcGx0LnBsb3QoeCwgeSkKcGx0LnRpdGxlKCdGdW5jacOzbiBTaWdtb2lkZScpCnBsdC54bGFiZWwoJ3gnKQpwbHQueWxhYmVsKCfPgyh4KScpCnBsdC5ncmlkKFRydWUpCnBsdC5zaG93KCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.linspace(-10, 10, 100)
y = sigmoid(x)

plt.plot(x, y)
plt.title('Funci&oacute;n Sigmoide')
plt.xlabel('x')
plt.ylabel('&sigma;(x)')
plt.grid(True)
plt.show()</pre></div><div class='content'></div><h2><ol start="2">
<li>Tanh (Tangente Hiperbólica)</li>
</ol>
</h2>
<div class='content'><p>La función Tanh es otra función de activación comúnmente utilizada y se define como:</p>
<p>\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<h4>Características:</h4>
<ul>
<li><strong>Rango</strong>: (-1, 1)</li>
<li><strong>Ventajas</strong>: Salida centrada en cero, más fuerte que la sigmoide.</li>
<li><strong>Desventajas</strong>: También sufre del problema de desvanecimiento del gradiente.</li>
</ul>
<h4>Ejemplo en Python:</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHRhbmgoeCk6CiAgICByZXR1cm4gbnAudGFuaCh4KQoKeSA9IHRhbmgoeCkKCnBsdC5wbG90KHgsIHkpCnBsdC50aXRsZSgnRnVuY2nDs24gVGFuaCcpCnBsdC54bGFiZWwoJ3gnKQpwbHQueWxhYmVsKCd0YW5oKHgpJykKcGx0LmdyaWQoVHJ1ZSkKcGx0LnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def tanh(x):
    return np.tanh(x)

y = tanh(x)

plt.plot(x, y)
plt.title('Funci&oacute;n Tanh')
plt.xlabel('x')
plt.ylabel('tanh(x)')
plt.grid(True)
plt.show()</pre></div><div class='content'></div><h2><ol start="3">
<li>ReLU (Unidad Lineal Rectificada)</li>
</ol>
</h2>
<div class='content'><p>La función ReLU es actualmente la función de activación más popular y se define como:</p>
<p>\[ \text{ReLU}(x) = \max(0, x) \]</p>
<h4>Características:</h4>
<ul>
<li><strong>Rango</strong>: [0, ∞)</li>
<li><strong>Ventajas</strong>: Simple, eficiente, no sufre del problema de desvanecimiento del gradiente.</li>
<li><strong>Desventajas</strong>: Problema de neuronas muertas (cuando x &lt; 0).</li>
</ul>
<h4>Ejemplo en Python:</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHJlbHUoeCk6CiAgICByZXR1cm4gbnAubWF4aW11bSgwLCB4KQoKeSA9IHJlbHUoeCkKCnBsdC5wbG90KHgsIHkpCnBsdC50aXRsZSgnRnVuY2nDs24gUmVMVScpCnBsdC54bGFiZWwoJ3gnKQpwbHQueWxhYmVsKCdSZUxVKHgpJykKcGx0LmdyaWQoVHJ1ZSkKcGx0LnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def relu(x):
    return np.maximum(0, x)

y = relu(x)

plt.plot(x, y)
plt.title('Funci&oacute;n ReLU')
plt.xlabel('x')
plt.ylabel('ReLU(x)')
plt.grid(True)
plt.show()</pre></div><div class='content'></div><h2><ol start="4">
<li>Leaky ReLU</li>
</ol>
</h2>
<div class='content'><p>Leaky ReLU es una variante de ReLU que intenta resolver el problema de las neuronas muertas:</p>
<p>\[ \text{Leaky ReLU}(x) = \begin{cases}
x &amp; \text{si } x &gt; 0 <br>\alpha x &amp; \text{si } x \leq 0
\end{cases} \]</p>
<p>donde \( \alpha \) es un pequeño valor positivo.</p>
<h4>Características:</h4>
<ul>
<li><strong>Rango</strong>: (-∞, ∞)</li>
<li><strong>Ventajas</strong>: Mitiga el problema de neuronas muertas.</li>
<li><strong>Desventajas</strong>: Introduce un pequeño sesgo en las salidas negativas.</li>
</ul>
<h4>Ejemplo en Python:</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIGxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSk6CiAgICByZXR1cm4gbnAud2hlcmUoeCA+IDAsIHgsIGFscGhhICogeCkKCnkgPSBsZWFreV9yZWx1KHgpCgpwbHQucGxvdCh4LCB5KQpwbHQudGl0bGUoJ0Z1bmNpw7NuIExlYWt5IFJlTFUnKQpwbHQueGxhYmVsKCd4JykKcGx0LnlsYWJlbCgnTGVha3kgUmVMVSh4KScpCnBsdC5ncmlkKFRydWUpCnBsdC5zaG93KCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

y = leaky_relu(x)

plt.plot(x, y)
plt.title('Funci&oacute;n Leaky ReLU')
plt.xlabel('x')
plt.ylabel('Leaky ReLU(x)')
plt.grid(True)
plt.show()</pre></div><div class='content'></div><h1><p>Comparación de Funciones de Activación</p>
</h1>
<div class='content'><table>
<thead>
<tr>
<th>Función</th>
<th>Rango</th>
<th>Ventajas</th>
<th>Desventajas</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sigmoide</td>
<td>(0, 1)</td>
<td>Suave, diferenciable</td>
<td>Desvanecimiento del gradiente, no centrada en cero</td>
</tr>
<tr>
<td>Tanh</td>
<td>(-1, 1)</td>
<td>Salida centrada en cero, más fuerte que sigmoide</td>
<td>Desvanecimiento del gradiente</td>
</tr>
<tr>
<td>ReLU</td>
<td>[0, ∞)</td>
<td>Simple, eficiente, no desvanecimiento del gradiente</td>
<td>Neuronas muertas</td>
</tr>
<tr>
<td>Leaky ReLU</td>
<td>(-∞, ∞)</td>
<td>Mitiga neuronas muertas</td>
<td>Introduce sesgo en salidas negativas</td>
</tr>
</tbody>
</table>
</div><h1><p>Ejercicio Práctico</p>
</h1>
<div class='content'></div><h2><p>Ejercicio 1: Implementar y Comparar Funciones de Activación</p>
</h2>
<div class='content'><ol>
<li>Implementa las funciones de activación Sigmoide, Tanh, ReLU y Leaky ReLU en Python.</li>
<li>Genera gráficos para cada función de activación.</li>
<li>Compara las salidas y discute las diferencias.</li>
</ol>
<h4>Solución:</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKCiMgRGVmaW5pY2nDs24gZGUgZnVuY2lvbmVzIGRlIGFjdGl2YWNpw7NuCmRlZiBzaWdtb2lkKHgpOgogICAgcmV0dXJuIDEgLyAoMSArIG5wLmV4cCgteCkpCgpkZWYgdGFuaCh4KToKICAgIHJldHVybiBucC50YW5oKHgpCgpkZWYgcmVsdSh4KToKICAgIHJldHVybiBucC5tYXhpbXVtKDAsIHgpCgpkZWYgbGVha3lfcmVsdSh4LCBhbHBoYT0wLjAxKToKICAgIHJldHVybiBucC53aGVyZSh4ID4gMCwgeCwgYWxwaGEgKiB4KQoKIyBHZW5lcmFyIGRhdG9zCnggPSBucC5saW5zcGFjZSgtMTAsIDEwLCAxMDApCgojIENhbGN1bGFyIHNhbGlkYXMKeV9zaWdtb2lkID0gc2lnbW9pZCh4KQp5X3RhbmggPSB0YW5oKHgpCnlfcmVsdSA9IHJlbHUoeCkKeV9sZWFreV9yZWx1ID0gbGVha3lfcmVsdSh4KQoKIyBHcmFmaWNhcgpwbHQuZmlndXJlKGZpZ3NpemU9KDEyLCA4KSkKCnBsdC5zdWJwbG90KDIsIDIsIDEpCnBsdC5wbG90KHgsIHlfc2lnbW9pZCkKcGx0LnRpdGxlKCdGdW5jacOzbiBTaWdtb2lkZScpCnBsdC54bGFiZWwoJ3gnKQpwbHQueWxhYmVsKCfPgyh4KScpCnBsdC5ncmlkKFRydWUpCgpwbHQuc3VicGxvdCgyLCAyLCAyKQpwbHQucGxvdCh4LCB5X3RhbmgpCnBsdC50aXRsZSgnRnVuY2nDs24gVGFuaCcpCnBsdC54bGFiZWwoJ3gnKQpwbHQueWxhYmVsKCd0YW5oKHgpJykKcGx0LmdyaWQoVHJ1ZSkKCnBsdC5zdWJwbG90KDIsIDIsIDMpCnBsdC5wbG90KHgsIHlfcmVsdSkKcGx0LnRpdGxlKCdGdW5jacOzbiBSZUxVJykKcGx0LnhsYWJlbCgneCcpCnBsdC55bGFiZWwoJ1JlTFUoeCknKQpwbHQuZ3JpZChUcnVlKQoKcGx0LnN1YnBsb3QoMiwgMiwgNCkKcGx0LnBsb3QoeCwgeV9sZWFreV9yZWx1KQpwbHQudGl0bGUoJ0Z1bmNpw7NuIExlYWt5IFJlTFUnKQpwbHQueGxhYmVsKCd4JykKcGx0LnlsYWJlbCgnTGVha3kgUmVMVSh4KScpCnBsdC5ncmlkKFRydWUpCgpwbHQudGlnaHRfbGF5b3V0KCkKcGx0LnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import matplotlib.pyplot as plt

# Definici&oacute;n de funciones de activaci&oacute;n
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

# Generar datos
x = np.linspace(-10, 10, 100)

# Calcular salidas
y_sigmoid = sigmoid(x)
y_tanh = tanh(x)
y_relu = relu(x)
y_leaky_relu = leaky_relu(x)

# Graficar
plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
plt.plot(x, y_sigmoid)
plt.title('Funci&oacute;n Sigmoide')
plt.xlabel('x')
plt.ylabel('&sigma;(x)')
plt.grid(True)

plt.subplot(2, 2, 2)
plt.plot(x, y_tanh)
plt.title('Funci&oacute;n Tanh')
plt.xlabel('x')
plt.ylabel('tanh(x)')
plt.grid(True)

plt.subplot(2, 2, 3)
plt.plot(x, y_relu)
plt.title('Funci&oacute;n ReLU')
plt.xlabel('x')
plt.ylabel('ReLU(x)')
plt.grid(True)

plt.subplot(2, 2, 4)
plt.plot(x, y_leaky_relu)
plt.title('Funci&oacute;n Leaky ReLU')
plt.xlabel('x')
plt.ylabel('Leaky ReLU(x)')
plt.grid(True)

plt.tight_layout()
plt.show()</pre></div><div class='content'></div><h2><p>Retroalimentación y Consejos</p>
</h2>
<div class='content'><ul>
<li><strong>Errores Comunes</strong>: Asegúrate de que los valores de entrada a las funciones de activación estén correctamente escalados. Valores muy grandes o muy pequeños pueden causar problemas de desbordamiento o subdesbordamiento, especialmente en la función sigmoide.</li>
<li><strong>Consejo</strong>: Experimenta con diferentes funciones de activación en tus modelos para ver cuál funciona mejor para tu problema específico. No hay una solución única que funcione para todos los casos.</li>
</ul>
</div><h1><p>Conclusión</p>
</h1>
<div class='content'><p>En esta sección, hemos aprendido sobre las funciones de activación, su propósito y las más comunes utilizadas en redes neuronales. Hemos implementado y comparado varias funciones de activación, entendiendo sus ventajas y desventajas. Con este conocimiento, estarás mejor preparado para elegir y aplicar funciones de activación adecuadas en tus modelos de deep learning.</p>
<p>En el próximo tema, exploraremos la propagación hacia adelante y hacia atrás, procesos fundamentales para el entrenamiento de redes neuronales.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='02-01-perceptron-perceptron-multicapa' title="Perceptrón y Perceptrón Multicapa">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-03-propagacion-hacia-adelante-atras' title="Propagación hacia adelante y hacia atrás">
				<span class="d-none d-md-inline">Siguiente &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Usamos cookies para mejorar tu experiencia de uso y ofrecer contenidos adaptados a tus intereses.
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
