<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow, noarchive">
    <title>Proyecto 3: Desarrollo de un Agente con Aprendizaje Automático</title>

    <link rel="alternate" href="https://campusempresa.com/mod/ia_videojuegos/06-03-proyecto-agente" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/ia_videojuegos/06-03-projecte-agent" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/ia_videojuegos/06-03-project-agent" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css?v=3" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
	</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
			<span>	<a href="https://enterprisecampus.net/mod/ia_videojuegos/06-03-project-agent" class="px-2">EN</a></b>
	|
	<b class="px-2">ES</b>
	|
	<a href="https://campusempresa.cat/mod/ia_videojuegos/06-03-projecte-agent" class="px-2">CA</a>
</span>
			<span class="d-none d-md-inline"><br><cite>Construyendo la sociedad de hoy y del mañana</cite></span>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Proyecto</a> | 
<a href="/about">Sobre nosotros</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donaciones</a> | 
<a href="/licence">Licencia</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				 					<a href="/categ/languages">Lenguajes</a>
				 					<a href="/categ/frameworks">Frameworks</a>
				 					<a href="/categ/tech-tools">Herramientas</a>
				 					<a href="/categ/foundations">Fundamentos</a>
				 					<a href="/categ/soft-skills">Competencias</a>
							</div>
		</div>
	</div>
</div>
		
<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='06-02-proyecto-npc' title="Proyecto 2: Creación de un NPC con Toma de Decisiones">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Proyecto 3: Desarrollo de un Agente con Aprendizaje Automático</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='07-01-libros-articulos' title="Libros y Artículos Recomendados">
				<span class="d-none d-md-inline">Siguiente &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>En este proyecto, desarrollaremos un agente que utiliza técnicas de aprendizaje automático para mejorar su comportamiento en un entorno de videojuego. Este proyecto integrará conceptos de los módulos anteriores, especialmente del Módulo 4 sobre Aprendizaje Automático.</p>
</div><h1><p>Objetivos del Proyecto</p>
</h1>
<div class='content'><ol>
<li><strong>Comprender los fundamentos del aprendizaje automático aplicado a videojuegos.</strong></li>
<li><strong>Implementar un agente que aprende y mejora su desempeño a través de la experiencia.</strong></li>
<li><strong>Evaluar y optimizar el rendimiento del agente.</strong></li>
</ol>
</div><h1><p>Requisitos Previos</p>
</h1>
<div class='content'><ul>
<li>Conocimiento básico de programación en Python.</li>
<li>Familiaridad con conceptos de aprendizaje automático.</li>
<li>Comprensión de los algoritmos de navegación y toma de decisiones.</li>
</ul>
</div><h1><p>Herramientas Necesarias</p>
</h1>
<div class='content'><ul>
<li><strong>Python</strong>: Lenguaje de programación principal.</li>
<li><strong>TensorFlow o PyTorch</strong>: Librerías para implementar redes neuronales.</li>
<li><strong>Gym</strong>: Entorno de simulación para entrenar agentes de aprendizaje por refuerzo.</li>
</ul>
</div><h1><p>Paso 1: Configuración del Entorno</p>
</h1>
<div class='content'></div><h2><p>Instalación de Librerías</p>
</h2>
<div class='content'><p>Primero, asegúrate de tener instaladas las librerías necesarias. Puedes instalarlas usando <code>pip</code>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgbnVtcHkgdGVuc29yZmxvdyBneW0="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install numpy tensorflow gym</pre></div><div class='content'></div><h2><p>Creación del Entorno de Simulación</p>
</h2>
<div class='content'><p>Utilizaremos el entorno <code>CartPole-v1</code> de Gym para este proyecto. Este entorno es sencillo y adecuado para principiantes en aprendizaje por refuerzo.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGd5bQoKZW52ID0gZ3ltLm1ha2UoJ0NhcnRQb2xlLXYxJyk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import gym

env = gym.make('CartPole-v1')</pre></div><div class='content'></div><h1><p>Paso 2: Implementación del Agente</p>
</h1>
<div class='content'></div><h2><p>Definición del Modelo</p>
</h2>
<div class='content'><p>Utilizaremos una red neuronal para que el agente aprenda a tomar decisiones. La red tendrá una capa de entrada, una capa oculta y una capa de salida.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzIGltcG9ydCBsYXllcnMKCmRlZiBjcmVhdGVfbW9kZWwoKToKICAgIG1vZGVsID0gdGYua2VyYXMuU2VxdWVudGlhbChbCiAgICAgICAgbGF5ZXJzLkRlbnNlKDI0LCBhY3RpdmF0aW9uPSdyZWx1JywgaW5wdXRfc2hhcGU9KDQsKSksCiAgICAgICAgbGF5ZXJzLkRlbnNlKDI0LCBhY3RpdmF0aW9uPSdyZWx1JyksCiAgICAgICAgbGF5ZXJzLkRlbnNlKDIsIGFjdGl2YXRpb249J2xpbmVhcicpCiAgICBdKQogICAgbW9kZWwuY29tcGlsZShvcHRpbWl6ZXI9J2FkYW0nLCBsb3NzPSdtc2UnKQogICAgcmV0dXJuIG1vZGVsCgptb2RlbCA9IGNyZWF0ZV9tb2RlbCgp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras import layers

def create_model():
    model = tf.keras.Sequential([
        layers.Dense(24, activation='relu', input_shape=(4,)),
        layers.Dense(24, activation='relu'),
        layers.Dense(2, activation='linear')
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

model = create_model()</pre></div><div class='content'></div><h2><p>Función de Política</p>
</h2>
<div class='content'><p>La función de política decide la acción que el agente tomará en cada estado.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgcG9saWN5KHN0YXRlLCBtb2RlbCk6CiAgICBxX3ZhbHVlcyA9IG1vZGVsLnByZWRpY3Qoc3RhdGUpCiAgICByZXR1cm4gbnAuYXJnbWF4KHFfdmFsdWVzWzBdKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def policy(state, model):
    q_values = model.predict(state)
    return np.argmax(q_values[0])</pre></div><div class='content'></div><h2><p>Entrenamiento del Agente</p>
</h2>
<div class='content'><p>El agente se entrenará utilizando el algoritmo Q-Learning con experiencia de repetición.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from collections import deque
import random

def train_agent(env, model, episodes=1000, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=32):
    memory = deque(maxlen=2000)
    for episode in range(episodes):
        state = env.reset()
        state = np.reshape(state, [1, 4])
        total_reward = 0
        done = False
        while not done:
            if np.random.rand() &lt;= epsilon:
                action = env.action_space.sample()
            else:
                action = policy(state, model)
            
            next_state, reward, done, _ = env.step(action)
            next_state = np.reshape(next_state, [1, 4])
            memory.append((state, action, reward, next_state, done))
            state = next_state
            total_reward += reward
            
            if done:
                print(f&quot;Episode: {episode+1}/{episodes}, Score: {total_reward}, Epsilon: {epsilon:.2}&quot;)
                break
            
            if len(memory) &gt; batch_size:
                minibatch = random.sample(memory, batch_size)
                for state, action, reward, next_state, done in minibatch:
                    target = reward
                    if not done:
                        target += gamma * np.amax(model.predict(next_state)[0])
                    target_f = model.predict(state)
                    target_f[0][action] = target
                    model.fit(state, target_f, epochs=1, verbose=0)
        
        if epsilon &gt; epsilon_min:
            epsilon *= epsilon_decay

train_agent(env, model)</pre></div><div class='content'></div><h1><p>Paso 3: Evaluación del Agente</p>
</h1>
<div class='content'></div><h2><p>Evaluación del Desempeño</p>
</h2>
<div class='content'><p>Después del entrenamiento, evaluaremos el desempeño del agente.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIGV2YWx1YXRlX2FnZW50KGVudiwgbW9kZWwsIGVwaXNvZGVzPTEwMCk6CiAgICB0b3RhbF9yZXdhcmRzID0gMAogICAgZm9yIGVwaXNvZGUgaW4gcmFuZ2UoZXBpc29kZXMpOgogICAgICAgIHN0YXRlID0gZW52LnJlc2V0KCkKICAgICAgICBzdGF0ZSA9IG5wLnJlc2hhcGUoc3RhdGUsIFsxLCA0XSkKICAgICAgICBkb25lID0gRmFsc2UKICAgICAgICB3aGlsZSBub3QgZG9uZToKICAgICAgICAgICAgYWN0aW9uID0gcG9saWN5KHN0YXRlLCBtb2RlbCkKICAgICAgICAgICAgc3RhdGUsIHJld2FyZCwgZG9uZSwgXyA9IGVudi5zdGVwKGFjdGlvbikKICAgICAgICAgICAgc3RhdGUgPSBucC5yZXNoYXBlKHN0YXRlLCBbMSwgNF0pCiAgICAgICAgICAgIHRvdGFsX3Jld2FyZHMgKz0gcmV3YXJkCiAgICBhdmVyYWdlX3Jld2FyZCA9IHRvdGFsX3Jld2FyZHMgLyBlcGlzb2RlcwogICAgcHJpbnQoZiJBdmVyYWdlIFJld2FyZCBvdmVyIHtlcGlzb2Rlc30gZXBpc29kZXM6IHthdmVyYWdlX3Jld2FyZH0iKQoKZXZhbHVhdGVfYWdlbnQoZW52LCBtb2RlbCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def evaluate_agent(env, model, episodes=100):
    total_rewards = 0
    for episode in range(episodes):
        state = env.reset()
        state = np.reshape(state, [1, 4])
        done = False
        while not done:
            action = policy(state, model)
            state, reward, done, _ = env.step(action)
            state = np.reshape(state, [1, 4])
            total_rewards += reward
    average_reward = total_rewards / episodes
    print(f&quot;Average Reward over {episodes} episodes: {average_reward}&quot;)

evaluate_agent(env, model)</pre></div><div class='content'></div><h1><p>Paso 4: Optimización del Agente</p>
</h1>
<div class='content'></div><h2><p>Ajuste de Hiperparámetros</p>
</h2>
<div class='content'><p>Para mejorar el rendimiento del agente, podemos ajustar los hiperparámetros como la tasa de aprendizaje, el tamaño de la red neuronal y los parámetros de exploración/explotación.</p>
</div><h2><p>Implementación de Técnicas Avanzadas</p>
</h2>
<div class='content'><p>Podemos implementar técnicas avanzadas como Dueling DQN, Double DQN o Prioritized Experience Replay para mejorar aún más el rendimiento del agente.</p>
</div><h1><p>Conclusión</p>
</h1>
<div class='content'><p>En este proyecto, hemos desarrollado un agente que utiliza aprendizaje automático para mejorar su comportamiento en un entorno de videojuego. Hemos cubierto desde la configuración del entorno hasta la implementación y optimización del agente. Este proyecto proporciona una base sólida para explorar técnicas más avanzadas y aplicarlas a entornos de videojuegos más complejos.</p>
<hr />
<p><strong>Resumen del Proyecto:</strong></p>
<ul>
<li><strong>Configuración del entorno:</strong> Instalación de librerías y creación del entorno de simulación.</li>
<li><strong>Implementación del agente:</strong> Definición del modelo, función de política y entrenamiento del agente.</li>
<li><strong>Evaluación del agente:</strong> Evaluación del desempeño del agente después del entrenamiento.</li>
<li><strong>Optimización del agente:</strong> Ajuste de hiperparámetros e implementación de técnicas avanzadas.</li>
</ul>
<p>Con este conocimiento, estás preparado para desarrollar agentes más sofisticados y aplicar técnicas de aprendizaje automático en tus propios proyectos de videojuegos.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='06-02-proyecto-npc' title="Proyecto 2: Creación de un NPC con Toma de Decisiones">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='07-01-libros-articulos' title="Libros y Artículos Recomendados">
				<span class="d-none d-md-inline">Siguiente &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<!-- 
<h1>Publicidad</h1>
<p>Este espacio está destinado a publicidad.</p>
<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
<p>¡Gracias por colaborar!</p>
-->

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725"
     crossorigin="anonymous"></script>
<!-- enterprise_campus -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-0611338592562725"
     data-ad-slot="6914733106"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</div>
</div>

<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Proyecto</a> | 
<a href="/about">Sobre nosotros</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donaciones</a> | 
<a href="/licence">Licencia</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Usamos cookies para mejorar tu experiencia de uso y ofrecer contenidos adaptados a tus intereses.
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
