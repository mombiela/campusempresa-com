<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Implementación de un Agente de Aprendizaje</title>

    <link rel="alternate" href="https://campusempresa.com/mod/ia_videojuegos/04-04-implementacion-agente" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/ia_videojuegos/04-04-implementacio-agent" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/ia_videojuegos/04-04-implementation-agent" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-2 p-md-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/ia_videojuegos/04-04-implementation-agent" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/ia_videojuegos/04-04-implementacio-agent" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='04-03-aprendizaje-refuerzo' title="Aprendizaje por Refuerzo">&#x25C4;Anterior</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Implementación de un Agente de Aprendizaje</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='05-01-integracion-motores' title="Integración de IA en Motores de Juego">Siguiente &#x25BA;</a>
			</div>
</div>
<div class='content'><p>En este tema, aprenderemos cómo implementar un agente de aprendizaje automático en un videojuego. Nos enfocaremos en los conceptos clave, los pasos necesarios para desarrollar el agente y proporcionaremos ejemplos prácticos y ejercicios para reforzar el aprendizaje.</p>
</div><h1><p>Objetivos del Tema</p>
</h1>
<div class='content'><ul>
<li>Comprender los componentes esenciales de un agente de aprendizaje.</li>
<li>Implementar un agente utilizando técnicas de aprendizaje por refuerzo.</li>
<li>Integrar el agente en un entorno de videojuego.</li>
<li>Evaluar y optimizar el rendimiento del agente.</li>
</ul>
</div><h1><p>Conceptos Clave</p>
</h1>
<div class='content'></div><h2><ol>
<li>Agente de Aprendizaje</li>
</ol>
</h2>
<div class='content'><p>Un agente de aprendizaje es una entidad que puede percibir su entorno, tomar decisiones y aprender de sus experiencias para mejorar su desempeño en una tarea específica.</p>
</div><h2><ol start="2">
<li>Aprendizaje por Refuerzo (Reinforcement Learning, RL)</li>
</ol>
</h2>
<div class='content'><p>El aprendizaje por refuerzo es una técnica de aprendizaje automático donde un agente aprende a tomar decisiones mediante la interacción con su entorno, recibiendo recompensas o penalizaciones.</p>
</div><h2><ol start="3">
<li>Componentes de un Agente de Aprendizaje</li>
</ol>
</h2>
<div class='content'><ul>
<li><strong>Entorno:</strong> El mundo en el que el agente opera.</li>
<li><strong>Estado:</strong> La representación del entorno en un momento dado.</li>
<li><strong>Acción:</strong> Las decisiones que el agente puede tomar.</li>
<li><strong>Recompensa:</strong> La retroalimentación que el agente recibe después de realizar una acción.</li>
</ul>
</div><h1><p>Pasos para Implementar un Agente de Aprendizaje</p>
</h1>
<div class='content'></div><h2><p>Paso 1: Definir el Entorno</p>
</h2>
<div class='content'><p>El primer paso es definir el entorno en el que el agente operará. Esto incluye la configuración del espacio de estados y acciones, así como la función de recompensa.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGd5bQoKIyBDcmVhciB1biBlbnRvcm5vIGRlIGVqZW1wbG8gKHVzYXJlbW9zIGVsIGVudG9ybm8gJ0NhcnRQb2xlLXYxJyBkZSBPcGVuQUkgR3ltKQplbnYgPSBneW0ubWFrZSgnQ2FydFBvbGUtdjEnKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import gym

# Crear un entorno de ejemplo (usaremos el entorno 'CartPole-v1' de OpenAI Gym)
env = gym.make('CartPole-v1')</pre></div><div class='content'></div><h2><p>Paso 2: Inicializar el Agente</p>
</h2>
<div class='content'><p>Inicializamos el agente con una política inicial y una estructura para almacenar el valor de las acciones (Q-Table).</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIEluaWNpYWxpemFyIGxhIFEtVGFibGUKc3RhdGVfc3BhY2UgPSBlbnYub2JzZXJ2YXRpb25fc3BhY2Uuc2hhcGVbMF0KYWN0aW9uX3NwYWNlID0gZW52LmFjdGlvbl9zcGFjZS5uCnFfdGFibGUgPSBucC56ZXJvcygoc3RhdGVfc3BhY2UsIGFjdGlvbl9zcGFjZSkpCgojIFBhcsOhbWV0cm9zIGRlIGFwcmVuZGl6YWplCmFscGhhID0gMC4xICAjIFRhc2EgZGUgYXByZW5kaXphamUKZ2FtbWEgPSAwLjk5ICAjIEZhY3RvciBkZSBkZXNjdWVudG8KZXBzaWxvbiA9IDEuMCAgIyBUYXNhIGRlIGV4cGxvcmFjacOzbgplcHNpbG9uX2RlY2F5ID0gMC45OTUKbWluX2Vwc2lsb24gPSAwLjAx"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Inicializar la Q-Table
state_space = env.observation_space.shape[0]
action_space = env.action_space.n
q_table = np.zeros((state_space, action_space))

# Par&aacute;metros de aprendizaje
alpha = 0.1  # Tasa de aprendizaje
gamma = 0.99  # Factor de descuento
epsilon = 1.0  # Tasa de exploraci&oacute;n
epsilon_decay = 0.995
min_epsilon = 0.01</pre></div><div class='content'></div><h2><p>Paso 3: Implementar el Algoritmo de Aprendizaje</p>
</h2>
<div class='content'><p>Implementamos el algoritmo de Q-Learning para actualizar la Q-Table basado en las experiencias del agente.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIGNob29zZV9hY3Rpb24oc3RhdGUpOgogICAgaWYgbnAucmFuZG9tLnJhbmQoKSA8IGVwc2lsb246CiAgICAgICAgcmV0dXJuIGVudi5hY3Rpb25fc3BhY2Uuc2FtcGxlKCkgICMgRXhwbG9yYWNpw7NuCiAgICBlbHNlOgogICAgICAgIHJldHVybiBucC5hcmdtYXgocV90YWJsZVtzdGF0ZV0pICAjIEV4cGxvdGFjacOzbgoKZGVmIHVwZGF0ZV9xX3RhYmxlKHN0YXRlLCBhY3Rpb24sIHJld2FyZCwgbmV4dF9zdGF0ZSk6CiAgICBiZXN0X25leHRfYWN0aW9uID0gbnAuYXJnbWF4KHFfdGFibGVbbmV4dF9zdGF0ZV0pCiAgICB0ZF90YXJnZXQgPSByZXdhcmQgKyBnYW1tYSAqIHFfdGFibGVbbmV4dF9zdGF0ZSwgYmVzdF9uZXh0X2FjdGlvbl0KICAgIHRkX2Vycm9yID0gdGRfdGFyZ2V0IC0gcV90YWJsZVtzdGF0ZSwgYWN0aW9uXQogICAgcV90YWJsZVtzdGF0ZSwgYWN0aW9uXSArPSBhbHBoYSAqIHRkX2Vycm9y"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def choose_action(state):
    if np.random.rand() &lt; epsilon:
        return env.action_space.sample()  # Exploraci&oacute;n
    else:
        return np.argmax(q_table[state])  # Explotaci&oacute;n

def update_q_table(state, action, reward, next_state):
    best_next_action = np.argmax(q_table[next_state])
    td_target = reward + gamma * q_table[next_state, best_next_action]
    td_error = td_target - q_table[state, action]
    q_table[state, action] += alpha * td_error</pre></div><div class='content'></div><h2><p>Paso 4: Entrenar al Agente</p>
</h2>
<div class='content'><p>Entrenamos al agente mediante la interacción continua con el entorno.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("bnVtX2VwaXNvZGVzID0gMTAwMAoKZm9yIGVwaXNvZGUgaW4gcmFuZ2UobnVtX2VwaXNvZGVzKToKICAgIHN0YXRlID0gZW52LnJlc2V0KCkKICAgIGRvbmUgPSBGYWxzZQogICAgdG90YWxfcmV3YXJkID0gMAoKICAgIHdoaWxlIG5vdCBkb25lOgogICAgICAgIGFjdGlvbiA9IGNob29zZV9hY3Rpb24oc3RhdGUpCiAgICAgICAgbmV4dF9zdGF0ZSwgcmV3YXJkLCBkb25lLCBfID0gZW52LnN0ZXAoYWN0aW9uKQogICAgICAgIHVwZGF0ZV9xX3RhYmxlKHN0YXRlLCBhY3Rpb24sIHJld2FyZCwgbmV4dF9zdGF0ZSkKICAgICAgICBzdGF0ZSA9IG5leHRfc3RhdGUKICAgICAgICB0b3RhbF9yZXdhcmQgKz0gcmV3YXJkCgogICAgZXBzaWxvbiA9IG1heChtaW5fZXBzaWxvbiwgZXBzaWxvbiAqIGVwc2lsb25fZGVjYXkpCiAgICBwcmludChmIkVwaXNvZGUge2VwaXNvZGUgKyAxfTogVG90YWwgUmV3YXJkOiB7dG90YWxfcmV3YXJkfSIp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>num_episodes = 1000

for episode in range(num_episodes):
    state = env.reset()
    done = False
    total_reward = 0

    while not done:
        action = choose_action(state)
        next_state, reward, done, _ = env.step(action)
        update_q_table(state, action, reward, next_state)
        state = next_state
        total_reward += reward

    epsilon = max(min_epsilon, epsilon * epsilon_decay)
    print(f&quot;Episode {episode + 1}: Total Reward: {total_reward}&quot;)</pre></div><div class='content'></div><h2><p>Paso 5: Evaluar el Agente</p>
</h2>
<div class='content'><p>Evaluamos el desempeño del agente después del entrenamiento.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("dG90YWxfcmV3YXJkcyA9IFtdCgpmb3IgZXBpc29kZSBpbiByYW5nZSgxMDApOgogICAgc3RhdGUgPSBlbnYucmVzZXQoKQogICAgZG9uZSA9IEZhbHNlCiAgICB0b3RhbF9yZXdhcmQgPSAwCgogICAgd2hpbGUgbm90IGRvbmU6CiAgICAgICAgYWN0aW9uID0gbnAuYXJnbWF4KHFfdGFibGVbc3RhdGVdKQogICAgICAgIHN0YXRlLCByZXdhcmQsIGRvbmUsIF8gPSBlbnYuc3RlcChhY3Rpb24pCiAgICAgICAgdG90YWxfcmV3YXJkICs9IHJld2FyZAoKICAgIHRvdGFsX3Jld2FyZHMuYXBwZW5kKHRvdGFsX3Jld2FyZCkKCnByaW50KGYiQXZlcmFnZSBSZXdhcmQgb3ZlciAxMDAgZXBpc29kZXM6IHtucC5tZWFuKHRvdGFsX3Jld2FyZHMpfSIp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>total_rewards = []

for episode in range(100):
    state = env.reset()
    done = False
    total_reward = 0

    while not done:
        action = np.argmax(q_table[state])
        state, reward, done, _ = env.step(action)
        total_reward += reward

    total_rewards.append(total_reward)

print(f&quot;Average Reward over 100 episodes: {np.mean(total_rewards)}&quot;)</pre></div><div class='content'></div><h1><p>Ejercicios Prácticos</p>
</h1>
<div class='content'></div><h2><p>Ejercicio 1: Modificar el Entorno</p>
</h2>
<div class='content'><p>Modifica el entorno para que el agente aprenda en un entorno diferente, como <code>MountainCar-v0</code>. Ajusta los parámetros de aprendizaje según sea necesario.</p>
</div><h2><p>Ejercicio 2: Implementar una Política Epsilon-Greedy Mejorada</p>
</h2>
<div class='content'><p>Mejora la política epsilon-greedy para que la tasa de exploración disminuya de manera más eficiente a lo largo del tiempo.</p>
</div><h2><p>Ejercicio 3: Visualizar el Proceso de Aprendizaje</p>
</h2>
<div class='content'><p>Implementa una visualización del proceso de aprendizaje del agente utilizando gráficos para mostrar la evolución de las recompensas a lo largo de los episodios.</p>
</div><h1><p>Soluciones a los Ejercicios</p>
</h1>
<div class='content'></div><h2><p>Solución al Ejercicio 1</p>
</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZW52ID0gZ3ltLm1ha2UoJ01vdW50YWluQ2FyLXYwJykKIyBBanVzdGFyIGxvcyBwYXLDoW1ldHJvcyBkZSBhcHJlbmRpemFqZSBzZWfDum4gc2VhIG5lY2VzYXJpbw=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>env = gym.make('MountainCar-v0')
# Ajustar los par&aacute;metros de aprendizaje seg&uacute;n sea necesario</pre></div><div class='content'></div><h2><p>Solución al Ejercicio 2</p>
</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZXBzaWxvbl9kZWNheSA9IDAuOTkgICMgQWp1c3RhciBsYSB0YXNhIGRlIGRlY2FpbWllbnRvIGRlIGVwc2lsb24="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>epsilon_decay = 0.99  # Ajustar la tasa de decaimiento de epsilon</pre></div><div class='content'></div><h2><p>Solución al Ejercicio 3</p>
</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG1hdHBsb3RsaWIucHlwbG90IGFzIHBsdAoKcmV3YXJkcyA9IFtdCgpmb3IgZXBpc29kZSBpbiByYW5nZShudW1fZXBpc29kZXMpOgogICAgc3RhdGUgPSBlbnYucmVzZXQoKQogICAgZG9uZSA9IEZhbHNlCiAgICB0b3RhbF9yZXdhcmQgPSAwCgogICAgd2hpbGUgbm90IGRvbmU6CiAgICAgICAgYWN0aW9uID0gY2hvb3NlX2FjdGlvbihzdGF0ZSkKICAgICAgICBuZXh0X3N0YXRlLCByZXdhcmQsIGRvbmUsIF8gPSBlbnYuc3RlcChhY3Rpb24pCiAgICAgICAgdXBkYXRlX3FfdGFibGUoc3RhdGUsIGFjdGlvbiwgcmV3YXJkLCBuZXh0X3N0YXRlKQogICAgICAgIHN0YXRlID0gbmV4dF9zdGF0ZQogICAgICAgIHRvdGFsX3Jld2FyZCArPSByZXdhcmQKCiAgICByZXdhcmRzLmFwcGVuZCh0b3RhbF9yZXdhcmQpCiAgICBlcHNpbG9uID0gbWF4KG1pbl9lcHNpbG9uLCBlcHNpbG9uICogZXBzaWxvbl9kZWNheSkKCnBsdC5wbG90KHJld2FyZHMpCnBsdC54bGFiZWwoJ0VwaXNvZGUnKQpwbHQueWxhYmVsKCdUb3RhbCBSZXdhcmQnKQpwbHQudGl0bGUoJ0xlYXJuaW5nIFByb2dyZXNzJykKcGx0LnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import matplotlib.pyplot as plt

rewards = []

for episode in range(num_episodes):
    state = env.reset()
    done = False
    total_reward = 0

    while not done:
        action = choose_action(state)
        next_state, reward, done, _ = env.step(action)
        update_q_table(state, action, reward, next_state)
        state = next_state
        total_reward += reward

    rewards.append(total_reward)
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Learning Progress')
plt.show()</pre></div><div class='content'></div><h1><p>Conclusión</p>
</h1>
<div class='content'><p>En esta sección, hemos aprendido a implementar un agente de aprendizaje utilizando técnicas de aprendizaje por refuerzo. Hemos cubierto los conceptos clave, los pasos necesarios para desarrollar el agente y proporcionado ejemplos prácticos y ejercicios para reforzar el aprendizaje. Con esta base, estarás preparado para explorar técnicas más avanzadas y aplicar agentes de aprendizaje en tus propios proyectos de videojuegos.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='04-03-aprendizaje-refuerzo' title="Aprendizaje por Refuerzo">&#x25C4;Anterior</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='05-01-integracion-motores' title="Integración de IA en Motores de Juego">Siguiente &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Usamos cookies para mejorar tu experiencia de uso y ofrecer contenidos adaptados a tus intereses.
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
