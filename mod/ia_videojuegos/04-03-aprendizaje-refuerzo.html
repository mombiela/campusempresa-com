<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aprendizaje por Refuerzo</title>

    <link rel="alternate" href="https://campusempresa.com/mod/ia_videojuegos/04-03-aprendizaje-refuerzo" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/ia_videojuegos/04-03-aprenentatge-reforc" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/ia_videojuegos/04-03-reinforcement-learning" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-2 p-md-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
										<a href="https://enterprisecampus.net/mod/ia_videojuegos/04-03-reinforcement-learning" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/mod/ia_videojuegos/04-03-aprenentatge-reforc" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='04-02-redes-neuronales' title="Redes Neuronales en Videojuegos">&#x25C4;Anterior</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Aprendizaje por Refuerzo</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='04-04-implementacion-agente' title="Implementación de un Agente de Aprendizaje">Siguiente &#x25BA;</a>
			</div>
</div>
<div class='content'><p>El aprendizaje por refuerzo (RL, por sus siglas en inglés) es una rama del aprendizaje automático que se centra en cómo los agentes deben tomar acciones en un entorno para maximizar alguna noción de recompensa acumulada. A diferencia de otros paradigmas de aprendizaje, el RL se basa en la interacción continua con el entorno y la retroalimentación en forma de recompensas o castigos.</p>
</div><h1><p>Conceptos Clave</p>
</h1>
<div class='content'><ol>
<li><strong>Agente</strong>: La entidad que toma decisiones y aprende de las interacciones con el entorno.</li>
<li><strong>Entorno</strong>: Todo lo que rodea al agente y con lo que interactúa.</li>
<li><strong>Estado (s)</strong>: Una representación de la situación actual del entorno.</li>
<li><strong>Acción (a)</strong>: Una decisión tomada por el agente que afecta el estado del entorno.</li>
<li><strong>Recompensa (r)</strong>: Retroalimentación recibida por el agente después de tomar una acción.</li>
<li><strong>Política (π)</strong>: Una estrategia que define las acciones que debe tomar el agente en cada estado.</li>
<li><strong>Función de valor (V)</strong>: Una estimación del valor esperado de los estados, es decir, la recompensa total esperada a largo plazo.</li>
<li><strong>Función Q (Q(s, a))</strong>: Una estimación del valor esperado de tomar una acción en un estado particular.</li>
</ol>
</div><h1><p>Ejemplo Práctico: El Problema del Camino Óptimo</p>
</h1>
<div class='content'><p>Imaginemos un agente que debe encontrar el camino óptimo en un laberinto. El agente recibe una recompensa positiva al llegar a la salida y recompensas negativas al chocar con paredes.</p>
</div><h2><p>Pseudocódigo de Q-Learning</p>
</h2>
<div class='content'><p>Q-Learning es uno de los algoritmos más populares en RL. Aquí hay un pseudocódigo básico:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBJbmljaWFsaXphY2nDs24KUSA9IHt9ICAjIFRhYmxhIFEgdmFjw61hCmZvciBlYWNoIHN0YXRlIHMgYW5kIGFjdGlvbiBhOgogICAgUVtzLCBhXSA9IDAgICMgSW5pY2lhbGl6YXIgUShzLCBhKSBhcmJpdHJhcmlhbWVudGUKCiMgUGFyw6FtZXRyb3MKYWxwaGEgPSAwLjEgICMgVGFzYSBkZSBhcHJlbmRpemFqZQpnYW1tYSA9IDAuOSAgIyBGYWN0b3IgZGUgZGVzY3VlbnRvCmVwc2lsb24gPSAwLjEgICMgUGFyw6FtZXRybyBkZSBleHBsb3JhY2nDs24tZXhwbG90YWNpw7NuCgojIEVwaXNvZGlvcyBkZSBlbnRyZW5hbWllbnRvCmZvciBlcGlzb2RlIGluIHJhbmdlKG51bV9lcGlzb2Rlcyk6CiAgICBzdGF0ZSA9IGluaXRpYWxfc3RhdGUgICMgRXN0YWRvIGluaWNpYWwKICAgIHdoaWxlIHN0YXRlIGlzIG5vdCB0ZXJtaW5hbDoKICAgICAgICAjIFNlbGVjY2nDs24gZGUgYWNjacOzbiAozrUtZ3JlZWR5KQogICAgICAgIGlmIHJhbmRvbSgpIDwgZXBzaWxvbjoKICAgICAgICAgICAgYWN0aW9uID0gcmFuZG9tX2FjdGlvbigpCiAgICAgICAgZWxzZToKICAgICAgICAgICAgYWN0aW9uID0gYXJnbWF4KFFbc3RhdGUsIGFdIGZvciBhIGluIGFjdGlvbnMoc3RhdGUpKQogICAgICAgIAogICAgICAgICMgUmVhbGl6YXIgYWNjacOzbiB5IG9ic2VydmFyIHJlc3VsdGFkbwogICAgICAgIG5leHRfc3RhdGUsIHJld2FyZCA9IHRha2VfYWN0aW9uKHN0YXRlLCBhY3Rpb24pCiAgICAgICAgCiAgICAgICAgIyBBY3R1YWxpemFjacOzbiBkZSBRCiAgICAgICAgYmVzdF9uZXh0X2FjdGlvbiA9IGFyZ21heChRW25leHRfc3RhdGUsIGFdIGZvciBhIGluIGFjdGlvbnMobmV4dF9zdGF0ZSkpCiAgICAgICAgUVtzdGF0ZSwgYWN0aW9uXSA9IFFbc3RhdGUsIGFjdGlvbl0gKyBhbHBoYSAqIChyZXdhcmQgKyBnYW1tYSAqIFFbbmV4dF9zdGF0ZSwgYmVzdF9uZXh0X2FjdGlvbl0gLSBRW3N0YXRlLCBhY3Rpb25dKQogICAgICAgIAogICAgICAgIHN0YXRlID0gbmV4dF9zdGF0ZQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Inicializaci&oacute;n
Q = {}  # Tabla Q vac&iacute;a
for each state s and action a:
    Q[s, a] = 0  # Inicializar Q(s, a) arbitrariamente

# Par&aacute;metros
alpha = 0.1  # Tasa de aprendizaje
gamma = 0.9  # Factor de descuento
epsilon = 0.1  # Par&aacute;metro de exploraci&oacute;n-explotaci&oacute;n

# Episodios de entrenamiento
for episode in range(num_episodes):
    state = initial_state  # Estado inicial
    while state is not terminal:
        # Selecci&oacute;n de acci&oacute;n (&epsilon;-greedy)
        if random() &lt; epsilon:
            action = random_action()
        else:
            action = argmax(Q[state, a] for a in actions(state))
        
        # Realizar acci&oacute;n y observar resultado
        next_state, reward = take_action(state, action)
        
        # Actualizaci&oacute;n de Q
        best_next_action = argmax(Q[next_state, a] for a in actions(next_state))
        Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])
        
        state = next_state</pre></div><div class='content'></div><h2><p>Explicación del Código</p>
</h2>
<div class='content'><ol>
<li><strong>Inicialización</strong>: Se crea una tabla Q vacía y se inicializan los valores de Q(s, a) a cero.</li>
<li><strong>Parámetros</strong>: Se definen los parámetros de tasa de aprendizaje (alpha), factor de descuento (gamma) y el parámetro de exploración-explotación (epsilon).</li>
<li><strong>Episodios de Entrenamiento</strong>: Se ejecutan múltiples episodios de entrenamiento donde el agente interactúa con el entorno.</li>
<li><strong>Selección de Acción</strong>: Se utiliza una política ε-greedy para balancear la exploración y explotación.</li>
<li><strong>Actualización de Q</strong>: Se actualiza la tabla Q utilizando la fórmula de Q-Learning.</li>
</ol>
</div><h1><p>Ejercicio Práctico</p>
</h1>
<div class='content'></div><h2><p>Ejercicio 1: Implementar Q-Learning en un Laberinto</p>
</h2>
<div class='content'><p><strong>Objetivo</strong>: Implementar el algoritmo Q-Learning para que un agente encuentre la salida de un laberinto.</p>
<p><strong>Instrucciones</strong>:</p>
<ol>
<li>Define un entorno de laberinto.</li>
<li>Implementa el algoritmo Q-Learning.</li>
<li>Entrena al agente y observa su comportamiento.</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import random

# Definici&oacute;n del entorno
class Laberinto:
    def __init__(self, grid):
        self.grid = grid
        self.start = (0, 0)
        self.end = (len(grid) - 1, len(grid[0]) - 1)
    
    def reset(self):
        self.position = self.start
        return self.position
    
    def step(self, action):
        x, y = self.position
        if action == 'up':
            x -= 1
        elif action == 'down':
            x += 1
        elif action == 'left':
            y -= 1
        elif action == 'right':
            y += 1
        
        if x &lt; 0 or x &gt;= len(self.grid) or y &lt; 0 or y &gt;= len(self.grid[0]) or self.grid[x][y] == 1:
            return self.position, -1, False  # Recompensa negativa por chocar con una pared
        
        self.position = (x, y)
        if self.position == self.end:
            return self.position, 10, True  # Recompensa positiva por llegar a la salida
        
        return self.position, -0.1, False  # Recompensa negativa por cada paso

# Par&aacute;metros
alpha = 0.1
gamma = 0.9
epsilon = 0.1
num_episodes = 1000

# Inicializaci&oacute;n
laberinto = Laberinto([
    [0, 0, 0, 0],
    [1, 1, 0, 1],
    [0, 0, 0, 0],
    [0, 1, 1, 0],
    [0, 0, 0, 0]
])
Q = {}
actions = ['up', 'down', 'left', 'right']

for x in range(len(laberinto.grid)):
    for y in range(len(laberinto.grid[0])):
        for action in actions:
            Q[((x, y), action)] = 0

# Entrenamiento
for episode in range(num_episodes):
    state = laberinto.reset()
    done = False
    while not done:
        if random.random() &lt; epsilon:
            action = random.choice(actions)
        else:
            action = max(actions, key=lambda a: Q[(state, a)])
        
        next_state, reward, done = laberinto.step(action)
        best_next_action = max(actions, key=lambda a: Q[(next_state, a)])
        Q[(state, action)] = Q[(state, action)] + alpha * (reward + gamma * Q[(next_state, best_next_action)] - Q[(state, action)])
        state = next_state

# Prueba del agente entrenado
state = laberinto.reset()
done = False
path = [state]
while not done:
    action = max(actions, key=lambda a: Q[(state, a)])
    state, _, done = laberinto.step(action)
    path.append(state)

print(&quot;Camino encontrado por el agente:&quot;, path)</pre></div><div class='content'></div><h2><p>Solución del Ejercicio</p>
</h2>
<div class='content'><p>El código anterior implementa el algoritmo Q-Learning en un entorno de laberinto. El agente aprende a encontrar la salida del laberinto mediante la interacción continua con el entorno y la actualización de la tabla Q.</p>
</div><h1><p>Conclusión</p>
</h1>
<div class='content'><p>En esta sección, hemos aprendido los conceptos básicos del aprendizaje por refuerzo y cómo implementar el algoritmo Q-Learning para resolver problemas de navegación en un entorno de laberinto. El aprendizaje por refuerzo es una herramienta poderosa para desarrollar comportamientos inteligentes en los personajes de los videojuegos, permitiéndoles aprender y adaptarse a través de la experiencia.</p>
<p>En el siguiente módulo, exploraremos cómo integrar estos algoritmos de IA en motores de juego y optimizar su rendimiento.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='04-02-redes-neuronales' title="Redes Neuronales en Videojuegos">&#x25C4;Anterior</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='04-04-implementacion-agente' title="Implementación de un Agente de Aprendizaje">Siguiente &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
