<!DOCTYPE html>
<html lang="es">
<head>
    <title> Funciones de Activación </title>
        
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, nofollow, noarchive">
    
    <link rel="alternate" href="https://campusempresa.com/cursos/tensorflow/04-03-activation-functions" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/cursos/tensorflow/04-03-activation-functions" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/courses/tensorflow/04-03-activation-functions" hreflang="en" />
    
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.ea63f62b9e.css" rel="stylesheet">
	 
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script>
  		var LANG = "es";
  		var CATEGORY = "frameworks";
  		var MOD_NAME = "tensorflow";
  		var TEMA_NAME = "4-3";
  		var TYPE = "mod";
  		var PATH = "mod/tensorflow/04-03-activation-functions";
  		var IS_INDEX = false;
  	</script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="module" src="/js/app.902a5a267d.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>
	  	
</head>

<body class="d-none">
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-0 text-end">
			<p class="mb-0 p-0">	<a href="https://enterprisecampus.net/courses/tensorflow/04-03-activation-functions" class="px-2">EN</a></b>
	|
	<b class="px-2">ES</b>
	|
	<a href="https://campusempresa.cat/cursos/tensorflow/04-03-activation-functions" class="px-2">CA</a>
</p>
			<p class="mb-4 mt-0 mx-2  d-none d-md-block"><cite>Todo el conocimiento a tu alcance</cite></p>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objetivo" rel="nofollow">El Proyecto</a> | 
<a href="/acerca-de" rel="nofollow">Sobre nosotros</a> | 
<a href="/contribuir" rel="nofollow">Contribuir</a> | 
<a href="/donar" rel="nofollow">Donaciones</a> | 
<a href="/licencia" rel="nofollow">Licencia</a>
		</div>
	</div>
</div>
		<div class="top-bar container-fluid p-0">
	<div class="container-xxl p-0">
		<div class="row">
			<div class="col">
				<div class="d-flex justify-content-between">
					<div class="left">
						<a href="/" class="nav-link px-3" id="btnHome">
	<i class="bi bi-house-fill"></i>
	HOME
</a>

<a href="/mis-cursos" class="nav-link px-3 d-none" id="btnMyCourses">
	<i class="bi bi-rocket-takeoff-fill"></i>
	<i><b>Mis cursos</b></i>
</a>
<a href="/cursos-finalizados" class="nav-link px-3 d-none" id="trophy_button">
	<i class="bi bi-trophy-fill"></i>
	Finalizados             
</a>

					</div>
                    <div class="ms-auto right">
                        <a id="user_button" href="#" class="nav-link px-3" data-bs-toggle="modal" data-bs-target="#loginModal">
                            <i id="user_icon" class="bi"></i>                            
                        </a>
                    </div>					
				</div>
			</div>
		</div>
	</div>
</div>

		<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
										<div class="row py-1 m-0" id="buttonsModSection">
	<div class="col-6 p-0" data-mod="tensorflow">
		<a  href="#" class="text-secondary d-none" data-read-mod="tensorflow" data-read-unit="4-3" style="text-decoration:none;">
			<i class="bi bi-check-circle-fill"></i> 
			Marcar como leído
		</a>
		<a href="#" class="text-secondary d-none" data-unread-mod="tensorflow" data-unread-unit="4-3" style="text-decoration:none;">
			<i class="bi bi-x-circle-fill"></i>
			Marcar como no leído
		</a>
	</div>
	<div class="col-6 text-end p-0">
					<a href="./"  class="nav-link">
				<i class="bi bi-journal-text"></i>
				Contenido del curso
			</a>
			</div>
</div>						<div id="inner_content">
				<div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='04-02-creating-a-simple-neural-network' title="Creando una Red Neuronal Simple" class="py-2 px-3 btn btn-primary">
				&#x25C4; Anterior 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='04-02-creating-a-simple-neural-network' title="Creando una Red Neuronal Simple" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
					<h2 style="text-decoration:underline">Funciones de Activación</h2>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='04-04-loss-functions-and-optimizers' title="Funciones de Pérdida y Optimizadores" class="py-2 px-3 btn btn-primary"
				data-read-mod="tensorflow" data-read-unit="4-3">
				Siguiente &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='04-04-loss-functions-and-optimizers' title="Funciones de Pérdida y Optimizadores" class="py-2 px-3 btn btn-primary" 
				data-read-mod="tensorflow" data-read-unit="4-3">
				 &#x25BA;
			</a>
			</div>
</div>
<div class='content'><p>Las funciones de activación son un componente crucial en las redes neuronales, ya que introducen no linealidades en el modelo, permitiendo que la red aprenda y represente relaciones complejas en los datos. En esta sección, exploraremos las funciones de activación más comunes y cómo implementarlas en TensorFlow.</p>
</div><h1>¿Qué es una Función de Activación?</h1>
<div class='content'><p>Una función de activación es una función matemática que se aplica a la salida de una neurona en una red neuronal. Su propósito principal es introducir no linealidades en el modelo, lo que permite a la red neuronal aprender y modelar datos complejos.</p>
</div><h2>Propiedades Clave de las Funciones de Activación</h2>
<div class='content'><ol>
<li><strong>No Linealidad</strong>: Permite que la red neuronal aprenda relaciones complejas.</li>
<li><strong>Derivabilidad</strong>: La función debe ser diferenciable para permitir la retropropagación.</li>
<li><strong>Rango de Salida</strong>: El rango de valores que la función puede producir.</li>
<li><strong>Saturación</strong>: Algunas funciones pueden saturarse, es decir, sus derivadas se vuelven muy pequeñas para ciertos rangos de entrada.</li>
</ol>
</div><h1>Funciones de Activación Comunes</h1>
<div class='content'></div><h2><ol>
<li>Sigmoide</li>
</ol></h2>
<div class='content'><p>La función sigmoide es una de las funciones de activación más antiguas y se define como:</p>
<p>\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<ul>
<li><strong>Rango de Salida</strong>: (0, 1)</li>
<li><strong>Ventajas</strong>: Suaviza la salida y es útil para modelos probabilísticos.</li>
<li><strong>Desventajas</strong>: Puede sufrir de saturación y el problema del gradiente desvanecido.</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgRWplbXBsbyBkZSB1c28gZGUgbGEgZnVuY2nDs24gc2lnbW9pZGUgZW4gVGVuc29yRmxvdwp4ID0gdGYuY29uc3RhbnQoWy0xLjAsIDAuMCwgMS4wXSwgZHR5cGU9dGYuZmxvYXQzMikKc2lnbW9pZCA9IHRmLm5uLnNpZ21vaWQoeCkKcHJpbnQoc2lnbW9pZC5udW1weSgpKSAgIyBTYWxpZGE6IFswLjI2ODk0MTQzIDAuNSAwLjczMTA1ODZd"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Ejemplo de uso de la funci&oacute;n sigmoide en TensorFlow
x = tf.constant([-1.0, 0.0, 1.0], dtype=tf.float32)
sigmoid = tf.nn.sigmoid(x)
print(sigmoid.numpy())  # Salida: [0.26894143 0.5 0.7310586]</pre></div><div class='content'></div><h2><ol start="2">
<li>Tanh (Tangente Hiperbólica)</li>
</ol></h2>
<div class='content'><p>La función tanh es similar a la sigmoide pero escala la salida entre -1 y 1:</p>
<p>\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<ul>
<li><strong>Rango de Salida</strong>: (-1, 1)</li>
<li><strong>Ventajas</strong>: Centra los datos alrededor de cero, lo que puede acelerar el aprendizaje.</li>
<li><strong>Desventajas</strong>: También puede sufrir de saturación y el problema del gradiente desvanecido.</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFamVtcGxvIGRlIHVzbyBkZSBsYSBmdW5jacOzbiB0YW5oIGVuIFRlbnNvckZsb3cKdGFuaCA9IHRmLm5uLnRhbmgoeCkKcHJpbnQodGFuaC5udW1weSgpKSAgIyBTYWxpZGE6IFstMC43NjE1OTQyIDAuIDAuNzYxNTk0Ml0="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Ejemplo de uso de la funci&oacute;n tanh en TensorFlow
tanh = tf.nn.tanh(x)
print(tanh.numpy())  # Salida: [-0.7615942 0. 0.7615942]</pre></div><div class='content'></div><h2><ol start="3">
<li>ReLU (Rectified Linear Unit)</li>
</ol></h2>
<div class='content'><p>La función ReLU es actualmente la más popular debido a su simplicidad y efectividad:</p>
<p>\[ \text{ReLU}(x) = \max(0, x) \]</p>
<ul>
<li><strong>Rango de Salida</strong>: [0, ∞)</li>
<li><strong>Ventajas</strong>: No sufre de saturación en la región positiva y es computacionalmente eficiente.</li>
<li><strong>Desventajas</strong>: Puede causar el problema de &quot;neurona muerta&quot; donde algunas neuronas nunca se activan.</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFamVtcGxvIGRlIHVzbyBkZSBsYSBmdW5jacOzbiBSZUxVIGVuIFRlbnNvckZsb3cKcmVsdSA9IHRmLm5uLnJlbHUoeCkKcHJpbnQocmVsdS5udW1weSgpKSAgIyBTYWxpZGE6IFswLiAwLiAxLl0="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Ejemplo de uso de la funci&oacute;n ReLU en TensorFlow
relu = tf.nn.relu(x)
print(relu.numpy())  # Salida: [0. 0. 1.]</pre></div><div class='content'></div><h2><ol start="4">
<li>Leaky ReLU</li>
</ol></h2>
<div class='content'><p>Leaky ReLU es una variante de ReLU que permite un pequeño gradiente cuando la entrada es negativa:</p>
<p>\[ \text{Leaky ReLU}(x) = \max(0.01x, x) \]</p>
<ul>
<li><strong>Rango de Salida</strong>: (-∞, ∞)</li>
<li><strong>Ventajas</strong>: Mitiga el problema de &quot;neurona muerta&quot;.</li>
<li><strong>Desventajas</strong>: Introduce un hiperparámetro adicional (la pendiente negativa).</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFamVtcGxvIGRlIHVzbyBkZSBsYSBmdW5jacOzbiBMZWFreSBSZUxVIGVuIFRlbnNvckZsb3cKbGVha3lfcmVsdSA9IHRmLm5uLmxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSkKcHJpbnQobGVha3lfcmVsdS5udW1weSgpKSAgIyBTYWxpZGE6IFstMC4wMSAwLiAxLl0="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Ejemplo de uso de la funci&oacute;n Leaky ReLU en TensorFlow
leaky_relu = tf.nn.leaky_relu(x, alpha=0.01)
print(leaky_relu.numpy())  # Salida: [-0.01 0. 1.]</pre></div><div class='content'></div><h2><ol start="5">
<li>Softmax</li>
</ol></h2>
<div class='content'><p>La función softmax se utiliza principalmente en la capa de salida de una red neuronal para clasificación multiclase:</p>
<p>\[ \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]</p>
<ul>
<li><strong>Rango de Salida</strong>: (0, 1) y la suma de todas las salidas es 1.</li>
<li><strong>Ventajas</strong>: Proporciona una interpretación probabilística de las salidas.</li>
<li><strong>Desventajas</strong>: No es adecuada para capas ocultas debido a su complejidad computacional.</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFamVtcGxvIGRlIHVzbyBkZSBsYSBmdW5jacOzbiBzb2Z0bWF4IGVuIFRlbnNvckZsb3cKbG9naXRzID0gdGYuY29uc3RhbnQoWzEuMCwgMi4wLCAzLjBdLCBkdHlwZT10Zi5mbG9hdDMyKQpzb2Z0bWF4ID0gdGYubm4uc29mdG1heChsb2dpdHMpCnByaW50KHNvZnRtYXgubnVtcHkoKSkgICMgU2FsaWRhOiBbMC4wOTAwMzA1NyAwLjI0NDcyODQ4IDAuNjY1MjQwOTRd"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Ejemplo de uso de la funci&oacute;n softmax en TensorFlow
logits = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)
softmax = tf.nn.softmax(logits)
print(softmax.numpy())  # Salida: [0.09003057 0.24472848 0.66524094]</pre></div><div class='content'></div><h1>Ejercicio Práctico</h1>
<div class='content'></div><h2>Ejercicio 1: Implementar y Comparar Funciones de Activación</h2>
<div class='content'><ol>
<li><strong>Objetivo</strong>: Implementar las funciones de activación mencionadas y comparar sus salidas para un conjunto de datos de entrada.</li>
<li><strong>Instrucciones</strong>:
<ul>
<li>Crear un tensor de entrada con valores entre -10 y 10.</li>
<li>Aplicar cada una de las funciones de activación a este tensor.</li>
<li>Graficar las salidas para visualizar las diferencias.</li>
</ul>
</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKCiMgQ3JlYXIgdW4gdGVuc29yIGRlIGVudHJhZGEKeCA9IHRmLmxpbnNwYWNlKC0xMC4wLCAxMC4wLCAxMDApCgojIEFwbGljYXIgZnVuY2lvbmVzIGRlIGFjdGl2YWNpw7NuCnNpZ21vaWQgPSB0Zi5ubi5zaWdtb2lkKHgpCnRhbmggPSB0Zi5ubi50YW5oKHgpCnJlbHUgPSB0Zi5ubi5yZWx1KHgpCmxlYWt5X3JlbHUgPSB0Zi5ubi5sZWFreV9yZWx1KHgsIGFscGhhPTAuMDEpCnNvZnRtYXggPSB0Zi5ubi5zb2Z0bWF4KHRmLnN0YWNrKFt4LCB4XSwgYXhpcz0xKSwgYXhpcz0xKVs6LCAwXQoKIyBHcmFmaWNhciBsYXMgc2FsaWRhcwpwbHQuZmlndXJlKGZpZ3NpemU9KDEyLCA4KSkKcGx0LnBsb3QoeCwgc2lnbW9pZCwgbGFiZWw9J1NpZ21vaWQnKQpwbHQucGxvdCh4LCB0YW5oLCBsYWJlbD0nVGFuaCcpCnBsdC5wbG90KHgsIHJlbHUsIGxhYmVsPSdSZUxVJykKcGx0LnBsb3QoeCwgbGVha3lfcmVsdSwgbGFiZWw9J0xlYWt5IFJlTFUnKQpwbHQucGxvdCh4LCBzb2Z0bWF4LCBsYWJlbD0nU29mdG1heCcpCnBsdC5sZWdlbmQoKQpwbHQudGl0bGUoJ0NvbXBhcmFjacOzbiBkZSBGdW5jaW9uZXMgZGUgQWN0aXZhY2nDs24nKQpwbHQueGxhYmVsKCdFbnRyYWRhJykKcGx0LnlsYWJlbCgnU2FsaWRhJykKcGx0LmdyaWQoVHJ1ZSkKcGx0LnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import matplotlib.pyplot as plt

# Crear un tensor de entrada
x = tf.linspace(-10.0, 10.0, 100)

# Aplicar funciones de activaci&oacute;n
sigmoid = tf.nn.sigmoid(x)
tanh = tf.nn.tanh(x)
relu = tf.nn.relu(x)
leaky_relu = tf.nn.leaky_relu(x, alpha=0.01)
softmax = tf.nn.softmax(tf.stack([x, x], axis=1), axis=1)[:, 0]

# Graficar las salidas
plt.figure(figsize=(12, 8))
plt.plot(x, sigmoid, label='Sigmoid')
plt.plot(x, tanh, label='Tanh')
plt.plot(x, relu, label='ReLU')
plt.plot(x, leaky_relu, label='Leaky ReLU')
plt.plot(x, softmax, label='Softmax')
plt.legend()
plt.title('Comparaci&oacute;n de Funciones de Activaci&oacute;n')
plt.xlabel('Entrada')
plt.ylabel('Salida')
plt.grid(True)
plt.show()</pre></div><div class='content'></div><h2>Solución del Ejercicio</h2>
<div class='content'><p>El código anterior crea un tensor de entrada y aplica las funciones de activación mencionadas. Luego, grafica las salidas para comparar visualmente cómo cada función transforma los datos de entrada.</p>
</div><h1>Conclusión</h1>
<div class='content'><p>En esta sección, hemos explorado varias funciones de activación comunes y sus implementaciones en TensorFlow. Cada función tiene sus propias ventajas y desventajas, y la elección de la función de activación adecuada puede tener un impacto significativo en el rendimiento de la red neuronal. En el próximo módulo, aplicaremos estos conocimientos para construir redes neuronales más complejas y eficientes.</p>
</div><div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='04-02-creating-a-simple-neural-network' title="Creando una Red Neuronal Simple" class="py-2 px-3 btn btn-primary">
				&#x25C4; Anterior 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='04-02-creating-a-simple-neural-network' title="Creando una Red Neuronal Simple" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='04-04-loss-functions-and-optimizers' title="Funciones de Pérdida y Optimizadores" class="py-2 px-3 btn btn-primary"
				data-read-mod="tensorflow" data-read-unit="4-3">
				Siguiente &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='04-04-loss-functions-and-optimizers' title="Funciones de Pérdida y Optimizadores" class="py-2 px-3 btn btn-primary" 
				data-read-mod="tensorflow" data-read-unit="4-3">
				 &#x25BA;
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
						
	<div class="container mt-2 d-none d-md-block index">
		<h1>Curso de TensorFlow</h1>
<h2>Módulo 1: Introducción a TensorFlow</h2>
<ul>
<li><a href="01-01-what-is-tensorflow">¿Qué es TensorFlow?</a></li>
<li><a href="01-02-setting-up-tensorflow">Configuración de TensorFlow</a></li>
<li><a href="01-03-basic-tensorflow-concepts">Conceptos Básicos de TensorFlow</a></li>
<li><a href="01-04-tensorflow-hello-world">Hola Mundo en TensorFlow</a></li>
</ul>
<h2>Módulo 2: Fundamentos de TensorFlow</h2>
<ul>
<li><a href="02-01-tensors-and-operations">Tensores y Operaciones</a></li>
<li><a href="02-02-variables-and-constants">Variables y Constantes</a></li>
<li><a href="02-03-tensorflow-graphs">Grafos de TensorFlow</a></li>
<li><a href="02-04-eager-execution">Ejecución Eager</a></li>
</ul>
<h2>Módulo 3: Manejo de Datos en TensorFlow</h2>
<ul>
<li><a href="03-01-loading-data">Cargando Datos</a></li>
<li><a href="03-02-data-pipelines-with-tf-data">Pipelines de Datos con tf.data</a></li>
<li><a href="03-03-data-augmentation">Aumento de Datos</a></li>
<li><a href="03-04-working-with-datasets">Trabajando con Conjuntos de Datos</a></li>
</ul>
<h2>Módulo 4: Construcción de Redes Neuronales</h2>
<ul>
<li><a href="04-01-introduction-to-neural-networks">Introducción a Redes Neuronales</a></li>
<li><a href="04-02-creating-a-simple-neural-network">Creando una Red Neuronal Simple</a></li>
<li><a href="04-03-activation-functions">Funciones de Activación</a></li>
<li><a href="04-04-loss-functions-and-optimizers">Funciones de Pérdida y Optimizadores</a></li>
</ul>
<h2>Módulo 5: Redes Neuronales Convolucionales (CNNs)</h2>
<ul>
<li><a href="05-01-introduction-to-cnns">Introducción a CNNs</a></li>
<li><a href="05-02-building-a-cnn">Construyendo una CNN</a></li>
<li><a href="05-03-pooling-layers">Capas de Pooling</a></li>
<li><a href="05-04-advanced-cnn-architectures">Arquitecturas Avanzadas de CNN</a></li>
</ul>
<h2>Módulo 6: Redes Neuronales Recurrentes (RNNs)</h2>
<ul>
<li><a href="06-01-introduction-to-rnns">Introducción a RNNs</a></li>
<li><a href="06-02-building-an-rnn">Construyendo una RNN</a></li>
<li><a href="06-03-long-short-term-memory">Memoria a Largo Plazo (LSTM)</a></li>
<li><a href="06-04-gated-recurrent-units">Unidades Recurrentes Gated (GRUs)</a></li>
</ul>
<h2>Módulo 7: Técnicas Avanzadas de TensorFlow</h2>
<ul>
<li><a href="07-01-custom-layers-and-models">Capas y Modelos Personalizados</a></li>
<li><a href="07-02-tensorflow-hub">TensorFlow Hub</a></li>
<li><a href="07-03-transfer-learning">Aprendizaje por Transferencia</a></li>
<li><a href="07-04-hyperparameter-tuning">Ajuste de Hiperparámetros</a></li>
</ul>
<h2>Módulo 8: TensorFlow para Producción</h2>
<ul>
<li><a href="08-01-model-saving-and-loading">Guardado y Carga de Modelos</a></li>
<li><a href="08-02-tensorflow-serving">TensorFlow Serving</a></li>
<li><a href="08-03-deploying-models">Despliegue de Modelos</a></li>
<li><a href="08-04-monitoring-and-maintenance">Monitoreo y Mantenimiento</a></li>
</ul>
<h2>Módulo 9: TensorFlow Extendido (TFX)</h2>
<ul>
<li><a href="09-01-introduction-to-tfx">Introducción a TFX</a></li>
<li><a href="09-02-data-validation">Validación de Datos</a></li>
<li><a href="09-03-transforming-data">Transformación de Datos</a></li>
<li><a href="09-04-model-analysis">Análisis de Modelos</a></li>
</ul>
<h2>Módulo 10: Temas Especiales</h2>
<ul>
<li><a href="10-01-tensorflow-lite">TensorFlow Lite</a></li>
<li><a href="10-02-tensorflow-js">TensorFlow.js</a></li>
<li><a href="10-03-tensorflow-federated">TensorFlow Federated</a></li>
<li><a href="10-04-tensorflow-quantum">TensorFlow Quantum</a></li>
</ul>

	</div>










		</div>
	</div>
</div>		
<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objetivo" rel="nofollow">El Proyecto</a> | 
<a href="/acerca-de" rel="nofollow">Sobre nosotros</a> | 
<a href="/contribuir" rel="nofollow">Contribuir</a> | 
<a href="/donar" rel="nofollow">Donaciones</a> | 
<a href="/licencia" rel="nofollow">Licencia</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Usamos cookies para mejorar tu experiencia de uso y ofrecer contenidos adaptados a tus intereses.
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

		<div class="modal fade" id="loginModal" tabindex="-1" aria-labelledby="loginModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="loginModalLabel">Usuario no autenticado</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
            	<div id="modal-body-main"></div>
            </div>
        </div>
    </div>
</div>	</div>    
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
</body>
</html>
