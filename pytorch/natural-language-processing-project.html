<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proyecto de Procesamiento de Lenguaje Natural</title>

    <link rel="alternate" href="https://campusempresa.com/pytorch/natural-language-processing-project" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/pytorch/natural-language-processing-project" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/pytorch/natural-language-processing-project" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/pytorch/natural-language-processing-project" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/pytorch/natural-language-processing-project" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='image-classification-project'>&#x25C4;Proyecto de Clasificación de Imágenes</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Proyecto de Procesamiento de Lenguaje Natural</a>
	</div>
	<div class='col-4 text-end'>
					<a href='time-series-forecasting-project'>Proyecto de Pronóstico de Series Temporales &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introducción al Procesamiento de Lenguaje Natural (PLN)</h1>
<div class='content'><p>El Procesamiento de Lenguaje Natural (PLN) es un campo de la inteligencia artificial que se centra en la interacción entre computadoras y humanos a través del lenguaje natural. El objetivo es permitir que las computadoras comprendan, interpreten y generen el lenguaje humano de una manera que sea tanto significativa como útil.</p>
</div><h2>Conceptos Clave en PLN</h2>
<div class='content'><ul>
<li><strong>Tokenización</strong>: Dividir el texto en palabras o frases individuales.</li>
<li><strong>Palabras Vacías</strong>: Palabras comunes que generalmente se filtran antes del procesamiento.</li>
<li><strong>Stemming y Lematización</strong>: Reducir las palabras a su forma base o raíz.</li>
<li><strong>Bolsa de Palabras</strong>: Representar datos de texto como una colección de frecuencias de palabras.</li>
<li><strong>TF-IDF</strong>: Frecuencia de Término-Inversa Frecuencia de Documento, una medida estadística utilizada para evaluar la importancia de una palabra en un documento.</li>
<li><strong>Embeddings de Palabras</strong>: Representar palabras en un espacio vectorial continuo donde las palabras semánticamente similares están más cerca entre sí.</li>
</ul>
</div><h1>Configuración del Entorno</h1>
<div class='content'><p>Antes de sumergirnos en el proyecto, asegúrate de tener instaladas las bibliotecas necesarias. Usaremos PyTorch para construir y entrenar nuestros modelos.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgdG9yY2ggdG9yY2h2aXNpb24gdG9yY2hhdWRpbwpwaXAgaW5zdGFsbCBubHRrCnBpcCBpbnN0YWxsIHNrbGVhcm4="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install torch torchvision torchaudio
pip install nltk
pip install sklearn</pre></div><div class='content'></div><h1>Preprocesamiento de Datos</h1>
<h2>Tokenización y Eliminación de Palabras Vacías</h2>
<div class='content'><p>La tokenización es el proceso de descomponer el texto en palabras o tokens individuales. Las palabras vacías son palabras comunes que generalmente se eliminan para centrarse en las palabras más significativas.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG5sdGsKZnJvbSBubHRrLnRva2VuaXplIGltcG9ydCB3b3JkX3Rva2VuaXplCmZyb20gbmx0ay5jb3JwdXMgaW1wb3J0IHN0b3B3b3JkcwoKbmx0ay5kb3dubG9hZCgncHVua3QnKQpubHRrLmRvd25sb2FkKCdzdG9wd29yZHMnKQoKdGV4dCA9ICJOYXR1cmFsIExhbmd1YWdlIFByb2Nlc3Npbmcgd2l0aCBQeVRvcmNoIGlzIGZ1biBhbmQgZWR1Y2F0aW9uYWwuIgp0b2tlbnMgPSB3b3JkX3Rva2VuaXplKHRleHQpCnN0b3Bfd29yZHMgPSBzZXQoc3RvcHdvcmRzLndvcmRzKCdlbmdsaXNoJykpCmZpbHRlcmVkX3Rva2VucyA9IFt3b3JkIGZvciB3b3JkIGluIHRva2VucyBpZiB3b3JkLmxvd2VyKCkgbm90IGluIHN0b3Bfd29yZHNdCgpwcmludChmaWx0ZXJlZF90b2tlbnMp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

nltk.download('punkt')
nltk.download('stopwords')

text = &quot;Natural Language Processing with PyTorch is fun and educational.&quot;
tokens = word_tokenize(text)
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

print(filtered_tokens)</pre></div><div class='content'></div><h2>Stemming y Lematización</h2>
<div class='content'><p>El stemming y la lematización son técnicas utilizadas para reducir las palabras a su forma base o raíz.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBubHRrLnN0ZW0gaW1wb3J0IFBvcnRlclN0ZW1tZXIsIFdvcmROZXRMZW1tYXRpemVyCgpubHRrLmRvd25sb2FkKCd3b3JkbmV0JykKCnN0ZW1tZXIgPSBQb3J0ZXJTdGVtbWVyKCkKbGVtbWF0aXplciA9IFdvcmROZXRMZW1tYXRpemVyKCkKCnN0ZW1tZWRfd29yZHMgPSBbc3RlbW1lci5zdGVtKHdvcmQpIGZvciB3b3JkIGluIGZpbHRlcmVkX3Rva2Vuc10KbGVtbWF0aXplZF93b3JkcyA9IFtsZW1tYXRpemVyLmxlbW1hdGl6ZSh3b3JkKSBmb3Igd29yZCBpbiBmaWx0ZXJlZF90b2tlbnNdCgpwcmludCgiU3RlbW1lZCBXb3JkczoiLCBzdGVtbWVkX3dvcmRzKQpwcmludCgiTGVtbWF0aXplZCBXb3JkczoiLCBsZW1tYXRpemVkX3dvcmRzKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from nltk.stem import PorterStemmer, WordNetLemmatizer

nltk.download('wordnet')

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

stemmed_words = [stemmer.stem(word) for word in filtered_tokens]
lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]

print(&quot;Stemmed Words:&quot;, stemmed_words)
print(&quot;Lemmatized Words:&quot;, lemmatized_words)</pre></div><div class='content'></div><h1>Extracción de Características</h1>
<h2>Bolsa de Palabras</h2>
<div class='content'><p>El modelo de Bolsa de Palabras representa datos de texto como una colección de frecuencias de palabras.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmZlYXR1cmVfZXh0cmFjdGlvbi50ZXh0IGltcG9ydCBDb3VudFZlY3Rvcml6ZXIKCmNvcnB1cyA9IFsKICAgICdOYXR1cmFsIExhbmd1YWdlIFByb2Nlc3Npbmcgd2l0aCBQeVRvcmNoIGlzIGZ1biBhbmQgZWR1Y2F0aW9uYWwuJywKICAgICdQeVRvcmNoIG1ha2VzIGJ1aWxkaW5nIG5ldXJhbCBuZXR3b3JrcyBlYXN5LicsCiAgICAnTkxQIGlzIGEgZmFzY2luYXRpbmcgZmllbGQgb2Ygc3R1ZHkuJwpdCgp2ZWN0b3JpemVyID0gQ291bnRWZWN0b3JpemVyKCkKWCA9IHZlY3Rvcml6ZXIuZml0X3RyYW5zZm9ybShjb3JwdXMpCgpwcmludCh2ZWN0b3JpemVyLmdldF9mZWF0dXJlX25hbWVzX291dCgpKQpwcmludChYLnRvYXJyYXkoKSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    'Natural Language Processing with PyTorch is fun and educational.',
    'PyTorch makes building neural networks easy.',
    'NLP is a fascinating field of study.'
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)

print(vectorizer.get_feature_names_out())
print(X.toarray())</pre></div><div class='content'></div><h2>TF-IDF</h2>
<div class='content'><p>TF-IDF es una medida estadística utilizada para evaluar la importancia de una palabra en un documento.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmZlYXR1cmVfZXh0cmFjdGlvbi50ZXh0IGltcG9ydCBUZmlkZlZlY3Rvcml6ZXIKCnRmaWRmX3ZlY3Rvcml6ZXIgPSBUZmlkZlZlY3Rvcml6ZXIoKQpYX3RmaWRmID0gdGZpZGZfdmVjdG9yaXplci5maXRfdHJhbnNmb3JtKGNvcnB1cykKCnByaW50KHRmaWRmX3ZlY3Rvcml6ZXIuZ2V0X2ZlYXR1cmVfbmFtZXNfb3V0KCkpCnByaW50KFhfdGZpZGYudG9hcnJheSgpKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform(corpus)

print(tfidf_vectorizer.get_feature_names_out())
print(X_tfidf.toarray())</pre></div><div class='content'></div><h1>Construcción de un Modelo de PLN con PyTorch</h1>
<h2>Preparación del Conjunto de Datos</h2>
<div class='content'><p>Para este proyecto, usaremos un conjunto de datos simple para la clasificación de texto.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmZyb20gdG9yY2gudXRpbHMuZGF0YSBpbXBvcnQgRGF0YXNldCwgRGF0YUxvYWRlcgoKY2xhc3MgVGV4dERhdGFzZXQoRGF0YXNldCk6CiAgICBkZWYgX19pbml0X18oc2VsZiwgdGV4dHMsIGxhYmVscywgdmVjdG9yaXplcik6CiAgICAgICAgc2VsZi50ZXh0cyA9IHRleHRzCiAgICAgICAgc2VsZi5sYWJlbHMgPSBsYWJlbHMKICAgICAgICBzZWxmLnZlY3Rvcml6ZXIgPSB2ZWN0b3JpemVyCiAgICAgICAgc2VsZi52ZWN0b3JpemVkX3RleHRzID0gc2VsZi52ZWN0b3JpemVyLmZpdF90cmFuc2Zvcm0oc2VsZi50ZXh0cykudG9hcnJheSgpCiAgICAgICAgCiAgICBkZWYgX19sZW5fXyhzZWxmKToKICAgICAgICByZXR1cm4gbGVuKHNlbGYudGV4dHMpCiAgICAKICAgIGRlZiBfX2dldGl0ZW1fXyhzZWxmLCBpZHgpOgogICAgICAgIHRleHRfdmVjdG9yID0gdG9yY2gudGVuc29yKHNlbGYudmVjdG9yaXplZF90ZXh0c1tpZHhdLCBkdHlwZT10b3JjaC5mbG9hdDMyKQogICAgICAgIGxhYmVsID0gdG9yY2gudGVuc29yKHNlbGYubGFiZWxzW2lkeF0sIGR0eXBlPXRvcmNoLmxvbmcpCiAgICAgICAgcmV0dXJuIHRleHRfdmVjdG9yLCBsYWJlbAoKdGV4dHMgPSBbCiAgICAnTmF0dXJhbCBMYW5ndWFnZSBQcm9jZXNzaW5nIHdpdGggUHlUb3JjaCBpcyBmdW4gYW5kIGVkdWNhdGlvbmFsLicsCiAgICAnUHlUb3JjaCBtYWtlcyBidWlsZGluZyBuZXVyYWwgbmV0d29ya3MgZWFzeS4nLAogICAgJ05MUCBpcyBhIGZhc2NpbmF0aW5nIGZpZWxkIG9mIHN0dWR5LicKXQpsYWJlbHMgPSBbMCwgMSwgMF0gICMgRXRpcXVldGFzIGRlIGVqZW1wbG8KCmRhdGFzZXQgPSBUZXh0RGF0YXNldCh0ZXh0cywgbGFiZWxzLCB2ZWN0b3JpemVyKQpkYXRhbG9hZGVyID0gRGF0YUxvYWRlcihkYXRhc2V0LCBiYXRjaF9zaXplPTIsIHNodWZmbGU9VHJ1ZSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
from torch.utils.data import Dataset, DataLoader

class TextDataset(Dataset):
    def __init__(self, texts, labels, vectorizer):
        self.texts = texts
        self.labels = labels
        self.vectorizer = vectorizer
        self.vectorized_texts = self.vectorizer.fit_transform(self.texts).toarray()
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text_vector = torch.tensor(self.vectorized_texts[idx], dtype=torch.float32)
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        return text_vector, label

texts = [
    'Natural Language Processing with PyTorch is fun and educational.',
    'PyTorch makes building neural networks easy.',
    'NLP is a fascinating field of study.'
]
labels = [0, 1, 0]  # Etiquetas de ejemplo

dataset = TextDataset(texts, labels, vectorizer)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)</pre></div><div class='content'></div><h2>Definición del Modelo</h2>
<div class='content'><p>Definiremos una red neuronal feedforward simple para la clasificación de texto.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoLm5uIGFzIG5uCgpjbGFzcyBUZXh0Q2xhc3NpZmllcihubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIGlucHV0X2RpbSwgaGlkZGVuX2RpbSwgb3V0cHV0X2RpbSk6CiAgICAgICAgc3VwZXIoVGV4dENsYXNzaWZpZXIsIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmZjMSA9IG5uLkxpbmVhcihpbnB1dF9kaW0sIGhpZGRlbl9kaW0pCiAgICAgICAgc2VsZi5yZWx1ID0gbm4uUmVMVSgpCiAgICAgICAgc2VsZi5mYzIgPSBubi5MaW5lYXIoaGlkZGVuX2RpbSwgb3V0cHV0X2RpbSkKICAgIAogICAgZGVmIGZvcndhcmQoc2VsZiwgeCk6CiAgICAgICAgb3V0ID0gc2VsZi5mYzEoeCkKICAgICAgICBvdXQgPSBzZWxmLnJlbHUob3V0KQogICAgICAgIG91dCA9IHNlbGYuZmMyKG91dCkKICAgICAgICByZXR1cm4gb3V0CgppbnB1dF9kaW0gPSBsZW4odmVjdG9yaXplci5nZXRfZmVhdHVyZV9uYW1lc19vdXQoKSkKaGlkZGVuX2RpbSA9IDEwCm91dHB1dF9kaW0gPSAyICAjIE7Dum1lcm8gZGUgY2xhc2VzCgptb2RlbCA9IFRleHRDbGFzc2lmaWVyKGlucHV0X2RpbSwgaGlkZGVuX2RpbSwgb3V0cHV0X2RpbSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch.nn as nn

class TextClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(TextClassifier, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

input_dim = len(vectorizer.get_feature_names_out())
hidden_dim = 10
output_dim = 2  # N&uacute;mero de clases

model = TextClassifier(input_dim, hidden_dim, output_dim)</pre></div><div class='content'></div><h2>Entrenamiento del Modelo</h2>
<div class='content'><p>Entrenaremos el modelo usando un bucle de entrenamiento simple.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoLm9wdGltIGFzIG9wdGltCgpjcml0ZXJpb24gPSBubi5Dcm9zc0VudHJvcHlMb3NzKCkKb3B0aW1pemVyID0gb3B0aW0uQWRhbShtb2RlbC5wYXJhbWV0ZXJzKCksIGxyPTAuMDAxKQoKbnVtX2Vwb2NocyA9IDUKCmZvciBlcG9jaCBpbiByYW5nZShudW1fZXBvY2hzKToKICAgIGZvciB0ZXh0cywgbGFiZWxzIGluIGRhdGFsb2FkZXI6CiAgICAgICAgb3V0cHV0cyA9IG1vZGVsKHRleHRzKQogICAgICAgIGxvc3MgPSBjcml0ZXJpb24ob3V0cHV0cywgbGFiZWxzKQogICAgICAgIAogICAgICAgIG9wdGltaXplci56ZXJvX2dyYWQoKQogICAgICAgIGxvc3MuYmFja3dhcmQoKQogICAgICAgIG9wdGltaXplci5zdGVwKCkKICAgIAogICAgcHJpbnQoZidFcG9jaCBbe2Vwb2NoKzF9L3tudW1fZXBvY2hzfV0sIExvc3M6IHtsb3NzLml0ZW0oKTouNGZ9Jyk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

num_epochs = 5

for epoch in range(num_epochs):
    for texts, labels in dataloader:
        outputs = model(texts)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')</pre></div><div class='content'></div><h1>Conclusión</h1>
<div class='content'><p>En este tema, cubrimos los conceptos básicos del Procesamiento de Lenguaje Natural y cómo implementar un proyecto de PLN usando PyTorch. Comenzamos con técnicas de preprocesamiento de datos como tokenización, eliminación de palabras vacías, stemming y lematización. Luego, pasamos a la extracción de características utilizando Bolsa de Palabras y TF-IDF. Finalmente, construimos y entrenamos un modelo de clasificación de texto simple usando PyTorch.</p>
<p>Siguiendo estos pasos, ahora deberías tener una comprensión fundamental de cómo abordar proyectos de PLN e implementarlos usando PyTorch. Continúa explorando técnicas y modelos más avanzados para mejorar aún más tus habilidades en PLN.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='image-classification-project'>&#x25C4;Proyecto de Clasificación de Imágenes</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Proyecto de Procesamiento de Lenguaje Natural</a>
	</div>
	<div class='col-4 text-end'>
					<a href='time-series-forecasting-project'>Proyecto de Pronóstico de Series Temporales &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
