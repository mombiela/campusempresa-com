<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extender Autograd</title>

    <link rel="alternate" href="https://campusempresa.com/pytorch/extending-autograd" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/pytorch/extending-autograd" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/pytorch/extending-autograd" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/pytorch/extending-autograd" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/pytorch/extending-autograd" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='custom-layers-and-modules'>&#x25C4;Capas y Módulos Personalizados</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Extender Autograd</a>
	</div>
	<div class='col-4 text-end'>
					<a href='pytorch-lightning'>PyTorch Lightning &#x25BA;</a>
			</div>
</div>
<div class='content'><p>En esta sección, exploraremos cómo extender la funcionalidad de Autograd de PyTorch. Autograd es el motor de diferenciación automática de PyTorch que impulsa el entrenamiento de redes neuronales. Al extender Autograd, puedes definir operaciones y gradientes personalizados, lo cual puede ser útil para implementar algoritmos novedosos u optimizar partes específicas de tu modelo.</p>
</div><h1>Conceptos Clave</h1>
<div class='content'><ul>
<li><strong>Autograd</strong>: El motor de diferenciación automática de PyTorch.</li>
<li><strong>Funciones Autograd Personalizadas</strong>: Operaciones definidas por el usuario con pases hacia adelante y hacia atrás personalizados.</li>
<li><strong>Tensores</strong>: La estructura de datos fundamental en PyTorch, que soporta la diferenciación automática.</li>
</ul>
</div><h1>Funciones Autograd Personalizadas</h1>
<div class='content'><p>Para extender Autograd, necesitas crear funciones autograd personalizadas. Estas funciones te permiten definir tanto los pases hacia adelante como hacia atrás de una operación. Esto es particularmente útil cuando necesitas más control sobre el cálculo del gradiente.</p>
</div><h2>Creando una Función Autograd Personalizada</h2>
<div class='content'><p>Para crear una función autograd personalizada, necesitas subclasificar <code>torch.autograd.Function</code> e implementar dos métodos estáticos: <code>forward</code> y <code>backward</code>.</p>
<h4>Ejemplo: Función Cuadrada Personalizada</h4>
<p>Vamos a crear una función personalizada que calcule el cuadrado de su entrada y su gradiente.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgpjbGFzcyBTcXVhcmVGdW5jdGlvbih0b3JjaC5hdXRvZ3JhZC5GdW5jdGlvbik6CiAgICBAc3RhdGljbWV0aG9kCiAgICBkZWYgZm9yd2FyZChjdHgsIGlucHV0KToKICAgICAgICAjIEd1YXJkYXIgY29udGV4dG8gcGFyYSBlbCBwYXNlIGhhY2lhIGF0csOhcwogICAgICAgIGN0eC5zYXZlX2Zvcl9iYWNrd2FyZChpbnB1dCkKICAgICAgICByZXR1cm4gaW5wdXQgKiogMgoKICAgIEBzdGF0aWNtZXRob2QKICAgIGRlZiBiYWNrd2FyZChjdHgsIGdyYWRfb3V0cHV0KToKICAgICAgICAjIFJlY3VwZXJhciB0ZW5zb3IgZ3VhcmRhZG8KICAgICAgICBpbnB1dCwgPSBjdHguc2F2ZWRfdGVuc29ycwogICAgICAgICMgQ2FsY3VsYXIgZ3JhZGllbnRlCiAgICAgICAgZ3JhZF9pbnB1dCA9IGdyYWRfb3V0cHV0ICogMiAqIGlucHV0CiAgICAgICAgcmV0dXJuIGdyYWRfaW5wdXQKCiMgVXNvCnggPSB0b3JjaC50ZW5zb3IoWzIuMCwgMy4wXSwgcmVxdWlyZXNfZ3JhZD1UcnVlKQpzcXVhcmUgPSBTcXVhcmVGdW5jdGlvbi5hcHBseQp5ID0gc3F1YXJlKHgpCnkuYmFja3dhcmQodG9yY2gudGVuc29yKFsxLjAsIDEuMF0pKQoKcHJpbnQoeC5ncmFkKSAgIyBTYWxpZGE6IHRlbnNvcihbNC4sIDYuXSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

class SquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        # Guardar contexto para el pase hacia atr&aacute;s
        ctx.save_for_backward(input)
        return input ** 2

    @staticmethod
    def backward(ctx, grad_output):
        # Recuperar tensor guardado
        input, = ctx.saved_tensors
        # Calcular gradiente
        grad_input = grad_output * 2 * input
        return grad_input

# Uso
x = torch.tensor([2.0, 3.0], requires_grad=True)
square = SquareFunction.apply
y = square(x)
y.backward(torch.tensor([1.0, 1.0]))

print(x.grad)  # Salida: tensor([4., 6.])</pre></div><div class='content'><p>En este ejemplo:</p>
<ul>
<li>El método <code>forward</code> calcula el cuadrado del tensor de entrada y lo guarda para el pase hacia atrás.</li>
<li>El método <code>backward</code> recupera el tensor de entrada guardado y calcula el gradiente.</li>
</ul>
</div><h2>Usando Funciones Autograd Personalizadas</h2>
<div class='content'><p>Puedes usar funciones autograd personalizadas como cualquier otra operación de PyTorch. La clave es llamar al método <code>apply</code> de tu función personalizada.</p>
<h4>Ejemplo: Función ReLU Personalizada</h4>
<p>Vamos a crear una función ReLU (Unidad Lineal Rectificada) personalizada.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgUmVMVUZ1bmN0aW9uKHRvcmNoLmF1dG9ncmFkLkZ1bmN0aW9uKToKICAgIEBzdGF0aWNtZXRob2QKICAgIGRlZiBmb3J3YXJkKGN0eCwgaW5wdXQpOgogICAgICAgIGN0eC5zYXZlX2Zvcl9iYWNrd2FyZChpbnB1dCkKICAgICAgICByZXR1cm4gaW5wdXQuY2xhbXAobWluPTApCgogICAgQHN0YXRpY21ldGhvZAogICAgZGVmIGJhY2t3YXJkKGN0eCwgZ3JhZF9vdXRwdXQpOgogICAgICAgIGlucHV0LCA9IGN0eC5zYXZlZF90ZW5zb3JzCiAgICAgICAgZ3JhZF9pbnB1dCA9IGdyYWRfb3V0cHV0LmNsb25lKCkKICAgICAgICBncmFkX2lucHV0W2lucHV0IDwgMF0gPSAwCiAgICAgICAgcmV0dXJuIGdyYWRfaW5wdXQKCiMgVXNvCnggPSB0b3JjaC50ZW5zb3IoWy0xLjAsIDIuMCwgLTMuMCwgNC4wXSwgcmVxdWlyZXNfZ3JhZD1UcnVlKQpyZWx1ID0gUmVMVUZ1bmN0aW9uLmFwcGx5CnkgPSByZWx1KHgpCnkuYmFja3dhcmQodG9yY2gudGVuc29yKFsxLjAsIDEuMCwgMS4wLCAxLjBdKSkKCnByaW50KHguZ3JhZCkgICMgU2FsaWRhOiB0ZW5zb3IoWzAuLCAxLiwgMC4sIDEuXSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class ReLUFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input &lt; 0] = 0
        return grad_input

# Uso
x = torch.tensor([-1.0, 2.0, -3.0, 4.0], requires_grad=True)
relu = ReLUFunction.apply
y = relu(x)
y.backward(torch.tensor([1.0, 1.0, 1.0, 1.0]))

print(x.grad)  # Salida: tensor([0., 1., 0., 1.])</pre></div><div class='content'><p>En este ejemplo:</p>
<ul>
<li>El método <code>forward</code> aplica la operación ReLU.</li>
<li>El método <code>backward</code> calcula el gradiente, estableciéndolo en cero donde la entrada era negativa.</li>
</ul>
</div><h1>Conclusión</h1>
<div class='content'><p>Extender Autograd en PyTorch te permite definir operaciones personalizadas con pases hacia adelante y hacia atrás específicos. Esto puede ser particularmente útil para implementar nuevos algoritmos u optimizar ciertas partes de tu modelo. Al subclasificar <code>torch.autograd.Function</code> e implementar los métodos <code>forward</code> y <code>backward</code>, obtienes control total sobre el cálculo y el flujo de gradientes, permitiendo mayor flexibilidad y personalización en tus modelos de aprendizaje profundo.</p>
<p>En las próximas secciones, profundizaremos en temas y aplicaciones más avanzadas de funciones autograd personalizadas, incluyendo su uso en arquitecturas complejas de redes neuronales y rutinas de optimización.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='custom-layers-and-modules'>&#x25C4;Capas y Módulos Personalizados</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Extender Autograd</a>
	</div>
	<div class='col-4 text-end'>
					<a href='pytorch-lightning'>PyTorch Lightning &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
