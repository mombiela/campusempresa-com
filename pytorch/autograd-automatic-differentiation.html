<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autograd: Diferenciación Automática</title>

    <link rel="alternate" href="https://campusempresa.com/pytorch/autograd-automatic-differentiation" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/pytorch/autograd-automatic-differentiation" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/pytorch/autograd-automatic-differentiation" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/pytorch/autograd-automatic-differentiation" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/pytorch/autograd-automatic-differentiation" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='basic-tensor-operations'>&#x25C4;Operaciones Básicas con Tensores</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Autograd: Diferenciación Automática</a>
	</div>
	<div class='col-4 text-end'>
					<a href='introduction-to-neural-networks'>Introducción a las Redes Neuronales &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introducción a Autograd</h1>
<div class='content'><p>Autograd es el motor de diferenciación automática de PyTorch que impulsa el entrenamiento de redes neuronales. Proporciona la capacidad de calcular automáticamente los gradientes, que son esenciales para optimizar las redes neuronales.</p>
</div><h2>Conceptos Clave</h2>
<div class='content'><ul>
<li><strong>Tensores</strong>: Los bloques de construcción fundamentales en PyTorch, similares a los arrays en NumPy.</li>
<li><strong>Gradientes</strong>: Derivadas de tensores que se utilizan para actualizar los parámetros del modelo durante el entrenamiento.</li>
<li><strong>Grafo Computacional</strong>: Un grafo dinámico que registra las operaciones realizadas en los tensores para calcular los gradientes.</li>
</ul>
</div><h1>Configuración de Autograd</h1>
<div class='content'><p>Para usar Autograd, necesitas entender cómo crear tensores y habilitar el cálculo de gradientes.</p>
</div><h2>Creación de Tensores con Gradientes</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIENyZWFyIHVuIHRlbnNvciB5IGhhYmlsaXRhciBlbCBjw6FsY3VsbyBkZSBncmFkaWVudGVzCnggPSB0b3JjaC50ZW5zb3IoWzEuMCwgMi4wLCAzLjBdLCByZXF1aXJlc19ncmFkPVRydWUpCnByaW50KHgp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Crear un tensor y habilitar el c&aacute;lculo de gradientes
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
print(x)</pre></div><div class='content'><ul>
<li><code>requires_grad=True</code>: Esta bandera le dice a PyTorch que rastree todas las operaciones en este tensor.</li>
</ul>
</div><h2>Cálculo Básico de Gradientes</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBEZWZpbmlyIHVuYSBmdW5jacOzbiBzaW1wbGUKeSA9IHggKyAyCnogPSB5ICogeSAqIDIKCiMgQ2FsY3VsYXIgbG9zIGdyYWRpZW50ZXMKei5iYWNrd2FyZCh0b3JjaC50ZW5zb3IoWzEuMCwgMS4wLCAxLjBdKSkKcHJpbnQoeC5ncmFkKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Definir una funci&oacute;n simple
y = x + 2
z = y * y * 2

# Calcular los gradientes
z.backward(torch.tensor([1.0, 1.0, 1.0]))
print(x.grad)</pre></div><div class='content'><ul>
<li><code>backward()</code>: Esta función calcula el gradiente de <code>z</code> con respecto a <code>x</code>.</li>
<li><code>x.grad</code>: Este atributo contiene los gradientes calculados.</li>
</ul>
</div><h1>Entendiendo el Grafo Computacional</h1>
<div class='content'><p>El grafo computacional es un grafo acíclico dirigido donde los nodos representan operaciones y los bordes representan tensores.</p>
</div><h2>Ejemplo de un Grafo Computacional</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("YSA9IHRvcmNoLnRlbnNvcihbMi4wLCAzLjBdLCByZXF1aXJlc19ncmFkPVRydWUpCmIgPSBhICogMgpjID0gYiArIDMKZCA9IGMubWVhbigpCgpkLmJhY2t3YXJkKCkKcHJpbnQoYS5ncmFkKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>a = torch.tensor([2.0, 3.0], requires_grad=True)
b = a * 2
c = b + 3
d = c.mean()

d.backward()
print(a.grad)</pre></div><div class='content'><ul>
<li>Aquí, <code>a</code> es el tensor de entrada, <code>b</code> y <code>c</code> son tensores intermedios, y <code>d</code> es la salida final.</li>
<li><code>d.backward()</code>: Calcula el gradiente de <code>d</code> con respecto a <code>a</code>.</li>
</ul>
</div><h1>Técnicas Avanzadas de Autograd</h1>
<h2>Acumulación de Gradientes</h2>
<div class='content'><p>Los gradientes se acumulan en el atributo <code>.grad</code>. Esto es útil para el descenso de gradiente por mini-lotes.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBQb25lciBhIGNlcm8gbG9zIGdyYWRpZW50ZXMgYW50ZXMgZGUgZWplY3V0YXIgbGEgcGFzYWRhIGhhY2lhIGF0csOhcwp4LmdyYWQuemVyb18oKQoKIyBSZWFsaXphciBvdHJhIHBhc2FkYSBoYWNpYSBhdHLDoXMKei5iYWNrd2FyZCh0b3JjaC50ZW5zb3IoWzEuMCwgMS4wLCAxLjBdKSkKcHJpbnQoeC5ncmFkKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Poner a cero los gradientes antes de ejecutar la pasada hacia atr&aacute;s
x.grad.zero_()

# Realizar otra pasada hacia atr&aacute;s
z.backward(torch.tensor([1.0, 1.0, 1.0]))
print(x.grad)</pre></div><div class='content'></div><h2>Detener el Seguimiento de Gradientes</h2>
<div class='content'><p>A veces necesitas detener el seguimiento de gradientes, por ejemplo, durante la inferencia.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("d2l0aCB0b3JjaC5ub19ncmFkKCk6CiAgICB5ID0geCAqIDIKcHJpbnQoeS5yZXF1aXJlc19ncmFkKSAgIyBTYWxpZGE6IEZhbHNl"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>with torch.no_grad():
    y = x * 2
print(y.requires_grad)  # Salida: False</pre></div><div class='content'></div><h2>Gradientes Personalizados</h2>
<div class='content'><p>Puedes definir gradientes personalizados usando la clase <code>torch.autograd.Function</code>.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgTXlSZUxVKHRvcmNoLmF1dG9ncmFkLkZ1bmN0aW9uKToKICAgIEBzdGF0aWNtZXRob2QKICAgIGRlZiBmb3J3YXJkKGN0eCwgaW5wdXQpOgogICAgICAgIGN0eC5zYXZlX2Zvcl9iYWNrd2FyZChpbnB1dCkKICAgICAgICByZXR1cm4gaW5wdXQuY2xhbXAobWluPTApCgogICAgQHN0YXRpY21ldGhvZAogICAgZGVmIGJhY2t3YXJkKGN0eCwgZ3JhZF9vdXRwdXQpOgogICAgICAgIGlucHV0LCA9IGN0eC5zYXZlZF90ZW5zb3JzCiAgICAgICAgZ3JhZF9pbnB1dCA9IGdyYWRfb3V0cHV0LmNsb25lKCkKICAgICAgICBncmFkX2lucHV0W2lucHV0IDwgMF0gPSAwCiAgICAgICAgcmV0dXJuIGdyYWRfaW5wdXQKCiMgVXNhciBlbCBSZUxVIHBlcnNvbmFsaXphZG8KcmVsdSA9IE15UmVMVS5hcHBseQp4ID0gdG9yY2gudGVuc29yKFstMS4wLCAyLjAsIDMuMF0sIHJlcXVpcmVzX2dyYWQ9VHJ1ZSkKeSA9IHJlbHUoeCkKeS5iYWNrd2FyZCh0b3JjaC50ZW5zb3IoWzEuMCwgMS4wLCAxLjBdKSkKcHJpbnQoeC5ncmFkKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class MyReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input &lt; 0] = 0
        return grad_input

# Usar el ReLU personalizado
relu = MyReLU.apply
x = torch.tensor([-1.0, 2.0, 3.0], requires_grad=True)
y = relu(x)
y.backward(torch.tensor([1.0, 1.0, 1.0]))
print(x.grad)</pre></div><div class='content'></div><h1>Conclusión</h1>
<div class='content'><p>Autograd es una característica poderosa en PyTorch que simplifica el proceso de cálculo de gradientes, los cuales son esenciales para entrenar redes neuronales. Al entender cómo crear tensores con gradientes, manipular el grafo computacional y usar técnicas avanzadas como la acumulación de gradientes y gradientes personalizados, puedes aprovechar efectivamente Autograd en tus proyectos de aprendizaje profundo.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='basic-tensor-operations'>&#x25C4;Operaciones Básicas con Tensores</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Autograd: Diferenciación Automática</a>
	</div>
	<div class='col-4 text-end'>
					<a href='introduction-to-neural-networks'>Introducción a las Redes Neuronales &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
