<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Regularización y Dropout</title>

    <link rel="alternate" href="https://campusempresa.com/deep_learning/regularizacion-dropout" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/deep_learning/regularizacion-dropout" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/deep_learning/regularizacion-dropout" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<a href="https://enterprisecampus.net/deep_learning/regularizacion-dropout" class="px-2">EN</a></b>
				|
				<b class="px-2">ES</b>
				|
				<a href="https://campusempresa.cat/deep_learning/regularizacion-dropout" class="px-2">CA</a>
								</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Proyecto</a>
				<a href="/about">Sobre nosotros</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donaciones</a>
				<a href="/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='transfer-learning'>&#x25C4;Transfer Learning</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Regularización y Dropout</a>
	</div>
	<div class='col-4 text-end'>
					<a href='introduccion-tensorflow'>Introducción a TensorFlow &#x25BA;</a>
			</div>
</div>
<div class='content'><p>En este tema, exploraremos dos técnicas esenciales en el Deep Learning para prevenir el sobreajuste: la regularización y el dropout. Estas técnicas son fundamentales para mejorar la capacidad de generalización de los modelos de redes neuronales.</p>
</div><h1>Regularización</h1>
<div class='content'><p>La regularización es una técnica utilizada para reducir el sobreajuste en los modelos de aprendizaje automático. El sobreajuste ocurre cuando un modelo se ajusta demasiado a los datos de entrenamiento y no generaliza bien a los datos nuevos. Existen varias formas de regularización, pero las más comunes son L1 y L2.</p>
</div><h2>L1 Regularización (Lasso)</h2>
<div class='content'><ul>
<li><strong>Definición</strong>: La regularización L1 agrega una penalización igual a la suma de los valores absolutos de los coeficientes del modelo.</li>
<li><strong>Fórmula</strong>:
\[
\text{Loss} = \text{Loss original} + \lambda \sum_{i} |w_i|
\]</li>
<li><strong>Ventajas</strong>:
<ul>
<li>Puede llevar a soluciones esparsas, es decir, algunos coeficientes se vuelven exactamente cero.</li>
</ul>
</li>
<li><strong>Ejemplo de código</strong>:
<pre><code class="language-python">from keras.regularizers import l1
model.add(Dense(64, input_dim=64, kernel_regularizer=l1(0.01)))
</code></pre>
</li>
</ul>
</div><h2>L2 Regularización (Ridge)</h2>
<div class='content'><ul>
<li><strong>Definición</strong>: La regularización L2 agrega una penalización igual a la suma de los cuadrados de los coeficientes del modelo.</li>
<li><strong>Fórmula</strong>:
\[
\text{Loss} = \text{Loss original} + \lambda \sum_{i} w_i^2
\]</li>
<li><strong>Ventajas</strong>:
<ul>
<li>No conduce a soluciones esparsas, pero puede reducir la magnitud de los coeficientes.</li>
</ul>
</li>
<li><strong>Ejemplo de código</strong>:
<pre><code class="language-python">from keras.regularizers import l2
model.add(Dense(64, input_dim=64, kernel_regularizer=l2(0.01)))
</code></pre>
</li>
</ul>
</div><h2>Comparación entre L1 y L2</h2>
<div class='content'><p>| Característica | L1 (Lasso) | L2 (Ridge) |
|----------------|------------|------------|
| Penalización   | Suma de valores absolutos | Suma de cuadrados |
| Soluciones esparsas | Sí | No |
| Uso común      | Selección de características | Reducción de la magnitud de los coeficientes |</p>
</div><h1>Dropout</h1>
<div class='content'><p>El dropout es una técnica de regularización que consiste en &quot;apagar&quot; aleatoriamente un conjunto de neuronas durante el entrenamiento. Esto ayuda a prevenir el sobreajuste al reducir la dependencia de los modelos en neuronas específicas.</p>
</div><h2>Funcionamiento del Dropout</h2>
<div class='content'><ul>
<li><strong>Definición</strong>: Durante cada paso de entrenamiento, cada neurona tiene una probabilidad \( p \) de ser &quot;apagada&quot; (es decir, su salida se establece en cero).</li>
<li><strong>Ventajas</strong>:
<ul>
<li>Reduce el sobreajuste.</li>
<li>Mejora la capacidad de generalización del modelo.</li>
</ul>
</li>
<li><strong>Ejemplo de código</strong>:
<pre><code class="language-python">from keras.layers import Dropout
model.add(Dense(64, input_dim=64, activation='relu'))
model.add(Dropout(0.5))
</code></pre>
</li>
</ul>
</div><h2>Ejemplo Práctico</h2>
<div class='content'><p>A continuación, se muestra un ejemplo completo de cómo implementar la regularización y el dropout en un modelo de red neuronal utilizando Keras:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBrZXJhcy5tb2RlbHMgaW1wb3J0IFNlcXVlbnRpYWwKZnJvbSBrZXJhcy5sYXllcnMgaW1wb3J0IERlbnNlLCBEcm9wb3V0CmZyb20ga2VyYXMucmVndWxhcml6ZXJzIGltcG9ydCBsMgoKIyBDcmVhciBlbCBtb2RlbG8KbW9kZWwgPSBTZXF1ZW50aWFsKCkKCiMgQcOxYWRpciBjYXBhcyBjb24gcmVndWxhcml6YWNpw7NuIEwyIHkgZHJvcG91dAptb2RlbC5hZGQoRGVuc2UoNjQsIGlucHV0X2RpbT02NCwgYWN0aXZhdGlvbj0ncmVsdScsIGtlcm5lbF9yZWd1bGFyaXplcj1sMigwLjAxKSkpCm1vZGVsLmFkZChEcm9wb3V0KDAuNSkpCm1vZGVsLmFkZChEZW5zZSg2NCwgYWN0aXZhdGlvbj0ncmVsdScsIGtlcm5lbF9yZWd1bGFyaXplcj1sMigwLjAxKSkpCm1vZGVsLmFkZChEcm9wb3V0KDAuNSkpCm1vZGVsLmFkZChEZW5zZSgxMCwgYWN0aXZhdGlvbj0nc29mdG1heCcpKQoKIyBDb21waWxhciBlbCBtb2RlbG8KbW9kZWwuY29tcGlsZShsb3NzPSdjYXRlZ29yaWNhbF9jcm9zc2VudHJvcHknLCBvcHRpbWl6ZXI9J2FkYW0nLCBtZXRyaWNzPVsnYWNjdXJhY3knXSkKCiMgUmVzdW1lbiBkZWwgbW9kZWxvCm1vZGVsLnN1bW1hcnkoKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.regularizers import l2

# Crear el modelo
model = Sequential()

# A&ntilde;adir capas con regularizaci&oacute;n L2 y dropout
model.add(Dense(64, input_dim=64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

# Compilar el modelo
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Resumen del modelo
model.summary()</pre></div><div class='content'></div><h1>Conclusión</h1>
<div class='content'><p>La regularización y el dropout son técnicas cruciales para mejorar la capacidad de generalización de los modelos de redes neuronales y prevenir el sobreajuste. La regularización L1 y L2 ayudan a controlar la magnitud de los coeficientes del modelo, mientras que el dropout reduce la dependencia de neuronas específicas. Al combinar estas técnicas, podemos construir modelos más robustos y eficientes en Deep Learning.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='transfer-learning'>&#x25C4;Transfer Learning</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Regularización y Dropout</a>
	</div>
	<div class='col-4 text-end'>
					<a href='introduccion-tensorflow'>Introducción a TensorFlow &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Aceptar</a>
    <a href="/cookies">Mas información</a>
</div>	

	</div>    
</body>
</html>
