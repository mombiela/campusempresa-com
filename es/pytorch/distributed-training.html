<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title></title>

	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy<br> y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<b id="lit_lang_es">Castellano</b>
				|
				<a href="https://campusempresa.cat/pytorch/distributed-training" id="lnk_lang_ca" data-lang="ca">Català</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="objective">El Proyecto</a>
				<a href="about">Sobre nosotros</a>
				<a href="contribute">Contribuir</a>
				<a href="donate">Donaciones</a>
				<a href="licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'><p>El entrenamiento distribuido es una técnica utilizada para entrenar modelos de aprendizaje automático en múltiples dispositivos o máquinas. Esto puede reducir significativamente el tiempo de entrenamiento y permitir el manejo de conjuntos de datos y modelos más grandes. En esta sección, cubriremos los conceptos básicos del entrenamiento distribuido en PyTorch, progresando desde conceptos para principiantes hasta avanzados.</p>
</div><h1>Introducción al Entrenamiento Distribuido</h1>
<div class='content'><p>El entrenamiento distribuido implica dividir el proceso de entrenamiento en múltiples dispositivos o nodos. Esto se puede hacer de varias maneras, incluyendo el paralelismo de datos y el paralelismo de modelos.</p>
<ul>
<li><strong>Paralelismo de Datos</strong>: Cada dispositivo procesa un subconjunto diferente de los datos.</li>
<li><strong>Paralelismo de Modelos</strong>: El modelo en sí se divide en múltiples dispositivos.</li>
</ul>
</div><h1>Configuración del Entrenamiento Distribuido en PyTorch</h1>
<div class='content'></div><h2>Prerrequisitos</h2>
<div class='content'><p>Antes de sumergirse en el entrenamiento distribuido, asegúrese de tener lo siguiente:</p>
<ul>
<li>Una comprensión básica de PyTorch.</li>
<li>Múltiples GPUs o máquinas disponibles para el entrenamiento.</li>
<li>PyTorch instalado con soporte para CUDA.</li>
</ul>
</div><h2>Conceptos Básicos</h2>
<div class='content'><ul>
<li><strong>Distributed Data Parallel (DDP)</strong>: Módulo de PyTorch para el paralelismo de datos.</li>
<li><strong>Grupo de Procesos</strong>: Un grupo de procesos que pueden comunicarse entre sí.</li>
<li><strong>Backend</strong>: El backend de comunicación (por ejemplo, <code>nccl</code>, <code>gloo</code>, <code>mpi</code>).</li>
</ul>
</div><h2>Inicialización del Entrenamiento Distribuido</h2>
<div class='content'><p>Para comenzar con el entrenamiento distribuido, necesita inicializar el grupo de procesos y configurar el entorno.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5kaXN0cmlidXRlZCBhcyBkaXN0CgpkZWYgaW5pdF9wcm9jZXNzKHJhbmssIHNpemUsIGJhY2tlbmQ9J25jY2wnKToKICAgICIiIiBJbmljaWFsaXphciBlbCBlbnRvcm5vIGRpc3RyaWJ1aWRvLiAiIiIKICAgIGRpc3QuaW5pdF9wcm9jZXNzX2dyb3VwKGJhY2tlbmQsIHJhbms9cmFuaywgd29ybGRfc2l6ZT1zaXplKQoKIyBFamVtcGxvIGRlIHVzbwpyYW5rID0gMCAgIyBSYW5nbyBkZWwgcHJvY2VzbyBhY3R1YWwKc2l6ZSA9IDQgICMgTsO6bWVybyB0b3RhbCBkZSBwcm9jZXNvcwppbml0X3Byb2Nlc3MocmFuaywgc2l6ZSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.distributed as dist

def init_process(rank, size, backend='nccl'):
    &quot;&quot;&quot; Inicializar el entorno distribuido. &quot;&quot;&quot;
    dist.init_process_group(backend, rank=rank, world_size=size)

# Ejemplo de uso
rank = 0  # Rango del proceso actual
size = 4  # N&uacute;mero total de procesos
init_process(rank, size)</pre></div><div class='content'></div><h2>Distributed Data Parallel (DDP)</h2>
<div class='content'><p>DDP es un contenedor que ayuda a paralelizar el proceso de entrenamiento en múltiples GPUs.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgppbXBvcnQgdG9yY2gub3B0aW0gYXMgb3B0aW0KaW1wb3J0IHRvcmNoLmRpc3RyaWJ1dGVkIGFzIGRpc3QKZnJvbSB0b3JjaC5ubi5wYXJhbGxlbCBpbXBvcnQgRGlzdHJpYnV0ZWREYXRhUGFyYWxsZWwgYXMgRERQCgojIERlZmluaXIgdW4gbW9kZWxvIHNpbXBsZQpjbGFzcyBTaW1wbGVNb2RlbChubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYpOgogICAgICAgIHN1cGVyKFNpbXBsZU1vZGVsLCBzZWxmKS5fX2luaXRfXygpCiAgICAgICAgc2VsZi5mYyA9IG5uLkxpbmVhcigxMCwgMTApCgogICAgZGVmIGZvcndhcmQoc2VsZiwgeCk6CiAgICAgICAgcmV0dXJuIHNlbGYuZmMoeCkKCiMgSW5pY2lhbGl6YXIgZWwgZ3J1cG8gZGUgcHJvY2Vzb3MKZGlzdC5pbml0X3Byb2Nlc3NfZ3JvdXAoYmFja2VuZD0nbmNjbCcpCgojIENyZWFyIGVsIG1vZGVsbyB5IG1vdmVybG8gYSBsYSBHUFUKbW9kZWwgPSBTaW1wbGVNb2RlbCgpLmN1ZGEoKQpkZHBfbW9kZWwgPSBERFAobW9kZWwpCgojIERlZmluaXIgbGEgZnVuY2nDs24gZGUgcMOpcmRpZGEgeSBlbCBvcHRpbWl6YWRvcgpjcml0ZXJpb24gPSBubi5NU0VMb3NzKCkKb3B0aW1pemVyID0gb3B0aW0uU0dEKGRkcF9tb2RlbC5wYXJhbWV0ZXJzKCksIGxyPTAuMDAxKQoKIyBFbnRyYWRhIHkgb2JqZXRpdm8gZmljdGljaW9zCmlucHV0ID0gdG9yY2gucmFuZG4oMjAsIDEwKS5jdWRhKCkKdGFyZ2V0ID0gdG9yY2gucmFuZG4oMjAsIDEwKS5jdWRhKCkKCiMgUGFzbyBoYWNpYSBhZGVsYW50ZQpvdXRwdXQgPSBkZHBfbW9kZWwoaW5wdXQpCmxvc3MgPSBjcml0ZXJpb24ob3V0cHV0LCB0YXJnZXQpCgojIFBhc28gaGFjaWEgYXRyw6FzIHkgb3B0aW1pemFjacOzbgpvcHRpbWl6ZXIuemVyb19ncmFkKCkKbG9zcy5iYWNrd2FyZCgpCm9wdGltaXplci5zdGVwKCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# Definir un modelo simple
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 10)

    def forward(self, x):
        return self.fc(x)

# Inicializar el grupo de procesos
dist.init_process_group(backend='nccl')

# Crear el modelo y moverlo a la GPU
model = SimpleModel().cuda()
ddp_model = DDP(model)

# Definir la funci&oacute;n de p&eacute;rdida y el optimizador
criterion = nn.MSELoss()
optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

# Entrada y objetivo ficticios
input = torch.randn(20, 10).cuda()
target = torch.randn(20, 10).cuda()

# Paso hacia adelante
output = ddp_model(input)
loss = criterion(output, target)

# Paso hacia atr&aacute;s y optimizaci&oacute;n
optimizer.zero_grad()
loss.backward()
optimizer.step()</pre></div><div class='content'></div><h1>Técnicas Avanzadas de Entrenamiento Distribuido</h1>
<div class='content'></div><h2>Acumulación de Gradientes</h2>
<div class='content'><p>La acumulación de gradientes le permite simular un tamaño de lote más grande acumulando gradientes durante múltiples iteraciones.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBBY3VtdWxhciBncmFkaWVudGVzIGR1cmFudGUgbcO6bHRpcGxlcyBpdGVyYWNpb25lcwphY2N1bXVsYXRpb25fc3RlcHMgPSA0CmZvciBpIGluIHJhbmdlKGFjY3VtdWxhdGlvbl9zdGVwcyk6CiAgICBvdXRwdXQgPSBkZHBfbW9kZWwoaW5wdXQpCiAgICBsb3NzID0gY3JpdGVyaW9uKG91dHB1dCwgdGFyZ2V0KQogICAgbG9zcyA9IGxvc3MgLyBhY2N1bXVsYXRpb25fc3RlcHMgICMgTm9ybWFsaXphciBsYSBww6lyZGlkYQogICAgbG9zcy5iYWNrd2FyZCgpCgogICAgaWYgKGkgKyAxKSAlIGFjY3VtdWxhdGlvbl9zdGVwcyA9PSAwOgogICAgICAgIG9wdGltaXplci5zdGVwKCkKICAgICAgICBvcHRpbWl6ZXIuemVyb19ncmFkKCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Acumular gradientes durante m&uacute;ltiples iteraciones
accumulation_steps = 4
for i in range(accumulation_steps):
    output = ddp_model(input)
    loss = criterion(output, target)
    loss = loss / accumulation_steps  # Normalizar la p&eacute;rdida
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()</pre></div><div class='content'></div><h2>Entrenamiento de Precisión Mixta</h2>
<div class='content'><p>El entrenamiento de precisión mixta utiliza tipos de punto flotante de 16 bits y 32 bits para reducir el uso de memoria y acelerar el entrenamiento.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSB0b3JjaC5jdWRhLmFtcCBpbXBvcnQgR3JhZFNjYWxlciwgYXV0b2Nhc3QKCnNjYWxlciA9IEdyYWRTY2FsZXIoKQoKZm9yIGlucHV0LCB0YXJnZXQgaW4gZGF0YV9sb2FkZXI6CiAgICBpbnB1dCwgdGFyZ2V0ID0gaW5wdXQuY3VkYSgpLCB0YXJnZXQuY3VkYSgpCgogICAgd2l0aCBhdXRvY2FzdCgpOgogICAgICAgIG91dHB1dCA9IGRkcF9tb2RlbChpbnB1dCkKICAgICAgICBsb3NzID0gY3JpdGVyaW9uKG91dHB1dCwgdGFyZ2V0KQoKICAgIHNjYWxlci5zY2FsZShsb3NzKS5iYWNrd2FyZCgpCiAgICBzY2FsZXIuc3RlcChvcHRpbWl6ZXIpCiAgICBzY2FsZXIudXBkYXRlKCkKICAgIG9wdGltaXplci56ZXJvX2dyYWQoKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from torch.cuda.amp import GradScaler, autocast

scaler = GradScaler()

for input, target in data_loader:
    input, target = input.cuda(), target.cuda()

    with autocast():
        output = ddp_model(input)
        loss = criterion(output, target)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()</pre></div><div class='content'></div><h1>Conclusión</h1>
<div class='content'><p>El entrenamiento distribuido en PyTorch permite una escalabilidad eficiente de los modelos de aprendizaje automático en múltiples dispositivos. Al comprender los conceptos básicos de Distributed Data Parallel (DDP), inicializar grupos de procesos e implementar técnicas avanzadas como la acumulación de gradientes y el entrenamiento de precisión mixta, puede mejorar significativamente sus capacidades de entrenamiento de modelos. A medida que avance, puede explorar configuraciones y optimizaciones más complejas para mejorar aún más el rendimiento y la eficiencia.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
       <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
       <a href="cookies">Més informació</a>
   </div>	
	</div>    
</body>
</html>
