<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title></title>

	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy<br> y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<b id="lit_lang_es">Castellano</b>
				|
				<a href="https://campusempresa.cat/pytorch/optimizing-for-inference" id="lnk_lang_ca" data-lang="ca">Català</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="objective">El Proyecto</a>
				<a href="about">Sobre nosotros</a>
				<a href="contribute">Contribuir</a>
				<a href="donate">Donaciones</a>
				<a href="licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'><p>La inferencia es el proceso de usar un modelo entrenado para hacer predicciones sobre nuevos datos. Optimizar para la inferencia implica hacer que tu modelo funcione más rápido y de manera más eficiente durante esta fase. Esto es crucial para desplegar modelos en entornos de producción donde el rendimiento y la utilización de recursos son críticos.</p>
</div><h1>Conceptos Clave</h1>
<div class='content'><ul>
<li><strong>Cuantización del Modelo</strong>: Reducir la precisión de los pesos y activaciones del modelo a anchos de bits más bajos (por ejemplo, de punto flotante de 32 bits a entero de 8 bits).</li>
<li><strong>Poda</strong>: Eliminar pesos menos importantes del modelo para reducir su tamaño y mejorar la velocidad de inferencia.</li>
<li><strong>Loteo</strong>: Procesar múltiples entradas a la vez para aprovechar el paralelismo.</li>
<li><strong>Aceleración por Hardware</strong>: Utilizar hardware especializado como GPUs, TPUs o FPGAs para acelerar la inferencia.</li>
<li><strong>Exportación del Modelo</strong>: Convertir el modelo a un formato optimizado para la inferencia, como TorchScript o ONNX.</li>
</ul>
</div><h1>Cuantización del Modelo</h1>
<div class='content'><p>La cuantización puede reducir significativamente el tamaño del modelo y mejorar la velocidad de inferencia con una pérdida mínima de precisión.</p>
</div><h2>Ejemplo: Cuantización Post-Entrenamiento</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaHZpc2lvbi5tb2RlbHMgYXMgbW9kZWxzCmZyb20gdG9yY2gucXVhbnRpemF0aW9uIGltcG9ydCBxdWFudGl6ZV9keW5hbWljCgojIENhcmdhciB1biBtb2RlbG8gcHJlZW50cmVuYWRvCm1vZGVsID0gbW9kZWxzLnJlc25ldDE4KHByZXRyYWluZWQ9VHJ1ZSkKCiMgQXBsaWNhciBjdWFudGl6YWNpw7NuIGRpbsOhbWljYQpxdWFudGl6ZWRfbW9kZWwgPSBxdWFudGl6ZV9keW5hbWljKG1vZGVsLCB7dG9yY2gubm4uTGluZWFyfSwgZHR5cGU9dG9yY2gucWludDgpCgojIEd1YXJkYXIgZWwgbW9kZWxvIGN1YW50aXphZG8KdG9yY2guc2F2ZShxdWFudGl6ZWRfbW9kZWwuc3RhdGVfZGljdCgpLCAncXVhbnRpemVkX3Jlc25ldDE4LnB0aCcp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torchvision.models as models
from torch.quantization import quantize_dynamic

# Cargar un modelo preentrenado
model = models.resnet18(pretrained=True)

# Aplicar cuantizaci&oacute;n din&aacute;mica
quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)

# Guardar el modelo cuantizado
torch.save(quantized_model.state_dict(), 'quantized_resnet18.pth')</pre></div><div class='content'><ul>
<li><strong>Explicación</strong>: Este ejemplo demuestra cómo aplicar cuantización dinámica a un modelo ResNet-18 preentrenado. La función <code>quantize_dynamic</code> reduce la precisión de las capas lineales a enteros de 8 bits.</li>
</ul>
</div><h1>Poda</h1>
<div class='content'><p>La poda implica eliminar pesos que contribuyen menos a las predicciones del modelo, reduciendo así la complejidad del modelo.</p>
</div><h2>Ejemplo: Poda Básica de Pesos</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubi51dGlscy5wcnVuZSBhcyBwcnVuZQoKIyBEZWZpbmlyIHVuIG1vZGVsbyBzaW1wbGUKbW9kZWwgPSB0b3JjaC5ubi5TZXF1ZW50aWFsKAogICAgdG9yY2gubm4uTGluZWFyKDEwLCAxMCksCiAgICB0b3JjaC5ubi5SZUxVKCksCiAgICB0b3JjaC5ubi5MaW5lYXIoMTAsIDEpCikKCiMgQXBsaWNhciBwb2RhIGEgbGEgcHJpbWVyYSBjYXBhIGxpbmVhbApwcnVuZS5sMV91bnN0cnVjdHVyZWQobW9kZWxbMF0sIG5hbWU9J3dlaWdodCcsIGFtb3VudD0wLjQpCgojIFZlcmlmaWNhciBsYSBlc3BhcnNpZGFkIGRlIGxhIGNhcGEgcG9kYWRhCnByaW50KG1vZGVsWzBdLndlaWdodCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn.utils.prune as prune

# Definir un modelo simple
model = torch.nn.Sequential(
    torch.nn.Linear(10, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)

# Aplicar poda a la primera capa lineal
prune.l1_unstructured(model[0], name='weight', amount=0.4)

# Verificar la esparsidad de la capa podada
print(model[0].weight)</pre></div><div class='content'><ul>
<li><strong>Explicación</strong>: Este ejemplo muestra cómo podar el 40% de los pesos en la primera capa lineal de una red neuronal simple utilizando poda no estructurada L1.</li>
</ul>
</div><h1>Loteo</h1>
<div class='content'><p>El loteo te permite procesar múltiples entradas simultáneamente, lo que puede acelerar significativamente la inferencia, especialmente en GPUs.</p>
</div><h2>Ejemplo: Loteo de Entradas</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIERhdG9zIGRlIGVudHJhZGEgZmljdGljaW9zCmlucHV0cyA9IHRvcmNoLnJhbmRuKDMyLCAzLCAyMjQsIDIyNCkgICMgTG90ZSBkZSAzMiBpbcOhZ2VuZXMKCiMgQ2FyZ2FyIHVuIG1vZGVsbyBwcmVlbnRyZW5hZG8KbW9kZWwgPSBtb2RlbHMucmVzbmV0MTgocHJldHJhaW5lZD1UcnVlKQptb2RlbC5ldmFsKCkKCiMgUmVhbGl6YXIgaW5mZXJlbmNpYQp3aXRoIHRvcmNoLm5vX2dyYWQoKToKICAgIG91dHB1dHMgPSBtb2RlbChpbnB1dHMpCgpwcmludChvdXRwdXRzLnNoYXBlKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Datos de entrada ficticios
inputs = torch.randn(32, 3, 224, 224)  # Lote de 32 im&aacute;genes

# Cargar un modelo preentrenado
model = models.resnet18(pretrained=True)
model.eval()

# Realizar inferencia
with torch.no_grad():
    outputs = model(inputs)

print(outputs.shape)</pre></div><div class='content'><ul>
<li><strong>Explicación</strong>: Este ejemplo demuestra cómo realizar inferencia en un lote de 32 imágenes utilizando un modelo ResNet-18 preentrenado. El contexto <code>torch.no_grad()</code> se utiliza para deshabilitar el cálculo de gradientes, lo cual es innecesario para la inferencia y ahorra memoria.</li>
</ul>
</div><h1>Aceleración por Hardware</h1>
<div class='content'><p>Utilizar hardware especializado puede mejorar enormemente el rendimiento de la inferencia. PyTorch soporta la ejecución de modelos en GPUs y otros aceleradores.</p>
</div><h2>Ejemplo: Uso de GPU para la Inferencia</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIFZlcmlmaWNhciBzaSBsYSBHUFUgZXN0w6EgZGlzcG9uaWJsZQpkZXZpY2UgPSB0b3JjaC5kZXZpY2UoJ2N1ZGEnIGlmIHRvcmNoLmN1ZGEuaXNfYXZhaWxhYmxlKCkgZWxzZSAnY3B1JykKCiMgQ2FyZ2FyIHVuIG1vZGVsbyBwcmVlbnRyZW5hZG8geSBtb3ZlcmxvIGEgbGEgR1BVCm1vZGVsID0gbW9kZWxzLnJlc25ldDE4KHByZXRyYWluZWQ9VHJ1ZSkudG8oZGV2aWNlKQptb2RlbC5ldmFsKCkKCiMgRGF0b3MgZGUgZW50cmFkYSBmaWN0aWNpb3MKaW5wdXRzID0gdG9yY2gucmFuZG4oMzIsIDMsIDIyNCwgMjI0KS50byhkZXZpY2UpICAjIExvdGUgZGUgMzIgaW3DoWdlbmVzCgojIFJlYWxpemFyIGluZmVyZW5jaWEKd2l0aCB0b3JjaC5ub19ncmFkKCk6CiAgICBvdXRwdXRzID0gbW9kZWwoaW5wdXRzKQoKcHJpbnQob3V0cHV0cy5zaGFwZSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Verificar si la GPU est&aacute; disponible
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Cargar un modelo preentrenado y moverlo a la GPU
model = models.resnet18(pretrained=True).to(device)
model.eval()

# Datos de entrada ficticios
inputs = torch.randn(32, 3, 224, 224).to(device)  # Lote de 32 im&aacute;genes

# Realizar inferencia
with torch.no_grad():
    outputs = model(inputs)

print(outputs.shape)</pre></div><div class='content'><ul>
<li><strong>Explicación</strong>: Este ejemplo muestra cómo mover un modelo y datos de entrada a la GPU para una inferencia más rápida. La función <code>torch.cuda.is_available()</code> verifica si una GPU está disponible.</li>
</ul>
</div><h1>Exportación del Modelo</h1>
<div class='content'><p>Exportar tu modelo a un formato optimizado para la inferencia puede mejorar el rendimiento y la compatibilidad con varios entornos de despliegue.</p>
</div><h2>Ejemplo: Exportación a TorchScript</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIENhcmdhciB1biBtb2RlbG8gcHJlZW50cmVuYWRvCm1vZGVsID0gbW9kZWxzLnJlc25ldDE4KHByZXRyYWluZWQ9VHJ1ZSkKbW9kZWwuZXZhbCgpCgojIENvbnZlcnRpciBlbCBtb2RlbG8gYSBUb3JjaFNjcmlwdApzY3JpcHRlZF9tb2RlbCA9IHRvcmNoLmppdC5zY3JpcHQobW9kZWwpCgojIEd1YXJkYXIgZWwgbW9kZWxvIFRvcmNoU2NyaXB0CnNjcmlwdGVkX21vZGVsLnNhdmUoJ3Jlc25ldDE4X3NjcmlwdGVkLnB0Jyk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Cargar un modelo preentrenado
model = models.resnet18(pretrained=True)
model.eval()

# Convertir el modelo a TorchScript
scripted_model = torch.jit.script(model)

# Guardar el modelo TorchScript
scripted_model.save('resnet18_scripted.pt')</pre></div><div class='content'><ul>
<li><strong>Explicación</strong>: Este ejemplo demuestra cómo convertir un modelo ResNet-18 preentrenado a TorchScript, que es un formato optimizado para la inferencia. La función <code>torch.jit.script</code> se utiliza para esta conversión.</li>
</ul>
</div><h1>Conclusión</h1>
<div class='content'><p>Optimizar para la inferencia es un paso crítico en el despliegue de modelos de aprendizaje automático en producción. Técnicas como la cuantización, la poda, el loteo, la aceleración por hardware y la exportación del modelo pueden mejorar significativamente el rendimiento y la eficiencia de tus modelos. Al comprender y aplicar estas técnicas, puedes asegurarte de que tus modelos funcionen más rápido y de manera más eficiente, haciéndolos más adecuados para aplicaciones del mundo real.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
       <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
       <a href="cookies">Més informació</a>
   </div>	
	</div>    
</body>
</html>
