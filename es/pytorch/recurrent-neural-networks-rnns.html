<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Redes Neuronales Recurrentes (RNNs)</title>

    <link rel="alternate" href="https://campusempresa.com/es/pytorch/recurrent-neural-networks-rnns" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/pytorch/recurrent-neural-networks-rnns" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/pytorch/recurrent-neural-networks-rnns" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/es/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy<br> y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<b id="lit_lang_es" class="px-2">ES</b>
				|
				<a href="/ca/pytorch/recurrent-neural-networks-rnns" id="lnk_lang_ca" data-lang="ca" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/es/objective">El Proyecto</a>
				<a href="/es/about">Sobre nosotros</a>
				<a href="/es/contribute">Contribuir</a>
				<a href="/es/donate">Donaciones</a>
				<a href="/es/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'><p>Las Redes Neuronales Recurrentes (RNNs) son una clase de redes neuronales que son particularmente efectivas para datos secuenciales. En esta sección, exploraremos los fundamentos de las RNNs, cómo implementarlas usando PyTorch y técnicas avanzadas para mejorar su rendimiento.</p>
</div><h1>Introducción a las RNNs</h1>
<div class='content'><ul>
<li><strong>Datos Secuenciales</strong>: Datos donde el orden de los elementos es importante, como series temporales, texto y audio.</li>
<li><strong>Conexiones Recurrentes</strong>: A diferencia de las redes neuronales tradicionales, las RNNs tienen conexiones que forman ciclos, permitiendo que la información persista.</li>
</ul>
</div><h2>Conceptos Clave</h2>
<div class='content'><ul>
<li><strong>Estado Oculto</strong>: Una memoria que captura información sobre elementos previos en la secuencia.</li>
<li><strong>Gradientes Desvanecientes/Explosivos</strong>: Desafíos en el entrenamiento de RNNs debido a dependencias a largo plazo.</li>
</ul>
</div><h1>Implementación de RNNs en PyTorch</h1>
<div class='content'></div><h2>Estructura Básica de una RNN</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgoKY2xhc3MgU2ltcGxlUk5OKG5uLk1vZHVsZSk6CiAgICBkZWYgX19pbml0X18oc2VsZiwgaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemUsIG91dHB1dF9zaXplKToKICAgICAgICBzdXBlcihTaW1wbGVSTk4sIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmhpZGRlbl9zaXplID0gaGlkZGVuX3NpemUKICAgICAgICBzZWxmLnJubiA9IG5uLlJOTihpbnB1dF9zaXplLCBoaWRkZW5fc2l6ZSwgYmF0Y2hfZmlyc3Q9VHJ1ZSkKICAgICAgICBzZWxmLmZjID0gbm4uTGluZWFyKGhpZGRlbl9zaXplLCBvdXRwdXRfc2l6ZSkKICAgIAogICAgZGVmIGZvcndhcmQoc2VsZiwgeCk6CiAgICAgICAgaDAgPSB0b3JjaC56ZXJvcygxLCB4LnNpemUoMCksIHNlbGYuaGlkZGVuX3NpemUpLnRvKHguZGV2aWNlKSAgIyBFc3RhZG8gb2N1bHRvIGluaWNpYWwKICAgICAgICBvdXQsIGhuID0gc2VsZi5ybm4oeCwgaDApCiAgICAgICAgb3V0ID0gc2VsZi5mYyhvdXRbOiwgLTEsIDpdKSAgIyBUb21hbmRvIGxhIHNhbGlkYSBkZWwgw7psdGltbyBwYXNvIGRlIHRpZW1wbwogICAgICAgIHJldHVybiBvdXQKCiMgRWplbXBsbyBkZSB1c28KaW5wdXRfc2l6ZSA9IDEwCmhpZGRlbl9zaXplID0gMjAKb3V0cHV0X3NpemUgPSAxCm1vZGVsID0gU2ltcGxlUk5OKGlucHV0X3NpemUsIGhpZGRlbl9zaXplLCBvdXRwdXRfc2l6ZSk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)  # Estado oculto inicial
        out, hn = self.rnn(x, h0)
        out = self.fc(out[:, -1, :])  # Tomando la salida del &uacute;ltimo paso de tiempo
        return out

# Ejemplo de uso
input_size = 10
hidden_size = 20
output_size = 1
model = SimpleRNN(input_size, hidden_size, output_size)</pre></div><div class='content'></div><h2>Explicación</h2>
<div class='content'><ul>
<li><strong>Tamaño de Entrada</strong>: Número de características en la entrada.</li>
<li><strong>Tamaño Oculto</strong>: Número de características en el estado oculto.</li>
<li><strong>Tamaño de Salida</strong>: Número de características en la salida.</li>
<li><strong>Batch First</strong>: Indica que el tensor de entrada tiene el tamaño del lote como la primera dimensión.</li>
</ul>
</div><h2>Entrenamiento de la RNN</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoLm9wdGltIGFzIG9wdGltCgojIERhdG9zIGZpY3RpY2lvcwppbnB1dHMgPSB0b3JjaC5yYW5kbigzMiwgMTAsIGlucHV0X3NpemUpICAjIFRhbWHDsW8gZGVsIGxvdGUgZGUgMzIsIGxvbmdpdHVkIGRlIHNlY3VlbmNpYSBkZSAxMAp0YXJnZXRzID0gdG9yY2gucmFuZG4oMzIsIG91dHB1dF9zaXplKQoKIyBQw6lyZGlkYSB5IG9wdGltaXphZG9yCmNyaXRlcmlvbiA9IG5uLk1TRUxvc3MoKQpvcHRpbWl6ZXIgPSBvcHRpbS5BZGFtKG1vZGVsLnBhcmFtZXRlcnMoKSwgbHI9MC4wMDEpCgojIEJ1Y2xlIGRlIGVudHJlbmFtaWVudG8KZm9yIGVwb2NoIGluIHJhbmdlKDEwMCk6CiAgICBtb2RlbC50cmFpbigpCiAgICBvdXRwdXRzID0gbW9kZWwoaW5wdXRzKQogICAgbG9zcyA9IGNyaXRlcmlvbihvdXRwdXRzLCB0YXJnZXRzKQogICAgCiAgICBvcHRpbWl6ZXIuemVyb19ncmFkKCkKICAgIGxvc3MuYmFja3dhcmQoKQogICAgb3B0aW1pemVyLnN0ZXAoKQogICAgCiAgICBpZiAoZXBvY2grMSkgJSAxMCA9PSAwOgogICAgICAgIHByaW50KGYnRXBvY2ggW3tlcG9jaCsxfS8xMDBdLCBMb3NzOiB7bG9zcy5pdGVtKCk6LjRmfScp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch.optim as optim

# Datos ficticios
inputs = torch.randn(32, 10, input_size)  # Tama&ntilde;o del lote de 32, longitud de secuencia de 10
targets = torch.randn(32, output_size)

# P&eacute;rdida y optimizador
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Bucle de entrenamiento
for epoch in range(100):
    model.train()
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')</pre></div><div class='content'></div><h2>Explicación</h2>
<div class='content'><ul>
<li><strong>Función de Pérdida</strong>: Error Cuadrático Medio (MSE) para tareas de regresión.</li>
<li><strong>Optimizador</strong>: Optimizador Adam para un entrenamiento eficiente.</li>
<li><strong>Bucle de Entrenamiento</strong>: Itera sobre épocas, calcula la pérdida, realiza retropropagación y actualiza los pesos.</li>
</ul>
</div><h1>Técnicas Avanzadas de RNN</h1>
<div class='content'></div><h2>Memoria a Largo Plazo (LSTM)</h2>
<div class='content'><p>Las LSTMs son un tipo de RNN diseñadas para manejar el problema de los gradientes desvanecientes introduciendo puertas que controlan el flujo de información.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgU2ltcGxlTFNUTShubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIGlucHV0X3NpemUsIGhpZGRlbl9zaXplLCBvdXRwdXRfc2l6ZSk6CiAgICAgICAgc3VwZXIoU2ltcGxlTFNUTSwgc2VsZikuX19pbml0X18oKQogICAgICAgIHNlbGYuaGlkZGVuX3NpemUgPSBoaWRkZW5fc2l6ZQogICAgICAgIHNlbGYubHN0bSA9IG5uLkxTVE0oaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemUsIGJhdGNoX2ZpcnN0PVRydWUpCiAgICAgICAgc2VsZi5mYyA9IG5uLkxpbmVhcihoaWRkZW5fc2l6ZSwgb3V0cHV0X3NpemUpCiAgICAKICAgIGRlZiBmb3J3YXJkKHNlbGYsIHgpOgogICAgICAgIGgwID0gdG9yY2guemVyb3MoMSwgeC5zaXplKDApLCBzZWxmLmhpZGRlbl9zaXplKS50byh4LmRldmljZSkKICAgICAgICBjMCA9IHRvcmNoLnplcm9zKDEsIHguc2l6ZSgwKSwgc2VsZi5oaWRkZW5fc2l6ZSkudG8oeC5kZXZpY2UpCiAgICAgICAgb3V0LCAoaG4sIGNuKSA9IHNlbGYubHN0bSh4LCAoaDAsIGMwKSkKICAgICAgICBvdXQgPSBzZWxmLmZjKG91dFs6LCAtMSwgOl0pCiAgICAgICAgcmV0dXJuIG91dAoKIyBFamVtcGxvIGRlIHVzbwptb2RlbCA9IFNpbXBsZUxTVE0oaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemUsIG91dHB1dF9zaXplKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class SimpleLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        out, (hn, cn) = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Ejemplo de uso
model = SimpleLSTM(input_size, hidden_size, output_size)</pre></div><div class='content'></div><h2>Explicación</h2>
<div class='content'><ul>
<li><strong>LSTM</strong>: Similar a RNN pero incluye estado de celda y puertas (entrada, olvido, salida) para gestionar dependencias a largo plazo.</li>
</ul>
</div><h2>Unidad Recurrente Gated (GRU)</h2>
<div class='content'><p>Las GRUs son una alternativa más simple a las LSTMs, usando menos puertas y parámetros.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgU2ltcGxlR1JVKG5uLk1vZHVsZSk6CiAgICBkZWYgX19pbml0X18oc2VsZiwgaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemUsIG91dHB1dF9zaXplKToKICAgICAgICBzdXBlcihTaW1wbGVHUlUsIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmhpZGRlbl9zaXplID0gaGlkZGVuX3NpemUKICAgICAgICBzZWxmLmdydSA9IG5uLkdSVShpbnB1dF9zaXplLCBoaWRkZW5fc2l6ZSwgYmF0Y2hfZmlyc3Q9VHJ1ZSkKICAgICAgICBzZWxmLmZjID0gbm4uTGluZWFyKGhpZGRlbl9zaXplLCBvdXRwdXRfc2l6ZSkKICAgIAogICAgZGVmIGZvcndhcmQoc2VsZiwgeCk6CiAgICAgICAgaDAgPSB0b3JjaC56ZXJvcygxLCB4LnNpemUoMCksIHNlbGYuaGlkZGVuX3NpemUpLnRvKHguZGV2aWNlKQogICAgICAgIG91dCwgaG4gPSBzZWxmLmdydSh4LCBoMCkKICAgICAgICBvdXQgPSBzZWxmLmZjKG91dFs6LCAtMSwgOl0pCiAgICAgICAgcmV0dXJuIG91dAoKIyBFamVtcGxvIGRlIHVzbwptb2RlbCA9IFNpbXBsZUdSVShpbnB1dF9zaXplLCBoaWRkZW5fc2l6ZSwgb3V0cHV0X3NpemUp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class SimpleGRU(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleGRU, self).__init__()
        self.hidden_size = hidden_size
        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        out, hn = self.gru(x, h0)
        out = self.fc(out[:, -1, :])
        return out

# Ejemplo de uso
model = SimpleGRU(input_size, hidden_size, output_size)</pre></div><div class='content'></div><h2>Explicación</h2>
<div class='content'><ul>
<li><strong>GRU</strong>: Combina las puertas de olvido y entrada en una sola puerta de actualización, simplificando la arquitectura.</li>
</ul>
</div><h1>Conclusión</h1>
<div class='content'><p>Las Redes Neuronales Recurrentes son herramientas poderosas para manejar datos secuenciales. Al entender los conceptos básicos de las RNNs e implementarlas en PyTorch, puedes abordar una amplia gama de problemas que involucran series temporales, texto y más. Las variantes avanzadas como LSTMs y GRUs ayudan a mitigar problemas comunes como los gradientes desvanecientes, haciéndolas adecuadas para tareas más complejas. Con práctica y experimentación, puedes aprovechar estos modelos para construir aplicaciones sofisticadas.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/es/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
