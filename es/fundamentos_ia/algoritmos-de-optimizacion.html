<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Algoritmos de Optimización</title>

    <link rel="alternate" href="https://campusempresa.com/es/fundamentos_ia/algoritmos-de-optimizacion" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/fundamentos_ia/algoritmos-de-optimizacion" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/fundamentos_ia/algoritmos-de-optimizacion" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/es/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<b id="lit_lang_es" class="px-2">ES</b>
				|
				<a href="/ca/fundamentos_ia/algoritmos-de-optimizacion" id="lnk_lang_ca" data-lang="ca" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/es/objective">El Proyecto</a>
				<a href="/es/about">Sobre nosotros</a>
				<a href="/es/contribute">Contribuir</a>
				<a href="/es/donate">Donaciones</a>
				<a href="/es/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'><div class='col-6'><a href='algoritmos-de-busqueda'>&#x25C4; Algoritmos de Búsqueda</a></div><div class='col-6 text-end'><a href='algoritmos-geneticos'>Algoritmos Genéticos &#x25BA;</a></div></div><div class='content'></div><h1>Introducción a los Algoritmos de Optimización</h1>
<div class='content'><p>Los algoritmos de optimización son fundamentales en el campo de la Inteligencia Artificial (IA) y el aprendizaje automático. Estos algoritmos se utilizan para encontrar la mejor solución posible (o una solución suficientemente buena) a un problema dado, dentro de un conjunto de posibles soluciones. La optimización es crucial en diversas aplicaciones, desde la planificación de rutas hasta la sintonización de hiperparámetros en modelos de aprendizaje automático.</p>
</div><h1>Tipos de Algoritmos de Optimización</h1>
<div class='content'></div><h2>Optimización Convexa</h2>
<div class='content'><ul>
<li><strong>Definición</strong>: La optimización convexa se refiere a un subcampo de la optimización en el que el objetivo y las restricciones son funciones convexas. Esto significa que cualquier combinación lineal de dos puntos en el dominio de la función también está en el dominio.</li>
<li><strong>Ejemplos de problemas</strong>:
<ul>
<li>Programación lineal</li>
<li>Programación cuadrática</li>
</ul>
</li>
<li><strong>Métodos comunes</strong>:
<ul>
<li>Método del gradiente</li>
<li>Método del gradiente conjugado</li>
<li>Método de Newton</li>
</ul>
</li>
</ul>
</div><h2>Optimización No Convexa</h2>
<div class='content'><ul>
<li><strong>Definición</strong>: En la optimización no convexa, el objetivo o las restricciones no son necesariamente convexas, lo que puede llevar a múltiples óptimos locales.</li>
<li><strong>Ejemplos de problemas</strong>:
<ul>
<li>Redes neuronales profundas</li>
<li>Optimización de funciones no lineales</li>
</ul>
</li>
<li><strong>Métodos comunes</strong>:
<ul>
<li>Algoritmos genéticos</li>
<li>Simulated Annealing</li>
<li>Optimización por enjambre de partículas (PSO)</li>
</ul>
</li>
</ul>
</div><h2>Optimización Discreta</h2>
<div class='content'><ul>
<li><strong>Definición</strong>: La optimización discreta se ocupa de problemas en los que las variables de decisión son discretas, como enteros o booleanos.</li>
<li><strong>Ejemplos de problemas</strong>:
<ul>
<li>Problema del viajante (TSP)</li>
<li>Problema de la mochila</li>
</ul>
</li>
<li><strong>Métodos comunes</strong>:
<ul>
<li>Programación dinámica</li>
<li>Algoritmos de búsqueda (A*, BFS, DFS)</li>
</ul>
</li>
</ul>
</div><h1>Métodos de Optimización Comunes</h1>
<div class='content'></div><h2>Método del Gradiente Descendente</h2>
<div class='content'><ul>
<li>
<p><strong>Descripción</strong>: Es un algoritmo iterativo utilizado para minimizar una función objetivo. Se basa en moverse en la dirección opuesta al gradiente de la función en el punto actual.</p>
</li>
<li>
<p><strong>Fórmula</strong>:</p>
<pre><code class="language-python">θ = θ - α * ∇J(θ)
</code></pre>
<p>donde:</p>
<ul>
<li><code>θ</code> es el vector de parámetros.</li>
<li><code>α</code> es la tasa de aprendizaje.</li>
<li><code>∇J(θ)</code> es el gradiente de la función de costo <code>J</code> con respecto a <code>θ</code>.</li>
</ul>
</li>
<li>
<p><strong>Ejemplo de código</strong>:</p>
<pre><code class="language-python">import numpy as np

def gradient_descent(X, y, theta, learning_rate, iterations):
    m = len(y)
    for _ in range(iterations):
        gradient = X.T.dot(X.dot(theta) - y) / m
        theta = theta - learning_rate * gradient
    return theta

# Datos de ejemplo
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
y = np.dot(X, np.array([1, 2])) + 3
theta = np.zeros(2)

# Aplicar el método del gradiente descendente
theta = gradient_descent(X, y, theta, 0.01, 1000)
print(theta)
</code></pre>
</li>
</ul>
</div><h2>Algoritmos Genéticos</h2>
<div class='content'><ul>
<li>
<p><strong>Descripción</strong>: Los algoritmos genéticos son métodos de búsqueda inspirados en la teoría de la evolución de Darwin. Utilizan operaciones como la selección, cruce y mutación para evolucionar una población de soluciones.</p>
</li>
<li>
<p><strong>Pasos principales</strong>:</p>
<ul>
<li>Inicialización</li>
<li>Evaluación</li>
<li>Selección</li>
<li>Cruce (crossover)</li>
<li>Mutación</li>
</ul>
</li>
<li>
<p><strong>Ejemplo de código</strong>:</p>
<pre><code class="language-python">import random

def genetic_algorithm(population, fitness_fn, mutation_rate=0.01, generations=100):
    for _ in range(generations):
        population = sorted(population, key=fitness_fn, reverse=True)
        next_generation = population[:2]  # Elitismo: mantener los mejores dos
        for _ in range(len(population) // 2 - 1):
            parents = random.sample(population[:10], 2)
            crossover_point = random.randint(1, len(parents[0]) - 1)
            child1 = parents[0][:crossover_point] + parents[1][crossover_point:]
            child2 = parents[1][:crossover_point] + parents[0][crossover_point:]
            next_generation += [mutate(child1, mutation_rate), mutate(child2, mutation_rate)]
        population = next_generation
    return max(population, key=fitness_fn)

def mutate(individual, mutation_rate):
    return [gene if random.random() &gt; mutation_rate else random.choice([0, 1]) for gene in individual]

# Datos de ejemplo
population = [[random.choice([0, 1]) for _ in range(10)] for _ in range(20)]
fitness_fn = lambda x: sum(x)  # Ejemplo de función de aptitud

# Aplicar el algoritmo genético
best_solution = genetic_algorithm(population, fitness_fn)
print(best_solution)
</code></pre>
</li>
</ul>
</div><h1>Comparación de Métodos de Optimización</h1>
<div class='content'><p>| Método                  | Ventajas                                    | Desventajas                                  |
|-------------------------|---------------------------------------------|----------------------------------------------|
| Gradiente Descendente   | Simple, eficiente para problemas convexos   | Puede atascarse en óptimos locales           |
| Algoritmos Genéticos    | No requiere derivadas, maneja bien óptimos locales | Computacionalmente costoso, parámetros difíciles de ajustar |
| Simulated Annealing     | Escapa de óptimos locales                   | Lento, requiere ajuste de parámetros         |
| Optimización por Enjambre de Partículas (PSO) | Fácil de implementar, buena exploración | Puede converger prematuramente, parámetros difíciles de ajustar |</p>
</div><h1>Conclusión</h1>
<div class='content'><p>Los algoritmos de optimización son herramientas poderosas en la Inteligencia Artificial, permitiendo encontrar soluciones óptimas o casi óptimas a problemas complejos. Desde métodos clásicos como el gradiente descendente hasta técnicas más avanzadas como los algoritmos genéticos, cada enfoque tiene sus propias ventajas y desventajas. Comprender estos métodos y saber cuándo aplicarlos es esencial para cualquier profesional en el campo de la IA.</p>
</div><div class='row navigation'><div class='col-6'><a href='algoritmos-de-busqueda'>&#x25C4; Algoritmos de Búsqueda</a></div><div class='col-6 text-end'><a href='algoritmos-geneticos'>Algoritmos Genéticos &#x25BA;</a></div></div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/es/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
