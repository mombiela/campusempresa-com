<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title></title>

	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy<br> y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<b id="lit_lang_es">Castellano</b>
				|
				<a href="https://campusempresa.cat/tensorflow/reinforcement-learning" id="lnk_lang_ca" data-lang="ca">Català</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="objective">El Proyecto</a>
				<a href="about">Sobre nosotros</a>
				<a href="contribute">Contribuir</a>
				<a href="donate">Donaciones</a>
				<a href="licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'></div><h1>Introducción al Aprendizaje por Refuerzo</h1>
<div class='content'><p>El Aprendizaje por Refuerzo (RL) es un tipo de aprendizaje automático donde un agente aprende a tomar decisiones realizando acciones en un entorno para maximizar la recompensa acumulada.</p>
<ul>
<li><strong>Agente</strong>: El aprendiz o tomador de decisiones.</li>
<li><strong>Entorno</strong>: El sistema externo con el que interactúa el agente.</li>
<li><strong>Acción</strong>: Lo que el agente puede hacer.</li>
<li><strong>Estado</strong>: La situación actual del agente.</li>
<li><strong>Recompensa</strong>: La retroalimentación del entorno basada en la acción tomada.</li>
</ul>
</div><h1>Conceptos Clave en el Aprendizaje por Refuerzo</h1>
<div class='content'><ul>
<li><strong>Política</strong>: Una estrategia utilizada por el agente para decidir qué acciones tomar.</li>
<li><strong>Función de Valor</strong>: Una función que estima la recompensa esperada de estados o pares estado-acción.</li>
<li><strong>Modelo</strong>: La representación del entorno por parte del agente.</li>
</ul>
</div><h1>Configuración de TensorFlow para el Aprendizaje por Refuerzo</h1>
<div class='content'><p>Primero, asegúrate de tener TensorFlow instalado. Puedes instalarlo usando pip:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgdGVuc29yZmxvdw=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install tensorflow</pre></div><div class='content'></div><h1>Construcción de un Modelo RL Simple con TensorFlow</h1>
<h2>Paso 1: Definir el Entorno</h2>
<div class='content'><p>Usaremos la biblioteca Gym de OpenAI para crear un entorno simple. Instala Gym usando pip:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgZ3lt"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install gym</pre></div><div class='content'></div><h2>Paso 2: Crear el Entorno</h2>
<div class='content'><p>Aquí tienes un ejemplo de cómo crear un entorno simple de CartPole:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGd5bQoKZW52ID0gZ3ltLm1ha2UoJ0NhcnRQb2xlLXYxJykKc3RhdGUgPSBlbnYucmVzZXQoKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import gym

env = gym.make('CartPole-v1')
state = env.reset()</pre></div><div class='content'></div><h2>Paso 3: Definir la Red Neuronal</h2>
<div class='content'><p>Usaremos TensorFlow para definir una red neuronal simple que actuará como nuestra política.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzIGltcG9ydCBsYXllcnMKCm1vZGVsID0gdGYua2VyYXMuU2VxdWVudGlhbChbCiAgICBsYXllcnMuRGVuc2UoMjQsIGFjdGl2YWNpw7NuPSdyZWx1JywgaW5wdXRfc2hhcGU9KGVudi5vYnNlcnZhdGlvbl9zcGFjZS5zaGFwZVswXSwpKSwKICAgIGxheWVycy5EZW5zZSgyNCwgYWN0aXZhY2nDs249J3JlbHUnKSwKICAgIGxheWVycy5EZW5zZShlbnYuYWN0aW9uX3NwYWNlLm4sIGFjdGl2YWNpw7NuPSdsaW5lYXInKQpdKQoKbW9kZWwuY29tcGlsZShvcHRpbWl6ZXI9dGYua2VyYXMub3B0aW1pemVycy5BZGFtKGxlYXJuaW5nX3JhdGU9MC4wMDEpLCBsb3NzPSdtc2UnKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras import layers

model = tf.keras.Sequential([
    layers.Dense(24, activaci&oacute;n='relu', input_shape=(env.observation_space.shape[0],)),
    layers.Dense(24, activaci&oacute;n='relu'),
    layers.Dense(env.action_space.n, activaci&oacute;n='linear')
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')</pre></div><div class='content'></div><h2>Paso 4: Implementar el Bucle de Entrenamiento</h2>
<div class='content'><p>Implementaremos un bucle de entrenamiento simple donde el agente interactúa con el entorno y aprende de las recompensas.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgdHJhaW5fbW9kZWwoZW52LCBtb2RlbCwgZXBpc29kZXM9MTAwMCwgZ2FtbWE9MC45OSk6CiAgICBmb3IgZXBpc29kZSBpbiByYW5nZShlcGlzb2Rlcyk6CiAgICAgICAgc3RhdGUgPSBlbnYucmVzZXQoKQogICAgICAgIHRvdGFsX3Jld2FyZCA9IDAKICAgICAgICBkb25lID0gRmFsc2UKCiAgICAgICAgd2hpbGUgbm90IGRvbmU6CiAgICAgICAgICAgIHN0YXRlID0gc3RhdGUucmVzaGFwZShbMSwgc3RhdGUuc2hhcGVbMF1dKQogICAgICAgICAgICBxX3ZhbHVlcyA9IG1vZGVsLnByZWRpY3Qoc3RhdGUpCiAgICAgICAgICAgIGFjdGlvbiA9IG5wLmFyZ21heChxX3ZhbHVlc1swXSkKCiAgICAgICAgICAgIG5leHRfc3RhdGUsIHJld2FyZCwgZG9uZSwgXyA9IGVudi5zdGVwKGFjdGlvbikKICAgICAgICAgICAgdG90YWxfcmV3YXJkICs9IHJld2FyZAoKICAgICAgICAgICAgbmV4dF9zdGF0ZSA9IG5leHRfc3RhdGUucmVzaGFwZShbMSwgbmV4dF9zdGF0ZS5zaGFwZVswXV0pCiAgICAgICAgICAgIHFfdmFsdWVzX25leHQgPSBtb2RlbC5wcmVkaWN0KG5leHRfc3RhdGUpCiAgICAgICAgICAgIHFfdmFsdWVzWzBdW2FjdGlvbl0gPSByZXdhcmQgKyBnYW1tYSAqIG5wLm1heChxX3ZhbHVlc19uZXh0WzBdKQoKICAgICAgICAgICAgbW9kZWwuZml0KHN0YXRlLCBxX3ZhbHVlcywgdmVyYm9zZT0wKQoKICAgICAgICAgICAgc3RhdGUgPSBuZXh0X3N0YXRlCgogICAgICAgIHByaW50KGYiRXBpc29kaW86IHtlcGlzb2RlICsgMX0sIFJlY29tcGVuc2EgVG90YWw6IHt0b3RhbF9yZXdhcmR9IikKCmVudiA9IGd5bS5tYWtlKCdDYXJ0UG9sZS12MScpCnRyYWluX21vZGVsKGVudiwgbW9kZWwp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def train_model(env, model, episodes=1000, gamma=0.99):
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            state = state.reshape([1, state.shape[0]])
            q_values = model.predict(state)
            action = np.argmax(q_values[0])

            next_state, reward, done, _ = env.step(action)
            total_reward += reward

            next_state = next_state.reshape([1, next_state.shape[0]])
            q_values_next = model.predict(next_state)
            q_values[0][action] = reward + gamma * np.max(q_values_next[0])

            model.fit(state, q_values, verbose=0)

            state = next_state

        print(f&quot;Episodio: {episode + 1}, Recompensa Total: {total_reward}&quot;)

env = gym.make('CartPole-v1')
train_model(env, model)</pre></div><div class='content'></div><h1>Temas Avanzados en el Aprendizaje por Refuerzo</h1>
<h2>Aprendizaje Profundo Q</h2>
<div class='content'><p>El Aprendizaje Profundo Q (DQN) utiliza una red neuronal para aproximar la función de valor Q.</p>
<ul>
<li><strong>Repetición de Experiencia</strong>: Almacena las experiencias del agente para romper la correlación entre muestras consecutivas.</li>
<li><strong>Red de Objetivo</strong>: Una red separada para estabilizar el entrenamiento.</li>
</ul>
</div><h2>Métodos de Gradiente de Política</h2>
<div class='content'><p>Los métodos de gradiente de política optimizan la política directamente.</p>
<ul>
<li><strong>Algoritmo REINFORCE</strong>: Un método de gradiente de política de Monte Carlo.</li>
<li><strong>Métodos Actor-Crítico</strong>: Combina métodos basados en valor y basados en política.</li>
</ul>
</div><h2>Ejemplo de DQN con TensorFlow</h2>
<div class='content'><p>Aquí tienes un ejemplo más avanzado usando DQN con repetición de experiencia y una red de objetivo.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import random
from collections import deque

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        self.learning_rate = 0.001
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_model()

    def _build_model(self):
        model = tf.keras.Sequential([
            layers.Dense(24, activaci&oacute;n='relu', input_shape=(self.state_size,)),
            layers.Dense(24, activaci&oacute;n='relu'),
            layers.Dense(self.action_size, activaci&oacute;n='linear')
        ])
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')
        return model

    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() &lt;= self.epsilon:
            return random.randrange(self.action_size)
        q_values = self.model.predict(state)
        return np.argmax(q_values[0])

    def replay(self, batch_size):
        minibatch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])
            target_f = self.model.predict(state)
            target_f[0][action] = target
            self.model.fit(state, target_f, epochs=1, verbose=0)
        if self.epsilon &gt; self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def load(self, name):
        self.model.load_weights(name)

    def save(self, name):
        self.model.save_weights(name)

env = gym.make('CartPole-v1')
state_size = env.observation_space.shape[0]
action_size = env.action_space.n
agent = DQNAgent(state_size, action_size)
episodes = 1000
batch_size = 32

for e in range(episodes):
    state = env.reset()
    state = np.reshape(state, [1, state_size])
    for time in range(500):
        action = agent.act(state)
        next_state, reward, done, _ = env.step(action)
        reward = reward if not done else -10
        next_state = np.reshape(next_state, [1, state_size])
        agent.remember(state, action, reward, next_state, done)
        state = next_state
        if done:
            agent.update_target_model()
            print(f&quot;Episodio: {e + 1}/{episodes}, Puntuaci&oacute;n: {time}, Epsilon: {agent.epsilon:.2}&quot;)
            break
        if len(agent.memory) &gt; batch_size:
            agent.replay(batch_size)</pre></div><div class='content'></div><h1>Conclusión</h1>
<div class='content'><p>El Aprendizaje por Refuerzo es un paradigma poderoso para entrenar agentes a tomar decisiones. TensorFlow proporciona herramientas robustas para implementar tanto algoritmos RL simples como avanzados. Al comprender los conceptos clave y seguir ejemplos estructurados, puedes construir y entrenar tus propios modelos RL. Continúa explorando entornos y algoritmos más complejos para profundizar tu comprensión y habilidades en RL con TensorFlow.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
       <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
       <a href="cookies">Més informació</a>
   </div>	
	</div>    
</body>
</html>
