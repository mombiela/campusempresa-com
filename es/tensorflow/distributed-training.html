<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Entrenamiento Distribuido</title>

    <link rel="alternate" href="https://campusempresa.com/es/tensorflow/distributed-training" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/tensorflow/distributed-training" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/tensorflow/distributed-training" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/es/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<b id="lit_lang_es" class="px-2">ES</b>
				|
				<a href="/ca/tensorflow/distributed-training" id="lnk_lang_ca" data-lang="ca" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/es/objective">El Proyecto</a>
				<a href="/es/about">Sobre nosotros</a>
				<a href="/es/contribute">Contribuir</a>
				<a href="/es/donate">Donaciones</a>
				<a href="/es/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'><div class='col-6'><a href='tensorflow-hub'>&#x25C4; TensorFlow Hub</a></div><div class='col-6 text-end'><a href='tensorflow-serving'>TensorFlow Serving &#x25BA;</a></div></div><div class='content'><p>El entrenamiento distribuido es una técnica utilizada para entrenar modelos de aprendizaje automático en múltiples dispositivos o máquinas. Este enfoque puede acelerar significativamente el proceso de entrenamiento y manejar conjuntos de datos más grandes que no cabrían en la memoria de un solo dispositivo. TensorFlow proporciona un soporte robusto para el entrenamiento distribuido, lo que facilita la escalabilidad de tus modelos.</p>
</div><h1>Introducción al Entrenamiento Distribuido</h1>
<div class='content'><ul>
<li><strong>Definición</strong>: El entrenamiento distribuido implica dividir la carga de trabajo del entrenamiento entre múltiples dispositivos o máquinas.</li>
<li><strong>Beneficios</strong>:
<ul>
<li><strong>Velocidad</strong>: Tiempos de entrenamiento más rápidos al aprovechar múltiples procesadores.</li>
<li><strong>Escalabilidad</strong>: Capacidad para entrenar en conjuntos de datos más grandes.</li>
<li><strong>Utilización de Recursos</strong>: Uso eficiente del hardware disponible.</li>
</ul>
</li>
</ul>
</div><h1>Tipos de Entrenamiento Distribuido</h1>
<div class='content'></div><h2>Paralelismo de Datos</h2>
<div class='content'><ul>
<li><strong>Concepto</strong>: Cada dispositivo obtiene una porción diferente de los datos pero el mismo modelo.</li>
<li><strong>Mecanismo</strong>: Los gradientes se promedian entre los dispositivos y los pesos del modelo se actualizan de manera sincrónica.</li>
<li><strong>Ejemplo</strong>:
<pre><code class="language-python">import tensorflow as tf

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=['accuracy'])

# Cargar y preprocesar datos
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# Entrenar el modelo
model.fit(x_train, y_train, epochs=5)
</code></pre>
</li>
</ul>
</div><h2>Paralelismo de Modelos</h2>
<div class='content'><ul>
<li><strong>Concepto</strong>: Diferentes partes del modelo se distribuyen entre múltiples dispositivos.</li>
<li><strong>Mecanismo</strong>: Cada dispositivo calcula una porción del modelo y los resultados se combinan.</li>
<li><strong>Caso de Uso</strong>: Útil cuando el modelo es demasiado grande para caber en la memoria de un solo dispositivo.</li>
</ul>
</div><h1>Estrategias de TensorFlow para Entrenamiento Distribuido</h1>
<div class='content'></div><h2>MirroredStrategy</h2>
<div class='content'><ul>
<li><strong>Descripción</strong>: Entrenamiento sincrónico en múltiples GPUs en una máquina.</li>
<li><strong>Uso</strong>:
<pre><code class="language-python">strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    # Define y compila tu modelo aquí
</code></pre>
</li>
</ul>
</div><h2>MultiWorkerMirroredStrategy</h2>
<div class='content'><ul>
<li><strong>Descripción</strong>: Entrenamiento sincrónico en múltiples máquinas.</li>
<li><strong>Uso</strong>:
<pre><code class="language-python">strategy = tf.distribute.MultiWorkerMirroredStrategy()
with strategy.scope():
    # Define y compila tu modelo aquí
</code></pre>
</li>
</ul>
</div><h2>TPUStrategy</h2>
<div class='content'><ul>
<li><strong>Descripción</strong>: Entrenamiento en dispositivos TPU (Unidad de Procesamiento Tensorial).</li>
<li><strong>Uso</strong>:
<pre><code class="language-python">resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='tu-dirección-tpu')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)

strategy = tf.distribute.TPUStrategy(resolver)
with strategy.scope():
    # Define y compila tu modelo aquí
</code></pre>
</li>
</ul>
</div><h1>Ejemplo Práctico: Entrenamiento Distribuido con MirroredStrategy</h1>
<div class='content'></div><h2>Guía Paso a Paso</h2>
<div class='content'><ol>
<li>
<p><strong>Configurar la Estrategia</strong>:</p>
<pre><code class="language-python">import tensorflow as tf

strategy = tf.distribute.MirroredStrategy()
</code></pre>
</li>
<li>
<p><strong>Definir el Modelo dentro del Alcance de la Estrategia</strong>:</p>
<pre><code class="language-python">with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=['accuracy'])
</code></pre>
</li>
<li>
<p><strong>Cargar y Preprocesar Datos</strong>:</p>
<pre><code class="language-python">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
</code></pre>
</li>
<li>
<p><strong>Entrenar el Modelo</strong>:</p>
<pre><code class="language-python">model.fit(x_train, y_train, epochs=5)
</code></pre>
</li>
</ol>
</div><h2>Explicación</h2>
<div class='content'><ul>
<li><strong>Configuración de la Estrategia</strong>: <code>MirroredStrategy</code> se utiliza para el entrenamiento sincrónico en múltiples GPUs.</li>
<li><strong>Definición del Modelo</strong>: El modelo se define dentro del alcance de la estrategia para asegurar que se distribuya entre las GPUs disponibles.</li>
<li><strong>Carga de Datos</strong>: Se carga y normaliza el conjunto de datos MNIST.</li>
<li><strong>Entrenamiento</strong>: El modelo se entrena utilizando el método <code>fit</code>, que distribuirá el entrenamiento entre las GPUs.</li>
</ul>
</div><h1>Conclusión</h1>
<div class='content'><p>El entrenamiento distribuido en TensorFlow te permite escalar tus modelos y acelerar el proceso de entrenamiento al aprovechar múltiples dispositivos. Al entender y utilizar diferentes estrategias como <code>MirroredStrategy</code>, <code>MultiWorkerMirroredStrategy</code> y <code>TPUStrategy</code>, puedes entrenar eficientemente modelos grandes en conjuntos de datos extensos. Comienza con estrategias simples y gradualmente avanza a configuraciones más complejas a medida que crezcan tus necesidades.</p>
</div><div class='row navigation'><div class='col-6'><a href='tensorflow-hub'>&#x25C4; TensorFlow Hub</a></div><div class='col-6 text-end'><a href='tensorflow-serving'>TensorFlow Serving &#x25BA;</a></div></div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/es/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
