<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autoencoders en TensorFlow</title>

    <link rel="alternate" href="https://campusempresa.com/es/tensorflow/autoencoders" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/tensorflow/autoencoders" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/tensorflow/autoencoders" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/es/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<b id="lit_lang_es" class="px-2">ES</b>
				|
				<a href="/ca/tensorflow/autoencoders" id="lnk_lang_ca" data-lang="ca" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/es/objective">El Proyecto</a>
				<a href="/es/about">Sobre nosotros</a>
				<a href="/es/contribute">Contribuir</a>
				<a href="/es/donate">Donaciones</a>
				<a href="/es/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='generative-adversarial-networks'>&#x25C4;Redes Generativas Antagónicas (GANs)</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Autoencoders en TensorFlow</a>
	</div>
	<div class='col-4 text-end'>
					<a href='reinforcement-learning'>Aprendizaje por Refuerzo con TensorFlow &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Los autoencoders son un tipo de red neuronal artificial utilizada para aprender codificaciones eficientes de datos no etiquetados. Se utilizan principalmente para la reducción de dimensionalidad, el aprendizaje de características y la detección de anomalías. En esta sección, exploraremos cómo implementar autoencoders utilizando TensorFlow, comenzando desde lo básico y progresando hacia conceptos más avanzados.</p>
</div><h1>Introducción a los Autoencoders</h1>
<div class='content'><p>Los autoencoders consisten en dos partes principales:</p>
<ul>
<li><strong>Codificador</strong>: Esta parte comprime la entrada en una representación del espacio latente.</li>
<li><strong>Decodificador</strong>: Esta parte reconstruye la entrada desde el espacio latente.</li>
</ul>
<p>El objetivo es hacer que la salida sea lo más cercana posible a la entrada.</p>
</div><h2>Conceptos Clave</h2>
<div class='content'><ul>
<li><strong>Espacio Latente</strong>: La representación comprimida de los datos de entrada.</li>
<li><strong>Pérdida de Reconstrucción</strong>: La diferencia entre la entrada y la salida reconstruida.</li>
<li><strong>Cuello de Botella</strong>: La capa en el medio del autoencoder que representa el espacio latente.</li>
</ul>
</div><h1>Construyendo un Autoencoder Básico</h1>
<div class='content'><p>Comencemos construyendo un autoencoder simple utilizando TensorFlow y Keras.</p>
</div><h2>Implementación Paso a Paso</h2>
<div class='content'><ol>
<li>
<p><strong>Importar Librerías</strong></p>
<pre><code class="language-python">import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
import numpy as np
</code></pre>
</li>
<li>
<p><strong>Preparar Datos</strong>
Para simplificar, utilizaremos el conjunto de datos MNIST.</p>
<pre><code class="language-python">(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
</code></pre>
</li>
<li>
<p><strong>Definir la Arquitectura del Autoencoder</strong></p>
<pre><code class="language-python">input_dim = x_train.shape[1]
encoding_dim = 32  # Este es el tamaño de nuestras representaciones codificadas

# Marcador de posición de entrada
input_img = Input(shape=(input_dim,))
# Representación codificada de la entrada
encoded = Dense(encoding_dim, activation='relu')(input_img)
# Representación decodificada de la entrada
decoded = Dense(input_dim, activation='sigmoid')(encoded)

# Modelo de autoencoder
autoencoder = Model(input_img, decoded)
</code></pre>
</li>
<li>
<p><strong>Compilar el Modelo</strong></p>
<pre><code class="language-python">autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
</code></pre>
</li>
<li>
<p><strong>Entrenar el Modelo</strong></p>
<pre><code class="language-python">autoencoder.fit(x_train, x_train,
                epochs=50,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))
</code></pre>
</li>
<li>
<p><strong>Evaluar el Modelo</strong></p>
<pre><code class="language-python">decoded_imgs = autoencoder.predict(x_test)
</code></pre>
</li>
</ol>
</div><h2>Explicación</h2>
<div class='content'><ul>
<li><strong>Capa de Entrada</strong>: La capa de entrada toma las imágenes aplanadas de MNIST.</li>
<li><strong>Capa de Codificación</strong>: Esta capa comprime la entrada en un espacio de menor dimensión.</li>
<li><strong>Capa de Decodificación</strong>: Esta capa reconstruye la entrada desde la representación codificada.</li>
<li><strong>Función de Pérdida</strong>: Se utiliza la entropía cruzada binaria para medir la diferencia entre la entrada y la salida reconstruida.</li>
</ul>
</div><h1>Autoencoders Avanzados</h1>
<div class='content'></div><h2>Autoencoders Variacionales (VAEs)</h2>
<div class='content'><p>Los VAEs son un tipo de autoencoder que imponen una estructura probabilística en el espacio latente. Esto permite interpolaciones más significativas y la generación de nuevos datos.</p>
<h4>Conceptos Clave</h4>
<ul>
<li><strong>Variables Latentes</strong>: Variables que capturan la estructura subyacente de los datos.</li>
<li><strong>Divergencia KL</strong>: Una medida de cómo una distribución de probabilidad se desvía de una segunda distribución de probabilidad esperada.</li>
</ul>
</div><h2>Implementando un VAE</h2>
<div class='content'><ol>
<li>
<p><strong>Definir la Arquitectura del VAE</strong></p>
<pre><code class="language-python">from tensorflow.keras.layers import Lambda, Layer
from tensorflow.keras.losses import mse

# Codificador
inputs = Input(shape=(input_dim,))
h = Dense(128, activation='relu')(inputs)
z_mean = Dense(encoding_dim)(h)
z_log_var = Dense(encoding_dim)(h)

def sampling(args):
    z_mean, z_log_var = args
    batch = tf.shape(z_mean)[0]
    dim = tf.shape(z_mean)[1]
    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon

z = Lambda(sampling, output_shape=(encoding_dim,))([z_mean, z_log_var])

# Decodificador
decoder_h = Dense(128, activation='relu')
decoder_mean = Dense(input_dim, activation='sigmoid')
h_decoded = decoder_h(z)
x_decoded_mean = decoder_mean(h_decoded)

# Modelo VAE
vae = Model(inputs, x_decoded_mean)

# Función de Pérdida
reconstruction_loss = mse(inputs, x_decoded_mean)
reconstruction_loss *= input_dim
kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
kl_loss = tf.reduce_mean(kl_loss) * -0.5
vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)
vae.add_loss(vae_loss)
</code></pre>
</li>
<li>
<p><strong>Compilar y Entrenar el VAE</strong></p>
<pre><code class="language-python">vae.compile(optimizer='adam')
vae.fit(x_train, epochs=50, batch_size=256, validation_data=(x_test, None))
</code></pre>
</li>
</ol>
</div><h2>Explicación</h2>
<div class='content'><ul>
<li><strong>Variables Latentes</strong>: <code>z_mean</code> y <code>z_log_var</code> representan la media y la varianza de las variables latentes.</li>
<li><strong>Capa de Muestreo</strong>: Esta capa toma muestras del espacio latente utilizando el truco de reparametrización.</li>
<li><strong>Divergencia KL</strong>: Este término asegura que las variables latentes aprendidas sigan una distribución normal estándar.</li>
</ul>
</div><h1>Conclusión</h1>
<div class='content'><p>Los autoencoders son herramientas poderosas para el aprendizaje no supervisado, capaces de aprender representaciones eficientes de los datos. Comenzamos con un autoencoder básico y progresamos hacia conceptos más avanzados como los Autoencoders Variacionales. Al comprender e implementar estos modelos, puedes aplicarlos a diversas tareas como la compresión de datos, la eliminación de ruido y la detección de anomalías.</p>
<p>En las siguientes secciones, exploraremos otras arquitecturas avanzadas de redes neuronales y sus aplicaciones en TensorFlow.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='generative-adversarial-networks'>&#x25C4;Redes Generativas Antagónicas (GANs)</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Autoencoders en TensorFlow</a>
	</div>
	<div class='col-4 text-end'>
					<a href='reinforcement-learning'>Aprendizaje por Refuerzo con TensorFlow &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/es/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
