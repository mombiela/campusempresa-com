<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Funciones de Activación en TensorFlow</title>

    <link rel="alternate" href="https://campusempresa.com/es/tensorflow/activation-functions" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/tensorflow/activation-functions" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/tensorflow/activation-functions" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/es/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy<br> y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<b id="lit_lang_es" class="px-2">ES</b>
				|
				<a href="/ca/tensorflow/activation-functions" id="lnk_lang_ca" data-lang="ca" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/es/objective">El Proyecto</a>
				<a href="/es/about">Sobre nosotros</a>
				<a href="/es/contribute">Contribuir</a>
				<a href="/es/donate">Donaciones</a>
				<a href="/es/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'><p>Las funciones de activación juegan un papel crucial en las redes neuronales al introducir no linealidad en el modelo, permitiéndole aprender patrones complejos. En esta sección, exploraremos varias funciones de activación, sus propiedades y cómo implementarlas en TensorFlow.</p>
</div><h1>Conceptos Clave</h1>
<div class='content'><ul>
<li><strong>No linealidad</strong>: Las funciones de activación introducen propiedades no lineales en la red, permitiéndole aprender de los errores y ajustar los pesos en consecuencia.</li>
<li><strong>Diferenciabilidad</strong>: La mayoría de las funciones de activación son diferenciables, lo cual es esencial para la retropropagación.</li>
<li><strong>Rango</strong>: El rango de salida de una función de activación puede afectar la estabilidad y el rendimiento de la red.</li>
</ul>
</div><h1>Funciones de Activación Comunes</h1>
<div class='content'></div><h2>Sigmoide</h2>
<div class='content'><p>La función sigmoide mapea cualquier entrada a un valor entre 0 y 1. A menudo se usa en la capa de salida para problemas de clasificación binaria.</p>
<p><strong>Fórmula</strong>:
\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<p><strong>Implementación en TensorFlow</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgRWplbXBsbyBkZSB1c28gZGUgbGEgZnVuY2nDs24gZGUgYWN0aXZhY2nDs24gc2lnbW9pZGUKeCA9IHRmLmNvbnN0YW50KFstMS4wLCAwLjAsIDEuMF0sIGR0eXBlPXRmLmZsb2F0MzIpCnNpZ21vaWRfb3V0cHV0ID0gdGYubm4uc2lnbW9pZCh4KQpwcmludChzaWdtb2lkX291dHB1dC5udW1weSgpKSAgIyBTYWxpZGE6IFswLjI2ODk0MTQzIDAuNSAwLjczMTA1ODZd"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Ejemplo de uso de la funci&oacute;n de activaci&oacute;n sigmoide
x = tf.constant([-1.0, 0.0, 1.0], dtype=tf.float32)
sigmoid_output = tf.nn.sigmoid(x)
print(sigmoid_output.numpy())  # Salida: [0.26894143 0.5 0.7310586]</pre></div><div class='content'></div><h2>Tanh</h2>
<div class='content'><p>La función tanh mapea cualquier entrada a un valor entre -1 y 1. Está centrada en cero, lo que puede facilitar la optimización.</p>
<p><strong>Fórmula</strong>:
\[ \text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<p><strong>Implementación en TensorFlow</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFamVtcGxvIGRlIHVzbyBkZSBsYSBmdW5jacOzbiBkZSBhY3RpdmFjacOzbiB0YW5oCnRhbmhfb3V0cHV0ID0gdGYubm4udGFuaCh4KQpwcmludCh0YW5oX291dHB1dC5udW1weSgpKSAgIyBTYWxpZGE6IFstMC43NjE1OTQyICAwLiAgICAgICAgIDAuNzYxNTk0Ml0="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Ejemplo de uso de la funci&oacute;n de activaci&oacute;n tanh
tanh_output = tf.nn.tanh(x)
print(tanh_output.numpy())  # Salida: [-0.7615942  0.         0.7615942]</pre></div><div class='content'></div><h2>ReLU (Unidad Lineal Rectificada)</h2>
<div class='content'><p>ReLU es una de las funciones de activación más populares debido a su simplicidad y efectividad. Devuelve la entrada directamente si es positiva; de lo contrario, devuelve cero.</p>
<p><strong>Fórmula</strong>:
\[ \text{ReLU}(x) = \max(0, x) \]</p>
<p><strong>Implementación en TensorFlow</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFamVtcGxvIGRlIHVzbyBkZSBsYSBmdW5jacOzbiBkZSBhY3RpdmFjacOzbiBSZUxVCnJlbHVfb3V0cHV0ID0gdGYubm4ucmVsdSh4KQpwcmludChyZWx1X291dHB1dC5udW1weSgpKSAgIyBTYWxpZGE6IFswLiAwLiAxLl0="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Ejemplo de uso de la funci&oacute;n de activaci&oacute;n ReLU
relu_output = tf.nn.relu(x)
print(relu_output.numpy())  # Salida: [0. 0. 1.]</pre></div><div class='content'></div><h2>Leaky ReLU</h2>
<div class='content'><p>Leaky ReLU es una variante de ReLU que permite un pequeño gradiente no nulo cuando la entrada es negativa, lo que ayuda a mitigar el problema del &quot;ReLU moribundo&quot;.</p>
<p><strong>Fórmula</strong>:
\[ \text{Leaky ReLU}(x) = \begin{cases}
x &amp; \text{si } x &gt; 0 <br>\alpha x &amp; \text{si } x \leq 0
\end{cases} \]</p>
<p><strong>Implementación en TensorFlow</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFamVtcGxvIGRlIHVzbyBkZSBsYSBmdW5jacOzbiBkZSBhY3RpdmFjacOzbiBMZWFreSBSZUxVCmxlYWt5X3JlbHVfb3V0cHV0ID0gdGYubm4ubGVha3lfcmVsdSh4LCBhbHBoYT0wLjEpCnByaW50KGxlYWt5X3JlbHVfb3V0cHV0Lm51bXB5KCkpICAjIFNhbGlkYTogWy0wLjEgIDAuICAgMS4gXQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Ejemplo de uso de la funci&oacute;n de activaci&oacute;n Leaky ReLU
leaky_relu_output = tf.nn.leaky_relu(x, alpha=0.1)
print(leaky_relu_output.numpy())  # Salida: [-0.1  0.   1. ]</pre></div><div class='content'></div><h2>Softmax</h2>
<div class='content'><p>Softmax se utiliza típicamente en la capa de salida de una red neuronal para problemas de clasificación multiclase. Convierte logits en probabilidades.</p>
<p><strong>Fórmula</strong>:
\[ \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]</p>
<p><strong>Implementación en TensorFlow</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFamVtcGxvIGRlIHVzbyBkZSBsYSBmdW5jacOzbiBkZSBhY3RpdmFjacOzbiBTb2Z0bWF4CmxvZ2l0cyA9IHRmLmNvbnN0YW50KFsxLjAsIDIuMCwgMy4wXSwgZHR5cGU9dGYuZmxvYXQzMikKc29mdG1heF9vdXRwdXQgPSB0Zi5ubi5zb2Z0bWF4KGxvZ2l0cykKcHJpbnQoc29mdG1heF9vdXRwdXQubnVtcHkoKSkgICMgU2FsaWRhOiBbMC4wOTAwMzA1NyAwLjI0NDcyODQ4IDAuNjY1MjQwOTRd"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Ejemplo de uso de la funci&oacute;n de activaci&oacute;n Softmax
logits = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)
softmax_output = tf.nn.softmax(logits)
print(softmax_output.numpy())  # Salida: [0.09003057 0.24472848 0.66524094]</pre></div><div class='content'></div><h1>Comparación de Funciones de Activación</h1>
<div class='content'><p>| Función de Activación | Rango de Salida | Diferenciable | Casos de Uso Comunes |
|-----------------------|-----------------|---------------|----------------------|
| Sigmoide              | (0, 1)          | Sí            | Clasificación binaria, capas ocultas |
| Tanh                  | (-1, 1)         | Sí            | Capas ocultas |
| ReLU                  | [0, ∞)          | Sí            | Capas ocultas |
| Leaky ReLU            | (-∞, ∞)         | Sí            | Capas ocultas, mitigación del ReLU moribundo |
| Softmax               | (0, 1)          | Sí            | Capa de salida para clasificación multiclase |</p>
</div><h1>Conclusión</h1>
<div class='content'><p>Las funciones de activación son componentes esenciales de las redes neuronales, introduciendo no linealidad y permitiendo que el modelo aprenda patrones complejos. En TensorFlow, implementar estas funciones es sencillo, y entender sus propiedades puede ayudarte a elegir la adecuada para tu problema específico. Experimenta con diferentes funciones de activación para ver cómo afectan el rendimiento y la estabilidad de tu modelo.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/es/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
