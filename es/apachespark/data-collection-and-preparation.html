<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Recolección y Preparación de Datos</title>

    <link rel="alternate" href="https://campusempresa.com/es/apachespark/data-collection-and-preparation" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/apachespark/data-collection-and-preparation" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/apachespark/data-collection-and-preparation" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/es/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<b id="lit_lang_es" class="px-2">ES</b>
				|
				<a href="/ca/apachespark/data-collection-and-preparation" id="lnk_lang_ca" data-lang="ca" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/es/objective">El Proyecto</a>
				<a href="/es/about">Sobre nosotros</a>
				<a href="/es/contribute">Contribuir</a>
				<a href="/es/donate">Donaciones</a>
				<a href="/es/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'><div class='col-6'><a href='project-overview-and-requirements'>&#x25C4; Descripción General y Requisitos del Proyecto</a></div><div class='col-6 text-end'><a href='implementation-and-development'>Implementación y Desarrollo &#x25BA;</a></div></div><div class='content'></div><h1>Introducción</h1>
<div class='content'><p>La recolección y preparación de datos son pasos críticos en cualquier flujo de procesamiento de datos. En Apache Spark, estos pasos implican reunir datos de diversas fuentes, limpiarlos y transformarlos en un formato adecuado para el análisis. Esta sección te guiará a través de los conceptos básicos de la recolección y preparación de datos usando Spark, progresando desde niveles principiantes hasta avanzados.</p>
</div><h1>Recolección de Datos</h1>
<div class='content'></div><h2>Lectura de Datos desde Varias Fuentes</h2>
<div class='content'><p>Apache Spark admite la lectura de datos desde múltiples fuentes, incluidos archivos locales, HDFS, S3 y varias bases de datos.</p>
<h4>Lectura desde Archivos Locales</h4>
<p>Para leer datos desde un archivo local, puedes usar el método <code>spark.read</code>. Aquí tienes un ejemplo de lectura de un archivo CSV:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBweXNwYXJrLnNxbCBpbXBvcnQgU3BhcmtTZXNzaW9uCgojIEluaWNpYWxpemFyIHNlc2nDs24gZGUgU3BhcmsKc3BhcmsgPSBTcGFya1Nlc3Npb24uYnVpbGRlci5hcHBOYW1lKCJEYXRhQ29sbGVjdGlvbiIpLmdldE9yQ3JlYXRlKCkKCiMgTGVlciBhcmNoaXZvIENTVgpkZiA9IHNwYXJrLnJlYWQuY3N2KCJwYXRoL3RvL2xvY2FsZmlsZS5jc3YiLCBoZWFkZXI9VHJ1ZSwgaW5mZXJTY2hlbWE9VHJ1ZSkKCiMgTW9zdHJhciBsYXMgcHJpbWVyYXMgZmlsYXMKZGYuc2hvdygp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from pyspark.sql import SparkSession

# Inicializar sesi&oacute;n de Spark
spark = SparkSession.builder.appName(&quot;DataCollection&quot;).getOrCreate()

# Leer archivo CSV
df = spark.read.csv(&quot;path/to/localfile.csv&quot;, header=True, inferSchema=True)

# Mostrar las primeras filas
df.show()</pre></div><div class='content'><h4>Lectura desde HDFS</h4>
<p>Leer desde HDFS es similar a leer desde archivos locales, pero necesitas especificar la ruta de HDFS:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGYgPSBzcGFyay5yZWFkLmNzdigiaGRmczovL25hbWVub2RlOjkwMDAvcGF0aC90by9oZGZzZmlsZS5jc3YiLCBoZWFkZXI9VHJ1ZSwgaW5mZXJTY2hlbWE9VHJ1ZSkKZGYuc2hvdygp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>df = spark.read.csv(&quot;hdfs://namenode:9000/path/to/hdfsfile.csv&quot;, header=True, inferSchema=True)
df.show()</pre></div><div class='content'><h4>Lectura desde S3</h4>
<p>Para leer desde S3, necesitas configurar tu sesión de Spark con credenciales de AWS:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("c3BhcmsuX2pzYy5oYWRvb3BDb25maWd1cmF0aW9uKCkuc2V0KCJmcy5zM2EuYWNjZXNzLmtleSIsICJZT1VSX0FDQ0VTU19LRVkiKQpzcGFyay5fanNjLmhhZG9vcENvbmZpZ3VyYXRpb24oKS5zZXQoImZzLnMzYS5zZWNyZXQua2V5IiwgIllPVVJfU0VDUkVUX0tFWSIpCgpkZiA9IHNwYXJrLnJlYWQuY3N2KCJzM2E6Ly9idWNrZXRuYW1lL3BhdGgvdG8vczNmaWxlLmNzdiIsIGhlYWRlcj1UcnVlLCBpbmZlclNjaGVtYT1UcnVlKQpkZi5zaG93KCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>spark._jsc.hadoopConfiguration().set(&quot;fs.s3a.access.key&quot;, &quot;YOUR_ACCESS_KEY&quot;)
spark._jsc.hadoopConfiguration().set(&quot;fs.s3a.secret.key&quot;, &quot;YOUR_SECRET_KEY&quot;)

df = spark.read.csv(&quot;s3a://bucketname/path/to/s3file.csv&quot;, header=True, inferSchema=True)
df.show()</pre></div><div class='content'><h4>Lectura desde Bases de Datos</h4>
<p>Spark también puede leer datos desde bases de datos relacionales usando JDBC:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("amRiY191cmwgPSAiamRiYzpteXNxbDovL2hvc3RuYW1lOnBvcnQvZGJuYW1lIgpwcm9wZXJ0aWVzID0gewogICAgInVzZXIiOiAidXNlcm5hbWUiLAogICAgInBhc3N3b3JkIjogInBhc3N3b3JkIgp9CgpkZiA9IHNwYXJrLnJlYWQuamRiYyh1cmw9amRiY191cmwsIHRhYmxlPSJ0YWJsZW5hbWUiLCBwcm9wZXJ0aWVzPXByb3BlcnRpZXMpCmRmLnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>jdbc_url = &quot;jdbc:mysql://hostname:port/dbname&quot;
properties = {
    &quot;user&quot;: &quot;username&quot;,
    &quot;password&quot;: &quot;password&quot;
}

df = spark.read.jdbc(url=jdbc_url, table=&quot;tablename&quot;, properties=properties)
df.show()</pre></div><div class='content'></div><h1>Limpieza de Datos</h1>
<div class='content'></div><h2>Manejo de Valores Faltantes</h2>
<div class='content'><p>Los valores faltantes son comunes en datos del mundo real. Spark proporciona varios métodos para manejarlos:</p>
<ul>
<li><strong>Eliminar valores faltantes:</strong></li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGZfY2xlYW5lZCA9IGRmLm5hLmRyb3AoKQpkZl9jbGVhbmVkLnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>df_cleaned = df.na.drop()
df_cleaned.show()</pre></div><div class='content'><ul>
<li><strong>Rellenar valores faltantes:</strong></li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGZfZmlsbGVkID0gZGYubmEuZmlsbCh7ImNvbHVtbjEiOiAwLCAiY29sdW1uMiI6ICJ1bmtub3duIn0pCmRmX2ZpbGxlZC5zaG93KCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>df_filled = df.na.fill({&quot;column1&quot;: 0, &quot;column2&quot;: &quot;unknown&quot;})
df_filled.show()</pre></div><div class='content'></div><h2>Eliminación de Duplicados</h2>
<div class='content'><p>Los duplicados pueden sesgar tu análisis. Puedes eliminarlos usando el método <code>dropDuplicates</code>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGZfdW5pcXVlID0gZGYuZHJvcER1cGxpY2F0ZXMoKQpkZl91bmlxdWUuc2hvdygp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>df_unique = df.dropDuplicates()
df_unique.show()</pre></div><div class='content'></div><h1>Transformación de Datos</h1>
<div class='content'></div><h2>Operaciones en Columnas</h2>
<div class='content'><p>Puedes realizar varias operaciones en las columnas de DataFrame, como renombrar, agregar y eliminar columnas.</p>
<ul>
<li><strong>Renombrar columnas:</strong></li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGZfcmVuYW1lZCA9IGRmLndpdGhDb2x1bW5SZW5hbWVkKCJvbGROYW1lIiwgIm5ld05hbWUiKQpkZl9yZW5hbWVkLnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>df_renamed = df.withColumnRenamed(&quot;oldName&quot;, &quot;newName&quot;)
df_renamed.show()</pre></div><div class='content'><ul>
<li><strong>Agregar nuevas columnas:</strong></li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IGNvbAoKZGZfbmV3Y29sID0gZGYud2l0aENvbHVtbigibmV3Q29sdW1uIiwgY29sKCJleGlzdGluZ0NvbHVtbiIpICogMikKZGZfbmV3Y29sLnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from pyspark.sql.functions import col

df_newcol = df.withColumn(&quot;newColumn&quot;, col(&quot;existingColumn&quot;) * 2)
df_newcol.show()</pre></div><div class='content'><ul>
<li><strong>Eliminar columnas:</strong></li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGZfZHJvcHBlZCA9IGRmLmRyb3AoImNvbHVtblRvRHJvcCIpCmRmX2Ryb3BwZWQuc2hvdygp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>df_dropped = df.drop(&quot;columnToDrop&quot;)
df_dropped.show()</pre></div><div class='content'></div><h2>Filtrado de Datos</h2>
<div class='content'><p>El filtrado te permite seleccionar filas basadas en condiciones específicas:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGZfZmlsdGVyZWQgPSBkZi5maWx0ZXIoZGZbImNvbHVtbiJdID4gMTApCmRmX2ZpbHRlcmVkLnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>df_filtered = df.filter(df[&quot;column&quot;] &gt; 10)
df_filtered.show()</pre></div><div class='content'></div><h2>Agregaciones</h2>
<div class='content'><p>Las agregaciones se utilizan para calcular estadísticas resumidas:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGZfZ3JvdXBlZCA9IGRmLmdyb3VwQnkoImNvbHVtbiIpLmNvdW50KCkKZGZfZ3JvdXBlZC5zaG93KCk="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>df_grouped = df.groupBy(&quot;column&quot;).count()
df_grouped.show()</pre></div><div class='content'></div><h1>Preparación Avanzada de Datos</h1>
<div class='content'></div><h2>Funciones Definidas por el Usuario (UDFs)</h2>
<div class='content'><p>Las UDFs te permiten aplicar transformaciones personalizadas a las columnas de DataFrame:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IHVkZgpmcm9tIHB5c3Bhcmsuc3FsLnR5cGVzIGltcG9ydCBJbnRlZ2VyVHlwZQoKZGVmIG11bHRpcGx5X2J5X3R3byh4KToKICAgIHJldHVybiB4ICogMgoKbXVsdGlwbHlfYnlfdHdvX3VkZiA9IHVkZihtdWx0aXBseV9ieV90d28sIEludGVnZXJUeXBlKCkpCgpkZl91ZGYgPSBkZi53aXRoQ29sdW1uKCJuZXdDb2x1bW4iLCBtdWx0aXBseV9ieV90d29fdWRmKGRmWyJleGlzdGluZ0NvbHVtbiJdKSkKZGZfdWRmLnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

def multiply_by_two(x):
    return x * 2

multiply_by_two_udf = udf(multiply_by_two, IntegerType())

df_udf = df.withColumn(&quot;newColumn&quot;, multiply_by_two_udf(df[&quot;existingColumn&quot;]))
df_udf.show()</pre></div><div class='content'></div><h2>Trabajando con Tipos de Datos Complejos</h2>
<div class='content'><p>Spark admite tipos de datos complejos como arrays y structs. Puedes manipular estos usando funciones integradas:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IGV4cGxvZGUKCiMgRXhwbG9zacOzbiBkZSB1bmEgY29sdW1uYSBkZSBhcnJheQpkZl9leHBsb2RlZCA9IGRmLndpdGhDb2x1bW4oImV4cGxvZGVkQ29sdW1uIiwgZXhwbG9kZShkZlsiYXJyYXlDb2x1bW4iXSkpCmRmX2V4cGxvZGVkLnNob3coKQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from pyspark.sql.functions import explode

# Explosi&oacute;n de una columna de array
df_exploded = df.withColumn(&quot;explodedColumn&quot;, explode(df[&quot;arrayColumn&quot;]))
df_exploded.show()</pre></div><div class='content'></div><h1>Conclusión</h1>
<div class='content'><p>La recolección y preparación de datos son pasos fundamentales en cualquier flujo de trabajo de procesamiento de datos. Apache Spark proporciona herramientas robustas para leer datos desde diversas fuentes, limpiarlos y transformarlos en un formato adecuado para el análisis. Al dominar estas técnicas, puedes asegurarte de que tus datos estén listos para las siguientes etapas de tu pipeline de datos, ya sea que impliquen aprendizaje automático, informes o análisis adicionales.</p>
</div><div class='row navigation'><div class='col-6'><a href='project-overview-and-requirements'>&#x25C4; Descripción General y Requisitos del Proyecto</a></div><div class='col-6 text-end'><a href='implementation-and-development'>Implementación y Desarrollo &#x25BA;</a></div></div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/es/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
