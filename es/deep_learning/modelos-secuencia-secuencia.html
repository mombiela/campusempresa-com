<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Modelos Secuencia a Secuencia</title>

    <link rel="alternate" href="https://campusempresa.com/es/deep_learning/modelos-secuencia-secuencia" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/deep_learning/modelos-secuencia-secuencia" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/deep_learning/modelos-secuencia-secuencia" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/es/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<b id="lit_lang_es" class="px-2">ES</b>
				|
				<a href="/ca/deep_learning/modelos-secuencia-secuencia" id="lnk_lang_ca" data-lang="ca" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/es/objective">El Proyecto</a>
				<a href="/es/about">Sobre nosotros</a>
				<a href="/es/contribute">Contribuir</a>
				<a href="/es/donate">Donaciones</a>
				<a href="/es/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'><div class='col-6'><a href='aplicaciones-rnn-pln'>&#x25C4; Aplicaciones de RNN en Procesamiento del Lenguaje Natural</a></div><div class='col-6 text-end'><a href='redes-generativas-antagonicas-gan'>Redes Generativas Antagónicas (GAN) &#x25BA;</a></div></div><div class='content'></div><h1>Introducción</h1>
<div class='content'><p>Los modelos secuencia a secuencia (Seq2Seq) son una clase de modelos de deep learning diseñados para transformar una secuencia de elementos en otra secuencia. Estos modelos son ampliamente utilizados en tareas como traducción automática, resumen de texto, y generación de texto.</p>
</div><h1>Conceptos Clave</h1>
<div class='content'><ul>
<li><strong>Codificador (Encoder)</strong>: Procesa la secuencia de entrada y la convierte en una representación de estado.</li>
<li><strong>Decodificador (Decoder)</strong>: Toma la representación de estado del codificador y genera la secuencia de salida.</li>
<li><strong>Atención (Attention)</strong>: Mecanismo que permite al decodificador enfocarse en diferentes partes de la secuencia de entrada en cada paso de la generación de la secuencia de salida.</li>
<li><strong>Embedding</strong>: Representación densa y de baja dimensión de palabras o tokens.</li>
<li><strong>RNN, LSTM, GRU</strong>: Tipos de redes neuronales recurrentes utilizadas comúnmente en modelos Seq2Seq.</li>
</ul>
</div><h1>Arquitectura de un Modelo Seq2Seq</h1>
<div class='content'></div><h2>Codificador</h2>
<div class='content'><p>El codificador toma una secuencia de entrada y la procesa a través de una red neuronal recurrente (RNN), Long Short-Term Memory (LSTM) o Gated Recurrent Unit (GRU) para producir una representación de estado.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLmxheWVycyBpbXBvcnQgTFNUTSwgRW1iZWRkaW5nLCBEZW5zZQoKIyBEZWZpbmljacOzbiBkZWwgY29kaWZpY2Fkb3IKZW5jb2Rlcl9pbnB1dHMgPSB0Zi5rZXJhcy5JbnB1dChzaGFwZT0oTm9uZSwpKQp4ID0gRW1iZWRkaW5nKGlucHV0X2RpbT12b2NhYl9zaXplLCBvdXRwdXRfZGltPWVtYmVkZGluZ19kaW0pKGVuY29kZXJfaW5wdXRzKQplbmNvZGVyX291dHB1dHMsIHN0YXRlX2gsIHN0YXRlX2MgPSBMU1RNKHVuaXRzPWxhdGVudF9kaW0sIHJldHVybl9zdGF0ZT1UcnVlKSh4KQplbmNvZGVyX3N0YXRlcyA9IFtzdGF0ZV9oLCBzdGF0ZV9jXQ=="))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.layers import LSTM, Embedding, Dense

# Definici&oacute;n del codificador
encoder_inputs = tf.keras.Input(shape=(None,))
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)
encoder_outputs, state_h, state_c = LSTM(units=latent_dim, return_state=True)(x)
encoder_states = [state_h, state_c]</pre></div><div class='content'></div><h2>Decodificador</h2>
<div class='content'><p>El decodificador utiliza la representación de estado del codificador para generar la secuencia de salida. En cada paso, el decodificador predice el siguiente token de la secuencia de salida.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBEZWZpbmljacOzbiBkZWwgZGVjb2RpZmljYWRvcgpkZWNvZGVyX2lucHV0cyA9IHRmLmtlcmFzLklucHV0KHNoYXBlPShOb25lLCkpCnggPSBFbWJlZGRpbmcoaW5wdXRfZGltPXZvY2FiX3NpemUsIG91dHB1dF9kaW09ZW1iZWRkaW5nX2RpbSkoZGVjb2Rlcl9pbnB1dHMpCnggPSBMU1RNKHVuaXRzPWxhdGVudF9kaW0sIHJldHVybl9zZXF1ZW5jZXM9VHJ1ZSkoeCwgaW5pdGlhbF9zdGF0ZT1lbmNvZGVyX3N0YXRlcykKZGVjb2Rlcl9vdXRwdXRzID0gRGVuc2UodW5pdHM9dm9jYWJfc2l6ZSwgYWN0aXZhdGlvbj0nc29mdG1heCcpKHgp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Definici&oacute;n del decodificador
decoder_inputs = tf.keras.Input(shape=(None,))
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)
x = LSTM(units=latent_dim, return_sequences=True)(x, initial_state=encoder_states)
decoder_outputs = Dense(units=vocab_size, activation='softmax')(x)</pre></div><div class='content'></div><h2>Mecanismo de Atención</h2>
<div class='content'><p>El mecanismo de atención permite al decodificador enfocarse en diferentes partes de la secuencia de entrada en cada paso de la generación de la secuencia de salida.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSB0ZW5zb3JmbG93LmtlcmFzLmxheWVycyBpbXBvcnQgQXR0ZW50aW9uCgojIERlZmluaWNpw7NuIGRlbCBtZWNhbmlzbW8gZGUgYXRlbmNpw7NuCmF0dGVudGlvbiA9IEF0dGVudGlvbigpCmNvbnRleHRfdmVjdG9yLCBhdHRlbnRpb25fd2VpZ2h0cyA9IGF0dGVudGlvbihbZW5jb2Rlcl9vdXRwdXRzLCBkZWNvZGVyX2lucHV0c10p"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from tensorflow.keras.layers import Attention

# Definici&oacute;n del mecanismo de atenci&oacute;n
attention = Attention()
context_vector, attention_weights = attention([encoder_outputs, decoder_inputs])</pre></div><div class='content'></div><h1>Comparación de RNN, LSTM y GRU</h1>
<div class='content'><p>| Característica | RNN | LSTM | GRU |
|----------------|-----|------|-----|
| Estructura     | Simple | Compleja (con celdas de memoria) | Intermedia |
| Rendimiento    | Menor | Alto | Alto |
| Velocidad      | Rápida | Lenta | Intermedia |
| Uso de memoria | Baja | Alta | Intermedia |</p>
</div><h1>Ejemplo Completo</h1>
<div class='content'><p>A continuación se muestra un ejemplo completo de un modelo Seq2Seq con un codificador y un decodificador utilizando LSTM.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLmxheWVycyBpbXBvcnQgSW5wdXQsIExTVE0sIEVtYmVkZGluZywgRGVuc2UKCiMgUGFyw6FtZXRyb3MKdm9jYWJfc2l6ZSA9IDEwMDAwCmVtYmVkZGluZ19kaW0gPSAyNTYKbGF0ZW50X2RpbSA9IDUxMgoKIyBDb2RpZmljYWRvcgplbmNvZGVyX2lucHV0cyA9IElucHV0KHNoYXBlPShOb25lLCkpCnggPSBFbWJlZGRpbmcoaW5wdXRfZGltPXZvY2FiX3NpemUsIG91dHB1dF9kaW09ZW1iZWRkaW5nX2RpbSkoZW5jb2Rlcl9pbnB1dHMpCmVuY29kZXJfb3V0cHV0cywgc3RhdGVfaCwgc3RhdGVfYyA9IExTVE0odW5pdHM9bGF0ZW50X2RpbSwgcmV0dXJuX3N0YXRlPVRydWUpKHgpCmVuY29kZXJfc3RhdGVzID0gW3N0YXRlX2gsIHN0YXRlX2NdCgojIERlY29kaWZpY2Fkb3IKZGVjb2Rlcl9pbnB1dHMgPSBJbnB1dChzaGFwZT0oTm9uZSwpKQp4ID0gRW1iZWRkaW5nKGlucHV0X2RpbT12b2NhYl9zaXplLCBvdXRwdXRfZGltPWVtYmVkZGluZ19kaW0pKGRlY29kZXJfaW5wdXRzKQp4ID0gTFNUTSh1bml0cz1sYXRlbnRfZGltLCByZXR1cm5fc2VxdWVuY2VzPVRydWUpKHgsIGluaXRpYWxfc3RhdGU9ZW5jb2Rlcl9zdGF0ZXMpCmRlY29kZXJfb3V0cHV0cyA9IERlbnNlKHVuaXRzPXZvY2FiX3NpemUsIGFjdGl2YXRpb249J3NvZnRtYXgnKSh4KQoKIyBNb2RlbG8gU2VxMlNlcQptb2RlbCA9IHRmLmtlcmFzLk1vZGVsKFtlbmNvZGVyX2lucHV0cywgZGVjb2Rlcl9pbnB1dHNdLCBkZWNvZGVyX291dHB1dHMpCm1vZGVsLmNvbXBpbGUob3B0aW1pemVyPSdhZGFtJywgbG9zcz0nc3BhcnNlX2NhdGVnb3JpY2FsX2Nyb3NzZW50cm9weScp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense

# Par&aacute;metros
vocab_size = 10000
embedding_dim = 256
latent_dim = 512

# Codificador
encoder_inputs = Input(shape=(None,))
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)
encoder_outputs, state_h, state_c = LSTM(units=latent_dim, return_state=True)(x)
encoder_states = [state_h, state_c]

# Decodificador
decoder_inputs = Input(shape=(None,))
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)
x = LSTM(units=latent_dim, return_sequences=True)(x, initial_state=encoder_states)
decoder_outputs = Dense(units=vocab_size, activation='softmax')(x)

# Modelo Seq2Seq
model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')</pre></div><div class='content'></div><h1>Conclusión</h1>
<div class='content'><p>Los modelos secuencia a secuencia son herramientas poderosas en el campo del deep learning, especialmente útiles para tareas que requieren la transformación de una secuencia de datos en otra. Comprender la arquitectura y los componentes clave de estos modelos, como el codificador, el decodificador y el mecanismo de atención, es fundamental para su implementación y optimización en diversas aplicaciones.</p>
</div><div class='row navigation'><div class='col-6'><a href='aplicaciones-rnn-pln'>&#x25C4; Aplicaciones de RNN en Procesamiento del Lenguaje Natural</a></div><div class='col-6 text-end'><a href='redes-generativas-antagonicas-gan'>Redes Generativas Antagónicas (GAN) &#x25BA;</a></div></div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/es/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
