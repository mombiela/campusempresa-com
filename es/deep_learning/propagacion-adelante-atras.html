<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Propagación Hacia Adelante y Hacia Atrás</title>

    <link rel="alternate" href="https://campusempresa.com/es/deep_learning/propagacion-adelante-atras" hreflang="es" />
    <link rel="alternate" href="https://campusempresa.com/es/deep_learning/propagacion-adelante-atras" hreflang="x-default" />
	<link rel="alternate" href="https://campusempresa.com/ca/deep_learning/propagacion-adelante-atras" hreflang="ca" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/es/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construyendo la sociedad de hoy y del mañana</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
										<b id="lit_lang_es" class="px-2">ES</b>
				|
				<a href="/ca/deep_learning/propagacion-adelante-atras" id="lnk_lang_ca" data-lang="ca" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/es/objective">El Proyecto</a>
				<a href="/es/about">Sobre nosotros</a>
				<a href="/es/contribute">Contribuir</a>
				<a href="/es/donate">Donaciones</a>
				<a href="/es/licence">Licencia</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='funciones-activacion'>&#x25C4;Funciones de Activación</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Propagación Hacia Adelante y Hacia Atrás</a>
	</div>
	<div class='col-4 text-end'>
					<a href='optimizacion-funciones-perdida'>Optimización y Funciones de Pérdida &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introducción</h1>
<div class='content'><p>En el contexto del Deep Learning, la propagación hacia adelante y hacia atrás son dos procesos fundamentales que permiten a las redes neuronales aprender de los datos. Estos procesos son esenciales para ajustar los pesos de la red y minimizar el error en las predicciones.</p>
</div><h1>Propagación Hacia Adelante</h1>
<div class='content'><p>La propagación hacia adelante (forward propagation) es el proceso mediante el cual los datos de entrada se pasan a través de la red neuronal para generar una predicción.</p>
</div><h2>Pasos de la Propagación Hacia Adelante</h2>
<div class='content'><ol>
<li><strong>Entrada de Datos</strong>: Los datos de entrada se introducen en la red.</li>
<li><strong>Cálculo de Activaciones</strong>: Cada neurona en la red calcula su activación en función de las entradas y los pesos.</li>
<li><strong>Aplicación de Funciones de Activación</strong>: Las activaciones se pasan a través de funciones de activación no lineales.</li>
<li><strong>Generación de Salida</strong>: La salida final se obtiene en la capa de salida de la red.</li>
</ol>
</div><h2>Ejemplo de Código</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIERlZmluaXIgdW5hIGZ1bmNpw7NuIGRlIGFjdGl2YWNpw7NuIChSZUxVKQpkZWYgcmVsdSh4KToKICAgIHJldHVybiBucC5tYXhpbXVtKDAsIHgpCgojIERhdG9zIGRlIGVudHJhZGEKWCA9IG5wLmFycmF5KFsxLCAyLCAzXSkKCiMgUGVzb3MgZGUgbGEgcmVkClcxID0gbnAuYXJyYXkoW1swLjIsIDAuOF0sIFswLjUsIDAuMV0sIFswLjksIDAuNF1dKQpXMiA9IG5wLmFycmF5KFswLjMsIDAuN10pCgojIFByb3BhZ2FjacOzbiBoYWNpYSBhZGVsYW50ZQpaMSA9IG5wLmRvdChYLCBXMSkKQTEgPSByZWx1KFoxKQpaMiA9IG5wLmRvdChBMSwgVzIpCm91dHB1dCA9IHJlbHUoWjIpCgpwcmludCgiU2FsaWRhIGRlIGxhIHJlZDoiLCBvdXRwdXQp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Definir una funci&oacute;n de activaci&oacute;n (ReLU)
def relu(x):
    return np.maximum(0, x)

# Datos de entrada
X = np.array([1, 2, 3])

# Pesos de la red
W1 = np.array([[0.2, 0.8], [0.5, 0.1], [0.9, 0.4]])
W2 = np.array([0.3, 0.7])

# Propagaci&oacute;n hacia adelante
Z1 = np.dot(X, W1)
A1 = relu(Z1)
Z2 = np.dot(A1, W2)
output = relu(Z2)

print(&quot;Salida de la red:&quot;, output)</pre></div><div class='content'></div><h1>Propagación Hacia Atrás</h1>
<div class='content'><p>La propagación hacia atrás (backpropagation) es el proceso de ajustar los pesos de la red neuronal en función del error de la predicción. Este proceso es crucial para el aprendizaje de la red.</p>
</div><h2>Pasos de la Propagación Hacia Atrás</h2>
<div class='content'><ol>
<li><strong>Cálculo del Error</strong>: Se calcula el error entre la predicción y el valor real.</li>
<li><strong>Cálculo del Gradiente</strong>: Se calcula el gradiente del error con respecto a cada peso utilizando la regla de la cadena.</li>
<li><strong>Actualización de Pesos</strong>: Los pesos se actualizan en la dirección opuesta al gradiente para minimizar el error.</li>
</ol>
</div><h2>Ejemplo de Código</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBEZWZpbmlyIHVuYSBmdW5jacOzbiBkZSBkZXJpdmFkYSBkZSBSZUxVCmRlZiByZWx1X2Rlcml2YXRpdmUoeCk6CiAgICByZXR1cm4gbnAud2hlcmUoeCA+IDAsIDEsIDApCgojIFZhbG9yIHJlYWwKeV90cnVlID0gMQoKIyBFcnJvcgplcnJvciA9IG91dHB1dCAtIHlfdHJ1ZQoKIyBQcm9wYWdhY2nDs24gaGFjaWEgYXRyw6FzCmRaMiA9IGVycm9yICogcmVsdV9kZXJpdmF0aXZlKFoyKQpkVzIgPSBucC5kb3QoQTEuVCwgZFoyKQoKZEExID0gbnAuZG90KGRaMiwgVzIuVCkKZFoxID0gZEExICogcmVsdV9kZXJpdmF0aXZlKFoxKQpkVzEgPSBucC5kb3QoWC5yZXNoYXBlKC0xLCAxKSwgZFoxKQoKIyBBY3R1YWxpemFjacOzbiBkZSBwZXNvcwpsZWFybmluZ19yYXRlID0gMC4wMQpXMSAtPSBsZWFybmluZ19yYXRlICogZFcxClcyIC09IGxlYXJuaW5nX3JhdGUgKiBkVzIKCnByaW50KCJQZXNvcyBhY3R1YWxpemFkb3MgVzE6IiwgVzEpCnByaW50KCJQZXNvcyBhY3R1YWxpemFkb3MgVzI6IiwgVzIp"))));alert("¡Copiado!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Definir una funci&oacute;n de derivada de ReLU
def relu_derivative(x):
    return np.where(x &gt; 0, 1, 0)

# Valor real
y_true = 1

# Error
error = output - y_true

# Propagaci&oacute;n hacia atr&aacute;s
dZ2 = error * relu_derivative(Z2)
dW2 = np.dot(A1.T, dZ2)

dA1 = np.dot(dZ2, W2.T)
dZ1 = dA1 * relu_derivative(Z1)
dW1 = np.dot(X.reshape(-1, 1), dZ1)

# Actualizaci&oacute;n de pesos
learning_rate = 0.01
W1 -= learning_rate * dW1
W2 -= learning_rate * dW2

print(&quot;Pesos actualizados W1:&quot;, W1)
print(&quot;Pesos actualizados W2:&quot;, W2)</pre></div><div class='content'></div><h1>Comparación de Propagación Hacia Adelante y Hacia Atrás</h1>
<div class='content'><p>| Característica                  | Propagación Hacia Adelante | Propagación Hacia Atrás |
|---------------------------------|----------------------------|-------------------------|
| Propósito                       | Generar predicciones       | Ajustar pesos           |
| Dirección del Flujo de Datos    | Entrada a salida           | Salida a entrada        |
| Cálculo Principal               | Activaciones               | Gradientes              |
| Dependencia de Datos            | Datos de entrada           | Error de predicción     |</p>
</div><h1>Conclusión</h1>
<div class='content'><p>La propagación hacia adelante y hacia atrás son procesos esenciales en el entrenamiento de redes neuronales. La propagación hacia adelante permite generar predicciones, mientras que la propagación hacia atrás ajusta los pesos de la red para minimizar el error. Comprender estos procesos es fundamental para cualquier profesional que trabaje en el campo del Deep Learning.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='funciones-activacion'>&#x25C4;Funciones de Activación</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Propagación Hacia Adelante y Hacia Atrás</a>
	</div>
	<div class='col-4 text-end'>
					<a href='optimizacion-funciones-perdida'>Optimización y Funciones de Pérdida &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicidad</h1>
			<p>Este espacio está destinado a publicidad.</p>
			<p>Si quieres ser patrocinador, contáctanos para incluir enlaces en esta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>¡Gracias por colaborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Todos los derechos reservados</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/es/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
